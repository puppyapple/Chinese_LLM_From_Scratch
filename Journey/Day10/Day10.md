# 从零手搓中文大模型｜🚀 Day10

中秋节到啦，假期不太有机会跑代码，打算码码字来讲一讲自己对**大模型**以及**人工智能**的一些拙见。

可能有些零碎或者发散，但都是自己平时工作或者学习过程中**反复在思考**的事情，一方面做个记录，一方面希望能抛砖引玉吧。



## 自回归模型的这一波AI崛起和以往有什么不同

这里特地用了**自回归模型**而没有用**大（语言）模型**，是因为这两天看到网上`Karpathy`大神关于「**大语言模型**应该纠正命名」这一引发热议的观点（详见下面的动态原文👇）。

![](https://erxuanyi-1257355350.cos.ap-beijing.myqcloud.com/202409170034223.png)

大意如下：

>「大型语言模型（`LLM`）名字虽然带有语言二字，但它们其实与语言关系并不大，这只是命名的历史遗留问题，更确切的名字应该是**自回归 Transformers** (`Autoregressive Transformers`)或者其他类似的表达。
>
>大家现在口中提到的`LLM` 更多是一种通用的统计建模技术：通过自回归的`Transformer`结构来模拟预测`token`流，而这些 `token`可以理论上可以是文本、图片、音频、动作、甚至是分子等任何东西。
>
>换句话说，只要能将实际问题转化为对一系列离散`token`的模拟，理论上都可以应用当下被称为`LLM`的技术路径来解决。

K神的这个观点我个人赞成**一半**。

从**纯粹的底层技术路径**角度来看，这个说法没有一点问题。

正如`Elon Musk`也同时发声所指出的：`Multimodal LLM(多模态大语言模型)`这种叫法实在很愚蠢，完全是自相矛盾的。

当下对于**大规模参数模型**+**大规模训练数据**结合`Next Token Prediction`进行预训练并且跟随后续的`SFT`/`RLHF`这一完整技术范式的应用，已经不仅仅限于文本数据了，而`LLM`里的`Language`这个词却很容易限制甚至说误导人们的思维在**文本**层面。

然而，尽管`Language`的概念应当且已经被从文本扩展到了图片、音频等更宽泛的范畴，这次以`ChatGPT`为引爆点的大模型革命之所以如此成功，很重要的一个不该被忽视的点在于，它在**应用层**让「语言」（这里我指的就是狭义的人类语言——文本或者语音）作为**交互的接口**成为了可能。

现在最主流的方向诸如`ChatGPT`以及其他类似的`AI Assistant`，又或是`Text2Image`、`Text2Video`等生成式应用，在其应用阶段甚至应用内部模块的构建阶段，我们都绕不开需要要通过人类的语言和它们底层的模型进行**交流**。

无论在大模型黑盒的内部是否建立了**自己的语言**，甚至说产生了**自己的意识**，只要我们还需要或者说还能掌控他们，那么**人类语言**在模型运转机制中的嵌入/耦合就是一个绕不开的必要条件。从这个角度讲，强调`LLM`中的人类`Language`在我看来依然有必要的。

扯得有点远哈，再稍微说回**人类语言作为交互接口**这个事儿，这是我个人认为这次「AI革命」中最为颠覆的一个点。

最末端的应用侧我就不多说了，用过`GPT4`、`Claude`或者玩过`Midjourney`、`Sora`的应该都深有体会：**人工智障**忽然能听懂人话了，尽管还不是那么利索那么聪明，但是显然已经不能用**智障**来形容人家了。

从应用的开发侧来讲，事情也发生了一些变化。

人工智能领域中纠缠了多年的**符号主义**、**联结主义**、**行为主义**，撇开其中抽象而复杂的底层逻辑，单看各个路径或者多个路径结合所构建出来的人工智能系统级应用（这里更多地是指类似`Bot`、`Agent`一类的产品形态），交互的**生硬**、**死板**是任何非专业人士也能一眼指出的致命缺陷。

私以为其中的一个原因：无论是大量规则构建的**专家系统**还是**（判别式）深度学习模型**为基础模块的**神经网络**系统，都在其内在的某个层面上，难以避免需要以严格的**符号表达或者说结构化数据**来实现通信，说人话就是整个系统需要依赖**类编程语言**这样的「接口」给串联起来。

> 可以说**符号主义**从来就没有真正消亡，它只是被削减了话语权，退居幕后隐藏得更深了。

而**大语言模型**为代表的**生成式模型**的异军突起，给这个问题提供了一个此前想都不敢想的可能性：那就是人类的自然语言也可以作为「接口」了。

为`Agent`系统做过`Prompt Engineering`，或者给大模型做过`SFT`之类的`Finetuning`的朋友，或许能感受到这些开发和以往的一些不同。

说来挺有意思，人类语言本质也是一种**符号**，但又天然具备了**模糊性**。

如果将纯`Symbolic`的东西比做**冰**，`Neural`的东西比做**水**，那么人类语言就好像是**冰水混合物**般的存在，能更加自然地在两者间来回转换。

在这个前提下，开发工作的重心之一变成了如何**更自如、更精确地引导这个双向转换的过程**。

我不敢也没有资格断言这个方式就一定能解决此前人工智能系统里的所有问题，但至少让大家看到了一种非常不一样的前景。

表达和比喻或许都有些抽象，但意思就是这么个意思，要是没看懂或者觉得我在胡说八道，大家权当一乐，不必太较真。



## 对我们的工作生活到底能有多大影响

有很多唱衰大模型这一波的，认为应用还是很难落地，我觉得得更具体地看对**落地**的定义。

如果说以`AGI`或者零门槛的「傻瓜式」应用为标准，那么的确现在的应用落地情况还远远达不到。

目前基于大模型技术的能带来真正增益的应用（所谓**新质生产力**）都多少有一定的门槛：

### 第一道槛是认知门槛

`Cursor`代码辑器这阵子很火，起初我以为是来了一波营销做得好，试用之后发现和两年前体验时确实不可同日而语，很增效、很智能，于是果断氪金入了坑。

作为`AI`从业者，我身边不乏高级程序员，但是像这样能极大提升工作效率的工具，我发现身边并没有太多人在使用。

他们或许有所耳闻，但对于它的`AI`能做到什么程度一无所知。

这还是我们这群每天在进行人工智能研发的群体里的情况。

再比如此前一度红得发紫的`Kimi`，可以说目前也只在一部分白领阶层有一定的知名度，对于更广大的普通老百姓而言，可能顶多就是在短视频平台上能刷到一些展示和`GPT`如何聊天的展示或者`AI`生成的逼真的图片/视频。

他们依然会觉得这个东西一定是离自己遥远的高端科技产品，自己不会有机会接触得到或者用得上。

他们自然也不会知道即使自己从没学过英语，也可以下一个免费的`APP`，通过不算太复杂的方式让一个`AI`助手辅导自己上小学的孩子的英语听说读写，效果不会逊色几千块的学习机多少。

从这个方面来讲，我觉得`AI`技术的**科普**是一件非常**迫在眉睫**的事情。

对一个`AI`应用而言，更庞大的、长尾的应用群体可以成为非常重要的发展助力。

而对个人而言，等到`AI`工具的使用已经和曾经的`Office`套件一样成为**基础技能**的时候才意识到需要学起来、用起来，可能已经落后了一大步了。

### 第二道槛才是技术门槛

这里的**技术**并非是编程这类具备专业门槛的技术，而是目前的`AI`应用的使用方式层面的技巧。

那些需要「**魔法**」/「**梯子**」的情况我就不多说了，大家懂的都懂。

可即便是国内可以接触到的面向C端的`AI Assistant`产品，能真正会用并且用好的人恐怕比例不大。

更别提那些完全免费但需要有**硬件支持和部署能力**的开源大模型了，否则像李某舟这样**锋利的镰刀**就不会出现了。

光是写`Prompt`这件看似只要会说人话就能干的事儿，其实不但需要对大模型的**技术和概念**有一定的了解，还需要大量**科学的实验和总结**。

所以，这两道门槛的存在导致了当下大模型为基础的`AI`应用现状是**割裂**的：头部的**早鸟**们要么已经在挖掘人工智能提效个人生产力的路上一路狂飞，要么已经开始利用信息差**割起了韭菜**；而绝大多数人还没有意识到变革已经在悄悄发生。



## 大模型出现后，AGI（通用人工智能）还有多远

我个人的工作算是`AGI`方向的，尽管我前面对于大模型的应用前景持乐观态度，但对于「**大模型是否带领我们走向AGI**」这件事儿还是存疑的。

首先，目前以大模型为基座的`Agent`系统还是在解决一些被动输入的任务，缺乏一个**内在的动力系统**提供自驱力。

即使推理能力强如`GPT-o1`，在没有任何外界指令的时候整个系统是静止的，简单来说不具备真正意义上的主动思考的能力。

这个问题的解决路径或许不在大模型的技术栈范畴。国内外也有一些前沿的研究在关注这块的内容，例如给机器构建类似人类的**价值体系**来让它具备自我驱动的可能，大家感兴趣的话可以去了解一下。

其次，性能是大模型的一个**硬伤**。

虽然**力大砖飞**的`Scaling Law`的的确确让大家看到了**大力出奇迹**的可能性，但是我们也不要忘了另一句话：「**天下武功，唯快不破**」。

但这反而也给我们提供了更多的想象空间：量变引发质变，其实「**速变**」也可以，这在计算机技术的发展历史上也得到过很多次的证明。

我们设想一下，如果能力最强的大模型在推理速度方面有了量级上的突破，那么一个以此为基础的多`Agent`系统能求解的问题复杂度可能要远超我们的想象。

最后是**硬件层面的门槛**，这可能已经上升到国家战略层面的博弈了。

这个问题和上面的**性能问题**有一定的关联，我个人对硬件技术一窍不通，只能从其他的角度来思考可能的解决途径。

于是我对于`SLM`（**小语言模型**）的相关研究产生了兴趣，其实`GPT2`的出现就已经一定程度地证明了**小尺寸的语言模型**也是具备很大的潜力的。

如果没有「**速度超快的最强大模型**」作为通用基座来构建系统，那么许多个**能力单一但卓越的小模型**是否能成为无限接近的平替呢？性能和硬件的问题同时得到了解决。



## 写在最后

不知不觉瞎扯的有点多了。

还是强调一下，个人水平有限，以上纯属自己个人的胡思乱想，可能有很多异想天开甚至错误的地方。

尽管历史证明，每一次人工智能的热潮都将逐渐降温和退去，但每次都将下一次的起点拉高了。

因此我对人工智能的未来还是十分乐观的，也很庆幸能在整个领域工作。

最后祝大家**中秋快乐**！
