# ä»é›¶æ‰‹æ“ä¸­æ–‡å¤§æ¨¡å‹ï½œğŸš€Day03
![](https://erxuanyi-1257355350.cos.ap-beijing.myqcloud.com/image.png)
## æ•°æ®é¢„å¤„ç†

è™½ç„¶çœç•¥äº†æ•°æ®æ¸…æ´—çš„é€»è¾‘ï¼Œä½†æ˜¯æˆ‘ä»¬è¿˜æ˜¯éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œä»¥ä¾¿äºåç»­çš„æ¨¡å‹è®­ç»ƒã€‚

åŒ…æ‹¬ä»¥ä¸‹ä¸¤ä¸ªç»†èŠ‚ï¼š

1. åœ¨æ¯ä¸ªæ–‡æœ¬åæ·»åŠ `eos`æ ‡è®°ï¼Œä»¥ä¾¿äºæ¨¡å‹è¯†åˆ«å¥å­çš„ç»“æŸã€‚
2. å°†æ–‡æœ¬è½¬æ¢ä¸º`æ•°å­—åºåˆ—`ï¼Œä»¥ä¾¿äºæ¨¡å‹å¤„ç†ã€‚
   
   è¿™ä¸€æ­¥å…¶å®ä¹Ÿå¯ä»¥æ”¾åˆ°æ¨¡å‹è®­ç»ƒçš„æ—¶å€™è¿›è¡Œï¼Œä½†æå‰å¤„ç†å¯ä»¥å‡å°‘è®­ç»ƒæ—¶çš„è®¡ç®—é‡ã€‚

### æ•°æ®é›†åˆ’åˆ†

è§£å‹æ•°æ®é›†ï¼Œå¾—åˆ°`48`ä¸ªjsonlæ–‡ä»¶ï¼Œå…±è®¡`3952863`è¡Œjsonæ•°æ®ã€‚

æˆ‘ä¹‹å‰å·²ç»è§£å‹è¿‡äº†ï¼Œå¹¶ä¸”å°†åŸå§‹æ•°æ®å’Œå¤„ç†è¿‡åçš„æ•°æ®åˆ†åˆ«å­˜åœ¨äº†ä¸åŒè·¯å¾„ä¸‹ã€‚

è¿™é‡ŒæŠŠå‘½ä»¤è´´å‡ºæ¥ä»¥ä¾›å‚è€ƒã€‚


```python
# !mkdir -p ../../Data/TinyStoriesChinese/raw_data/train
# !mkdir -p ../../Data/TinyStoriesChinese/raw_data/val
# !mkdir -p ../../Data/TinyStoriesChinese/processed_data/train
# !mkdir -p ../../Data/TinyStoriesChinese/processed_data/val

# !tar zxvf ../../Data/TinyStoriesChinese/TinyStories_all_data_zh.tar.gz -C ../../Data/TinyStoriesChinese/raw_data/train
```

æˆ‘æŠŠæœ€åä¸€ä¸ªæ–‡ä»¶`data47_zh.jsonl`ï¼ˆå…±è®¡78538è¡Œï¼‰é‡Œåˆ‡åˆ†å‡ºæ¥4wè¡Œä½œä¸º`eval`æ•°æ®ã€‚


```python
# !mv ../../Data/TinyStoriesChinese/raw_data/train/data47_zh.jsonl ../../Data/TinyStoriesChinese/raw_data/val/
# !head -n 40000 ../../Data/TinyStoriesChinese/raw_data/val/data47_zh.jsonl > ../../Data/TinyStoriesChinese/raw_data/val/val.jsonl
# !tail -n +40000 ../../Data/TinyStoriesChinese/raw_data/val/data47_zh.jsonl > ../../Data/TinyStoriesChinese/raw_data/train/data47_zh.jsonl
# !rm ../../Data/TinyStoriesChinese/raw_data/val/data47_zh.jsonl
```

### å…ˆçœ‹ä¸€æ¡æ•°æ®
ï¼ˆéƒ½æ‰“å°å‡ºæ¥å¤ªé•¿äº†ï¼Œæ‰€ä»¥åªè¾“å‡ºå‰100ä¸ªå­—ç¬¦ï¼‰


```python
import json

with open("../../Data/TinyStoriesChinese/raw_data/train/data00_zh.jsonl", "r") as f:
    for line in f.readlines():
        js = json.loads(line)
        print(js["story_zh"][:100])
        break
```
```
è‰è‰å’Œæœ¬æ˜¯æœ‹å‹ã€‚ä»–ä»¬å–œæ¬¢åœ¨å…¬å›­é‡Œç©ã€‚æœ‰ä¸€å¤©ï¼Œä»–ä»¬åœ¨ä¸€æ£µå¤§æ ‘ä¸‹çœ‹åˆ°äº†ä¸€ä¸ªç§‹åƒã€‚è‰è‰æƒ³è¯•è¯•é‚£ä¸ªç§‹åƒã€‚å¥¹è·‘åˆ°æ ‘ä¸‹ï¼Œçˆ¬ä¸Šäº†ç§‹åƒã€‚
"æ¨æˆ‘ï¼Œæœ¬ï¼"å¥¹è¯´ã€‚æœ¬è½»è½»åœ°æ¨äº†å¥¹ä¸€ä¸‹ã€‚è‰è‰æ„Ÿåˆ°å¾ˆå¼€å¿ƒã€‚å¥¹è¶Šè¡è¶Šé«˜ï¼Œç¬‘ç€å–Šå«ã€‚
```


### é€‚é…æ¡†æ¶API

ç”±äºé€‰æ‹©äº†ä½¿ç”¨[âš¡ï¸litgpt](https://github.com/Lightning-AI/litgpt/tree/main)æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œæ‰€ä»¥éœ€è¦å¼•å…¥æ¡†æ¶ç›¸å…³çš„`Class`å’Œ`API`æ¥å°è£…æˆ‘ä»¬çš„æ•°æ®å‡†å¤‡é€»è¾‘ã€‚

è¿™é‡Œæˆ‘ä»¬å¯ä»¥å‚è€ƒ[æºç é‡Œé›†æˆçš„Tinyllamaçš„æ•°æ®é¢„å¤„ç†ä»£ç ](https://github.com/Lightning-AI/litgpt/blob/main/litgpt/data/prepare_slimpajama.py)é‡Œçš„ä»£ç ï¼Œç¨ä½œä¿®æ”¹ã€‚

ä¸»è¦æ˜¯éœ€è¦å°†**Day02**é‡Œçš„`line`å¤„ç†é€»è¾‘å°è£…åˆ°`ligtgpt`çš„`API`ä¸­ã€‚

ä½†åœ¨æ­¤ä¹‹å‰æˆ‘ä»¬å…ˆç†Ÿæ‚‰ä¸€ä¸‹`litgpt`çš„Tokenizerçš„ä½¿ç”¨æ–¹æ³•ï¼š

å…ˆå®‰è£…ä¸€ä¸‹`litgpt`ä»¥åŠå®ƒæ‰€ä»¥èµ–çš„`litdata`:


```python
# !pip install litgpt
# !pip install litdata
```


```python
import torch
from litgpt import Tokenizer

litgpt_tokenizer = Tokenizer("../../References/chatglm3-6b")
```

è¿™é‡Œä¹Ÿå®éªŒäº†ä¸€ä¸‹ç»“æœï¼Œå¯¹æ¯”å‘ç°å’Œå’±ä»¬ä¹‹å‰**Day02**é‡Œç”¨åŸç”Ÿ`Tokenizer`å¤„ç†çš„**ç»“æœä¸€è‡´**ã€‚

ç»“æœè¿™é‡Œå°±ä¸è´´å‡ºæ¥äº†ï¼Œæœ‰å…´è¶£çš„å¯ä»¥è‡ªå·±è¯•ä¸€ä¸‹ã€‚

> âš ï¸ä¸è¿‡éœ€è¦æ³¨æ„`litgpt`çš„`Tokenizer.encode`è¿”å›çš„æ˜¯ä¸€ä¸ª`torch`çš„`Tensor`


```python
import numpy as np

litgpt_encoded = litgpt_tokenizer.encode(
    json.loads(line)["story_zh"][:100], eos=True
)  # è®°å¾—è®¾ç½®eos=True
print(litgpt_encoded)
# print(np.array(litgpt_encoded, dtype=np.uint16))
print(litgpt_tokenizer.decode(litgpt_encoded))
```
```
tensor([30910, 56623, 56623, 54542, 50154, 31761, 31155, 31633, 31815, 54534,
        32693, 54662, 55409, 31155, 35632, 31123, 31633, 34383, 57427, 47658,
        54578, 34518, 31623, 55567, 55226, 31155, 56623, 56623, 54695, 39887,
        32437, 55567, 55226, 31155, 54790, 41309, 52624, 31123, 56856, 32660,
        55567, 55226, 31155,    13, 30955, 54834, 54546, 31123, 54613, 31404,
        30955, 36213, 31155, 54613, 36660, 54563, 54834, 43881, 32024, 31155,
        56623, 56623, 32707, 54657, 33436, 31155, 54790, 54937, 56567, 40714,
        31123, 38502, 56653, 55483, 31155,     2], dtype=torch.int32)
è‰è‰å’Œæœ¬æ˜¯æœ‹å‹ã€‚ä»–ä»¬å–œæ¬¢åœ¨å…¬å›­é‡Œç©ã€‚æœ‰ä¸€å¤©ï¼Œä»–ä»¬åœ¨ä¸€æ£µå¤§æ ‘ä¸‹çœ‹åˆ°äº†ä¸€ä¸ªç§‹åƒã€‚è‰è‰æƒ³è¯•è¯•é‚£ä¸ªç§‹åƒã€‚å¥¹è·‘åˆ°æ ‘ä¸‹ï¼Œçˆ¬ä¸Šäº†ç§‹åƒã€‚
"æ¨æˆ‘ï¼Œæœ¬ï¼"å¥¹è¯´ã€‚æœ¬è½»è½»åœ°æ¨äº†å¥¹ä¸€ä¸‹ã€‚è‰è‰æ„Ÿåˆ°å¾ˆå¼€å¿ƒã€‚å¥¹è¶Šè¡è¶Šé«˜ï¼Œç¬‘ç€å–Šå«ã€‚
```


### æ•°æ®å¤„ç†ä»£ç 
æ•°æ®å¤„ç†ç›´æ¥å‚è€ƒäº†ä¸Šé¢ç»™å‡ºçš„[litgpt samples](https://github.com/Lightning-AI/litgpt/blob/main/litgpt/data/prepare_slimpajama.py)ï¼Œæˆ‘ä»¬éœ€è¦ä»¿ç…§`prepare_slimpajama.py`å®ç°é‡Œé¢ç›¸å…³å‡½æ•°ï¼ˆä¹‹å‰**Day 02**é‡Œå®ç°çš„å‡½æ•°éœ€è¦ç¨åŠ æ”¹é€ ä¸€ä¸‹ï¼‰ã€‚


```python
# Copyright Lightning AI. Licensed under the Apache License 2.0, see LICENSE file.

import json
import os
import time
import numpy as np
from pathlib import Path

from litgpt.tokenizer import Tokenizer
from litgpt.data.prepare_starcoder import DataChunkRecipe
from litdata import TokensLoader
from litgpt.utils import extend_checkpoint_dir


class TinyStoriesZhDataRecipe(DataChunkRecipe):
    is_generator = True

    def __init__(self, tokenizer: Tokenizer, chunk_size: int):
        super().__init__(chunk_size)
        self.tokenizer = tokenizer

    def prepare_structure(self, input_dir):
        files = Path(input_dir).rglob("*.jsonl")
        return [str(file) for file in files]

    def prepare_item(self, filepath):

        with open(filepath, "rb") as f:
            for line in f.readlines():
                js = json.loads(line)
                story = js["story_zh"]
                # æ³¨æ„è¿™é‡Œè¦æ·»åŠ eos
                # è¿˜è®°å¾—å—ï¼šæˆ‘ä»¬çš„vocab sizeåœ¨int16èŒƒå›´å†…ï¼Œæ‰€ä»¥å¯ä»¥è½¬æ¢ä¸ºuint16æ¥èŠ‚çœå†…å­˜
                # story_ids = np.array(
                #     self.tokenizer.encode(story, eos=True), dtype=np.uint16
                # )
                # å¾ˆé—æ†¾ï¼Œå®é™…ä½¿ç”¨çš„æ—¶å€™å‘ç°å¦‚æœæŒ‰ç…§ä¸Šé¢è¿™æ ·å†™ï¼Œ
                # litdataååºåˆ—åŒ–æ•°æ®çš„æ—¶å€™ä¼šé”™è¯¯åœ°å¾—åˆ°torch.int64ä¸”è¶…ç•Œçš„Tensorï¼Œ
                # ä½†ç›´æ¥å­˜torch.Tensoræ²¡é—®é¢˜ï¼ˆåŠ ä¸Šlitdataä¸æ”¯æŒtorch.uint16ï¼‰ï¼Œ
                # æ‰€ä»¥æœ€åå®é™…ä½¿ç”¨çš„æ—¶å€™è¿˜æ˜¯ç”¨ä¸‹é¢è¿™ç§å†™æ³•
                story_ids = self.tokenizer.encode(story, eos=True)
                yield story_ids


def prepare(
    input_dir: Path = Path("../../Data/TinyStoriesChinese/raw_data/train"),
    output_dir: Path = Path("../../Data/TinyStoriesChinese/processed_data/train"),
    tokenizer_path: Path = Path("../../References/chatglm3-6b"),
    chunk_size: int = (2049 * 8012),
    fast_dev_run: bool = False,
) -> None:
    from litdata.processing.data_processor import DataProcessor

    tokenizer_path = extend_checkpoint_dir(tokenizer_path)
    tokenizer = Tokenizer(tokenizer_path)
    data_recipe = TinyStoriesZhDataRecipe(tokenizer=tokenizer, chunk_size=chunk_size)
    data_processor = DataProcessor(
        input_dir=str(input_dir),
        output_dir=str(output_dir),
        fast_dev_run=fast_dev_run,
        num_workers=os.cpu_count(),
        num_downloaders=1,
        # è¿™é‡Œæœ‰ä¸ªã€Œå·¨å‘ã€ï¼Œå¦‚æœä¸åŠ è¿™ä¸€è¡Œï¼Œå¤„ç†å¥½çš„æ•°æ®é…å¯¹çš„index.jsoné‡Œ
        # æœ‰ä¸€ä¸ªåä¸º"dim"çš„keyå€¼ä¼šä¸ºnullï¼Œå¯¼è‡´åç»­æœ‰ä¸€ä¸ªæ— æ³•è§„é¿çš„æŠ¥é”™
        # ä½†æ˜¯å®˜æ–¹çš„ä¾‹å­é‡Œæ˜¯æ²¡æœ‰è¿™ä¸€è¡Œçš„ï¼Œå¾ˆå¥‡æ€ªä¸ºä½•ä¼šæœ‰è¿™ä¸ªé—®é¢˜
        item_loader=TokensLoader(),
    )

    start_time = time.time()
    data_processor.run(data_recipe)
    elapsed_time = time.time() - start_time
    print(f"Time taken: {elapsed_time:.2f} seconds")
```

é¦–å…ˆï¼Œæˆ‘è¿™é‡Œä¸»è¦å°±æ˜¯æŠŠä¹‹å‰å®ç°çš„`line`å¤„ç†é€»è¾‘å°è£…åˆ°`litgpt`çš„`DataChunkRecipe`ä¸­ï¼š
- `prepare_structure`å‡½æ•°ç»™å®šè·¯å¾„è¿”å›ç¬¦åˆæˆ‘ä»¬æœŸæœ›çš„æ•°æ®æ–‡ä»¶çš„è·¯å¾„åˆ—è¡¨
- `prepare_item`å‡½æ•°ç»™å®šä¸€ä¸ªä¸Šé¢çš„æ•°æ®æ–‡ä»¶çš„è·¯å¾„ï¼Œæ ¹æ®æˆ‘ä»¬**è‡ªå®šä¹‰**çš„`tokenization`å¤„ç†é€»è¾‘è¿”å›ä¸€ä¸ª`np.array`å¯¹è±¡
  
ç„¶åï¼Œå®šä¹‰äº†ä¸€ä¸ª`prepare`å‡½æ•°ï¼ŒæŒ‡å®šæˆ‘ä»¬æ•°æ®çš„è¾“å…¥è·¯å¾„å’Œè¾“å‡ºè·¯å¾„ä»¥åŠä¸€äº›å…¶å®ƒå‚æ•°é…ç½®ï¼ˆå…¶å®ç”¨é»˜è®¤çš„å³å¯ï¼‰ï¼Œå…¶ä½™çš„éƒ½äº¤ç»™äº†`litdata`çš„`DataProcessor`ï¼Œå®ƒåŸºäºæˆ‘å‰é¢å®šä¹‰çš„`DataChunkRecipe`æ¥å¤„ç†æ•°æ®ã€‚

æ„Ÿå…´è¶£çš„å¯ä»¥çœ‹çœ‹`DataProcessor`çš„æºç ï¼Œé‡Œé¢åšäº†å¾ˆå¤šå¹¶è¡Œä¹‹ç±»çš„æ•°æ®å¤„ç†ä¼˜åŒ–ã€‚

#### å…ˆç”¨evalæ•°æ®é›†æµ‹è¯•


```python
prepare(
    input_dir=Path("../../Data/TinyStoriesChinese/raw_data/val"),
    output_dir=Path("../../Data/TinyStoriesChinese/processed_data/val"),
    tokenizer_path=Path("../../References/chatglm3-6b"),
)
```

ï¼ˆä¹Ÿå¯ä»¥è®¾ç½®`fast_dev_run=True`æ¥å¤„ç†æ›´å°‘çš„æ•°æ®ï¼Œå°¤å…¶æ˜¯debugæ—¶ååˆ†æœ‰ç”¨ï¼‰

æ‰§è¡Œå®Œå¯ä»¥åœ¨`processed_data/eval`ç›®å½•ä¸‹çœ‹åˆ°ç”Ÿæˆçš„`.bin`æ–‡ä»¶ä»¥åŠè®°å½•äº†æ¯ä¸ª`chunk`æ–‡ä»¶ä¿¡æ¯çš„`index.json`ã€‚

æ¯”è¾ƒä¸€ä¸‹å¯ä»¥å‘ç°ä»åŸå…ˆçš„`83m`çš„`.jsonl`æ–‡ä»¶å‹ç¼©åˆ°äº†`13m`çš„`.bin`ï¼Œå‹ç¼©æ¯”ï¼ˆ83/13â‰ˆ6.385ï¼‰è¿˜æ˜¯å¾ˆå¯è§‚çš„ã€‚

#### å¤„ç†trainæ•°æ®é›†
åœ¨32æ ¸çš„CPUä¸Šå¤„ç†`train`æ•°æ®é›†è€—æ—¶ä¸åˆ°`1min`ã€‚


```python
prepare(
    input_dir=Path("../../Data/TinyStoriesChinese/raw_data/train"),
    output_dir=Path("../../Data/TinyStoriesChinese/processed_data/train"),
    tokenizer_path=Path("../../References/chatglm3-6b"),
)
```

## å°ç»“

1. æ•°æ®é¢„å¤„ç†çš„é€»è¾‘ä¸»è¦æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—åºåˆ—ï¼Œä»¥ä¾¿äºæ¨¡å‹å¤„ç†ã€‚
2. é€šè¿‡`litgpt`çš„`Tokenizer`å¯ä»¥æ–¹ä¾¿çš„å®ç°æ–‡æœ¬åˆ°æ•°å­—åºåˆ—çš„è½¬æ¢ã€‚
3. `litdata`æä¾›äº†æ•°æ®å¤„ç†çš„`API`ï¼Œå¯ä»¥æ–¹ä¾¿çš„å°è£…æˆ‘ä»¬çš„æ•°æ®å¤„ç†é€»è¾‘ã€‚
4. åŸºäºä¸Šé¢çš„å¼€å‘ï¼Œå°†`TinyStoriesChinese`æ•°æ®é›†åšäº†æ•°æ®åˆ’åˆ†å¹¶å®Œæˆäº†é¢„å¤„ç†ã€‚
