{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零手搓中文大模型｜🚀Day03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然省略了数据清洗的逻辑，但是我们还是需要对数据进行预处理，以便于后续的模型训练。\n",
    "\n",
    "包括以下两个细节：\n",
    "\n",
    "1. 在每个文本后添加`eos`标记，以便于模型识别句子的结束。\n",
    "2. 将文本转换为`数字序列`，以便于模型处理。\n",
    "   \n",
    "   这一步其实也可以放到模型训练的时候进行，但提前处理可以减少训练时的计算量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集划分\n",
    "\n",
    "解压数据集，得到`48`个jsonl文件，共计`3952863`行json数据。\n",
    "\n",
    "我之前已经解压过了，这里把命令贴出来做个记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !mkdir -p ../../Data/TinyStoriesChinese/raw_data/train\n",
    "# !mkdir -p ../../Data/TinyStoriesChinese/raw_data/val\n",
    "# !mkdir -p ../../Data/TinyStoriesChinese/processed_data/train\n",
    "# !mkdir -p ../../Data/TinyStoriesChinese/processed_data/val\n",
    "\n",
    "# !tar zxvf ../../Data/TinyStoriesChinese/TinyStories_all_data_zh.tar.gz -C ../../Data/TinyStoriesChinese/raw_data/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我把最后一个文件`data47_zh.jsonl`（共计78538行）里切分出来4w行作为`eval`数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !mv ../../Data/TinyStoriesChinese/raw_data/train/data47_zh.jsonl ../../Data/TinyStoriesChinese/raw_data/eval/\n",
    "# !head -n 40000 ../../Data/TinyStoriesChinese/raw_data/eval/data47_zh.jsonl > ../../Data/TinyStoriesChinese/raw_data/eval/eval.jsonl\n",
    "# !tail -n +40000 ../../Data/TinyStoriesChinese/raw_data/eval/data47_zh.jsonl > ../../Data/TinyStoriesChinese/raw_data/train/data47_zh.jsonl\n",
    "# !rm ../../Data/TinyStoriesChinese/raw_data/eval/data47_zh.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先看一条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "莉莉和本是朋友。他们喜欢在公园里玩。有一天，他们在一棵大树下看到了一个秋千。莉莉想试试那个秋千。她跑到树下，爬上了秋千。\n",
      "\"推我，本！\"她说。本轻轻地推了她一下。莉莉感到很开心。她越荡越高，笑着喊叫。\n",
      "本看着莉莉。他觉得她很可爱。他也想荡秋千。他在莉莉停下来之后等着。但是莉莉没有停下来。她越荡越快。她玩得太高兴了。\n",
      "\"我也可以荡秋千吗，莉莉？\"本问。莉莉没听到他的话。她忙着荡秋千。本觉得很难过。他走开了。\n",
      "莉莉荡得太高，失去了平衡。她从秋千上摔下来，落在地上。她扭伤了脚。她哭了起来。\n",
      "\"哎呀，哎呀，哎呀！\"她说。她在找本。她希望他能帮助她。但本不在那里。他走了。\n",
      "莉莉感到很抱歉。她希望她能和本分享秋千。她希望他在那里拥抱她。她一瘸一拐地走到树下。她看到有什么东西挂在树枝上。那是本的帽子。他留给她的。\n",
      "莉莉笑了。她觉得本很好。她戴上了他的帽子。她希望他会回来。她想道歉。她想再次成为朋友。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../../Data/TinyStoriesChinese/raw_data/train/data00_zh.jsonl\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        js = json.loads(line)\n",
    "        print(js[\"story_zh\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 适配框架API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于选择了使用[⚡️litgpt](https://github.com/Lightning-AI/litgpt/tree/main)框架进行训练，所以需要引入框架相关的`Class`和`API`来封装我们的数据准备逻辑。\n",
    "\n",
    "这里我们可以参考[源码里集成的Tinyllama的数据预处理代码](https://github.com/Lightning-AI/litgpt/blob/main/litgpt/data/prepare_slimpajama.py)里的代码，稍作修改。\n",
    "\n",
    "主要是需要将[Day02](../Day02/Day02.ipynb)里的`line`处理逻辑封装到`ligtgpt`的`API`中。\n",
    "\n",
    "但在此之前我们先熟悉一下`litgpt`的Tokenizer的使用方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from litgpt import Tokenizer\n",
    "\n",
    "litgpt_tokenizer = Tokenizer(\"../../References/chatglm3-6b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里也实验了一下结果，对比发现和上面咱们之前用原生Tokenizer处理的**结果一致**。\n",
    "\n",
    "（不过需要注意litgpt的`Tokenizer.encode`返回的是一个torch的`Tensor`）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([30910, 56623, 56623, 54542, 50154, 31761, 31155, 31633, 31815, 54534,\n",
      "        32693, 54662, 55409, 31155, 35632, 31123, 31633, 34383, 57427, 47658,\n",
      "        54578, 34518, 31623, 55567, 55226, 31155, 56623, 56623, 54695, 39887,\n",
      "        32437, 55567, 55226, 31155, 54790, 41309, 52624, 31123, 56856, 32660,\n",
      "        55567, 55226, 31155,    13, 30955, 54834, 54546, 31123, 54613, 31404,\n",
      "        30955, 36213, 31155, 54613, 36660, 54563, 54834, 43881, 32024, 31155,\n",
      "        56623, 56623, 32707, 54657, 33436, 31155, 54790, 54937, 56567, 40714,\n",
      "        31123, 38502, 56653, 55483, 31155,    13, 54613, 32984, 56623, 56623,\n",
      "        31155, 54572, 31897, 54790, 54657, 35245, 31155, 36551, 54695, 56567,\n",
      "        55567, 55226, 31155, 33152, 56623, 56623, 51556, 31797, 39055, 31155,\n",
      "        31694, 56623, 56623, 31631, 51556, 31155, 54790, 54937, 56567, 54937,\n",
      "        54929, 31155, 54790, 55409, 40915, 34492, 54537, 31155,    13, 30955,\n",
      "        54546, 32591, 56567, 55567, 55226, 55398, 31123, 56623, 56623, 31514,\n",
      "        30955, 54613, 54761, 31155, 56623, 56623, 54721, 33906, 31804, 54887,\n",
      "        31155, 54790, 46977, 56567, 55567, 55226, 31155, 54613, 31897, 32960,\n",
      "        54597, 31155, 54572, 54942, 34675, 31155,    13, 56623, 56623, 56567,\n",
      "        40915, 54589, 31123, 36467, 33501, 31155, 54790, 54708, 55567, 55226,\n",
      "        54547, 57456, 32246, 31123, 36712, 34245, 31155, 54790, 56901, 55328,\n",
      "        54537, 55673, 31155, 54790, 56399, 37247, 31155,    13, 30955, 58394,\n",
      "        56657, 31123, 58394, 56657, 31123, 58394, 56657, 31404, 30955, 36213,\n",
      "        31155, 35957, 55227, 54613, 31155, 54790, 31772, 47554, 31934, 54790,\n",
      "        31155, 54688, 54613, 33551, 33892, 31155, 54572, 34247, 31155,    13,\n",
      "        56623, 56623, 32707, 54657, 52992, 31155, 54790, 31772, 54790, 54558,\n",
      "        54542, 54613, 32097, 55567, 55226, 31155, 54790, 31772, 33152, 33892,\n",
      "        37322, 54790, 31155, 54790, 54531, 60337, 54531, 57635, 54563, 35220,\n",
      "        52624, 31155, 54790, 31857, 33277, 32086, 44829, 49102, 54547, 31155,\n",
      "        35328, 43352, 41147, 31155, 54572, 42393, 32233, 31155,    13, 56623,\n",
      "        56623, 40466, 31155, 54790, 31897, 54613, 33058, 31155, 54790, 55947,\n",
      "        32660, 31804, 41147, 31155, 54790, 31772, 38711, 33857, 31155, 54790,\n",
      "        54695, 37300, 31155, 54790, 54695, 32462, 31705, 31761, 31155,     2],\n",
      "       dtype=torch.int32)\n",
      "莉莉和本是朋友。他们喜欢在公园里玩。有一天，他们在一棵大树下看到了一个秋千。莉莉想试试那个秋千。她跑到树下，爬上了秋千。\n",
      "\"推我，本！\"她说。本轻轻地推了她一下。莉莉感到很开心。她越荡越高，笑着喊叫。\n",
      "本看着莉莉。他觉得她很可爱。他也想荡秋千。他在莉莉停下来之后等着。但是莉莉没有停下来。她越荡越快。她玩得太高兴了。\n",
      "\"我也可以荡秋千吗，莉莉？\"本问。莉莉没听到他的话。她忙着荡秋千。本觉得很难过。他走开了。\n",
      "莉莉荡得太高，失去了平衡。她从秋千上摔下来，落在地上。她扭伤了脚。她哭了起来。\n",
      "\"哎呀，哎呀，哎呀！\"她说。她在找本。她希望他能帮助她。但本不在那里。他走了。\n",
      "莉莉感到很抱歉。她希望她能和本分享秋千。她希望他在那里拥抱她。她一瘸一拐地走到树下。她看到有什么东西挂在树枝上。那是本的帽子。他留给她的。\n",
      "莉莉笑了。她觉得本很好。她戴上了他的帽子。她希望他会回来。她想道歉。她想再次成为朋友。\n"
     ]
    }
   ],
   "source": [
    "litgpt_encoded = litgpt_tokenizer.encode(\n",
    "    json.loads(line)[\"story_zh\"], eos=True\n",
    ")  # 记得设置eos=True\n",
    "print(litgpt_encoded)\n",
    "print(litgpt_tokenizer.decode(litgpt_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据处理参考上面给出的[链接](https://github.com/Lightning-AI/litgpt/blob/main/litgpt/data/prepare_slimpajama.py)，我们需要实现`prepare_slimpajama.py`里的相关函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Lightning AI. Licensed under the Apache License 2.0, see LICENSE file.\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from litgpt.tokenizer import Tokenizer\n",
    "from litgpt.data.prepare_starcoder import DataChunkRecipe\n",
    "from litgpt.utils import extend_checkpoint_dir\n",
    "\n",
    "\n",
    "class TinyStoriesZhDataRecipe(DataChunkRecipe):\n",
    "    is_generator = True\n",
    "\n",
    "    def __init__(self, tokenizer: Tokenizer, chunk_size: int):\n",
    "        super().__init__(chunk_size)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def prepare_structure(self, input_dir):\n",
    "        files = Path(input_dir).rglob(\"*.jsonl\")\n",
    "        return [str(file) for file in files]\n",
    "\n",
    "    def prepare_item(self, filepath):\n",
    "\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            for line in f.readlines():\n",
    "                js = json.loads(line)\n",
    "                story = js[\"story_zh\"]\n",
    "                # 注意这里要添加eos\n",
    "                # 另外还记得吗：我们的vocab size在int16范围内，所以可以转换为uint16来节省内存\n",
    "                story_ids = np.array(\n",
    "                    self.tokenizer.encode(story, eos=True), dtype=np.uint16\n",
    "                )\n",
    "                yield story_ids\n",
    "\n",
    "\n",
    "def prepare(\n",
    "    input_dir: Path = Path(\"../../Data/TinyStoriesChinese/raw_data/train\"),\n",
    "    output_dir: Path = Path(\"../../Data/TinyStoriesChinese/processed_data/train\"),\n",
    "    tokenizer_path: Path = Path(\"../../References/chatglm3-6b\"),\n",
    "    chunk_size: int = (2049 * 16384),\n",
    "    fast_dev_run: bool = False,\n",
    ") -> None:\n",
    "    from litdata.processing.data_processor import DataProcessor\n",
    "\n",
    "    tokenizer_path = extend_checkpoint_dir(tokenizer_path)\n",
    "    tokenizer = Tokenizer(tokenizer_path)\n",
    "    data_recipe = TinyStoriesZhDataRecipe(tokenizer=tokenizer, chunk_size=chunk_size)\n",
    "    data_processor = DataProcessor(\n",
    "        input_dir=str(input_dir),\n",
    "        output_dir=str(output_dir),\n",
    "        fast_dev_run=fast_dev_run,\n",
    "        num_workers=os.cpu_count(),\n",
    "        num_downloaders=1,\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    data_processor.run(data_recipe)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 先用eval数据集测试\n",
    "\n",
    "（也可以设置`fast_dev_run=True`来处理更少的数据，尤其是debug时）\n",
    "\n",
    "执行完可以在`processed_data/eval`目录下看到生成的chunk文件。\n",
    "\n",
    "比较一下可以发现从原先的`83m`的`.jsonl`文件压缩到了`13m`的`.bin`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare(\n",
    "    input_dir=Path(\"../../Data/TinyStoriesChinese/raw_data/eval\"),\n",
    "    output_dir=Path(\"../../Data/TinyStoriesChinese/processed_data/eval\"),\n",
    "    tokenizer_path=Path(\"../../References/chatglm3-6b\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理train数据集\n",
    "在32核的CPU上处理`train`数据集耗时不到`1min`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare(\n",
    "    input_dir=Path(\"../../Data/TinyStoriesChinese/raw_data/train\"),\n",
    "    output_dir=Path(\"../../Data/TinyStoriesChinese/processed_data/train\"),\n",
    "    tokenizer_path=Path(\"../../References/chatglm3-6b\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "1. 数据预处理的逻辑主要是将文本转换为数字序列，以便于模型处理。\n",
    "2. 通过litgpt的Tokenizer可以方便的实现文本到数字序列的转换。\n",
    "3. litgpt提供了数据处理的API，可以方便的封装我们的数据处理逻辑。\n",
    "4. 数据处理的结果可以通过压缩文件的方式减少存储空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
