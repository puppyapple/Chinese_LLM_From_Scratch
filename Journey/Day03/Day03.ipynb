{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä»é›¶æ‰‹æ“ä¸­æ–‡å¤§æ¨¡å‹ï½œğŸš€Day03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•°æ®é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è™½ç„¶çœç•¥äº†æ•°æ®æ¸…æ´—çš„é€»è¾‘ï¼Œä½†æ˜¯æˆ‘ä»¬è¿˜æ˜¯éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œä»¥ä¾¿äºåç»­çš„æ¨¡å‹è®­ç»ƒã€‚\n",
    "\n",
    "åŒ…æ‹¬ä»¥ä¸‹ä¸¤ä¸ªç»†èŠ‚ï¼š\n",
    "\n",
    "1. åœ¨æ¯ä¸ªæ–‡æœ¬åæ·»åŠ `eos`æ ‡è®°ï¼Œä»¥ä¾¿äºæ¨¡å‹è¯†åˆ«å¥å­çš„ç»“æŸã€‚\n",
    "2. å°†æ–‡æœ¬è½¬æ¢ä¸º`æ•°å­—åºåˆ—`ï¼Œä»¥ä¾¿äºæ¨¡å‹å¤„ç†ã€‚\n",
    "   \n",
    "   è¿™ä¸€æ­¥å…¶å®ä¹Ÿå¯ä»¥æ”¾åˆ°æ¨¡å‹è®­ç»ƒçš„æ—¶å€™è¿›è¡Œï¼Œä½†æå‰å¤„ç†å¯ä»¥å‡å°‘è®­ç»ƒæ—¶çš„è®¡ç®—é‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®é›†åˆ’åˆ†\n",
    "\n",
    "è§£å‹æ•°æ®é›†ï¼Œå¾—åˆ°`48`ä¸ªjsonlæ–‡ä»¶ï¼Œå…±è®¡`3952863`è¡Œjsonæ•°æ®ã€‚\n",
    "\n",
    "æˆ‘ä¹‹å‰å·²ç»è§£å‹è¿‡äº†ï¼Œå¹¶ä¸”å°†åŸå§‹æ•°æ®å’Œå¤„ç†è¿‡åçš„æ•°æ®åˆ†åˆ«å­˜åœ¨äº†ä¸åŒè·¯å¾„ä¸‹ã€‚\n",
    "\n",
    "è¿™é‡ŒæŠŠå‘½ä»¤è´´å‡ºæ¥ä»¥ä¾›å‚è€ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !mkdir -p ../../Data/TinyStoriesChinese/raw_data/train\n",
    "# !mkdir -p ../../Data/TinyStoriesChinese/raw_data/val\n",
    "# !mkdir -p ../../Data/TinyStoriesChinese/processed_data/train\n",
    "# !mkdir -p ../../Data/TinyStoriesChinese/processed_data/val\n",
    "\n",
    "# !tar zxvf ../../Data/TinyStoriesChinese/TinyStories_all_data_zh.tar.gz -C ../../Data/TinyStoriesChinese/raw_data/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘æŠŠæœ€åä¸€ä¸ªæ–‡ä»¶`data47_zh.jsonl`ï¼ˆå…±è®¡78538è¡Œï¼‰é‡Œåˆ‡åˆ†å‡ºæ¥4wè¡Œä½œä¸º`eval`æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !mv ../../Data/TinyStoriesChinese/raw_data/train/data47_zh.jsonl ../../Data/TinyStoriesChinese/raw_data/val/\n",
    "# !head -n 40000 ../../Data/TinyStoriesChinese/raw_data/val/data47_zh.jsonl > ../../Data/TinyStoriesChinese/raw_data/val/val.jsonl\n",
    "# !tail -n +40000 ../../Data/TinyStoriesChinese/raw_data/val/data47_zh.jsonl > ../../Data/TinyStoriesChinese/raw_data/train/data47_zh.jsonl\n",
    "# !rm ../../Data/TinyStoriesChinese/raw_data/val/data47_zh.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å…ˆçœ‹ä¸€æ¡æ•°æ®\n",
    "ï¼ˆéƒ½æ‰“å°å‡ºæ¥å¤ªé•¿äº†ï¼Œæ‰€ä»¥åªè¾“å‡ºå‰100ä¸ªå­—ç¬¦ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è‰è‰å’Œæœ¬æ˜¯æœ‹å‹ã€‚ä»–ä»¬å–œæ¬¢åœ¨å…¬å›­é‡Œç©ã€‚æœ‰ä¸€å¤©ï¼Œä»–ä»¬åœ¨ä¸€æ£µå¤§æ ‘ä¸‹çœ‹åˆ°äº†ä¸€ä¸ªç§‹åƒã€‚è‰è‰æƒ³è¯•è¯•é‚£ä¸ªç§‹åƒã€‚å¥¹è·‘åˆ°æ ‘ä¸‹ï¼Œçˆ¬ä¸Šäº†ç§‹åƒã€‚\n",
      "\"æ¨æˆ‘ï¼Œæœ¬ï¼\"å¥¹è¯´ã€‚æœ¬è½»è½»åœ°æ¨äº†å¥¹ä¸€ä¸‹ã€‚è‰è‰æ„Ÿåˆ°å¾ˆå¼€å¿ƒã€‚å¥¹è¶Šè¡è¶Šé«˜ï¼Œç¬‘ç€å–Šå«ã€‚\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../../Data/TinyStoriesChinese/raw_data/train/data00_zh.jsonl\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        js = json.loads(line)\n",
    "        print(js[\"story_zh\"][:100])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é€‚é…æ¡†æ¶API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”±äºé€‰æ‹©äº†ä½¿ç”¨[âš¡ï¸litgpt](https://github.com/Lightning-AI/litgpt/tree/main)æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œæ‰€ä»¥éœ€è¦å¼•å…¥æ¡†æ¶ç›¸å…³çš„`Class`å’Œ`API`æ¥å°è£…æˆ‘ä»¬çš„æ•°æ®å‡†å¤‡é€»è¾‘ã€‚\n",
    "\n",
    "è¿™é‡Œæˆ‘ä»¬å¯ä»¥å‚è€ƒ[æºç é‡Œé›†æˆçš„Tinyllamaçš„æ•°æ®é¢„å¤„ç†ä»£ç ](https://github.com/Lightning-AI/litgpt/blob/main/litgpt/data/prepare_slimpajama.py)é‡Œçš„ä»£ç ï¼Œç¨ä½œä¿®æ”¹ã€‚\n",
    "\n",
    "ä¸»è¦æ˜¯éœ€è¦å°†[Day02](../Day02/Day02.ipynb)é‡Œçš„`line`å¤„ç†é€»è¾‘å°è£…åˆ°`ligtgpt`çš„`API`ä¸­ã€‚\n",
    "\n",
    "ä½†åœ¨æ­¤ä¹‹å‰æˆ‘ä»¬å…ˆç†Ÿæ‚‰ä¸€ä¸‹`litgpt`çš„Tokenizerçš„ä½¿ç”¨æ–¹æ³•ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…ˆå®‰è£…ä¸€ä¸‹`litgpt`ä»¥åŠå®ƒæ‰€ä»¥èµ–çš„`litdata`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install litgpt\n",
    "# !pip install litdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from litgpt import Tokenizer\n",
    "\n",
    "litgpt_tokenizer = Tokenizer(\"../../References/chatglm3-6b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œä¹Ÿå®éªŒäº†ä¸€ä¸‹ç»“æœï¼Œå¯¹æ¯”å‘ç°å’Œå’±ä»¬ä¹‹å‰[Day02](../Day02/Day02.ipynb)é‡Œç”¨åŸç”Ÿ`Tokenizer`å¤„ç†çš„**ç»“æœä¸€è‡´**ã€‚\n",
    "\n",
    "ç»“æœè¿™é‡Œå°±ä¸è´´å‡ºæ¥äº†ï¼Œæœ‰å…´è¶£çš„å¯ä»¥è‡ªå·±è¯•ä¸€ä¸‹ã€‚\n",
    "\n",
    "> âš ï¸ä¸è¿‡éœ€è¦æ³¨æ„`litgpt`çš„`Tokenizer.encode`è¿”å›çš„æ˜¯ä¸€ä¸ª`torch`çš„`Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([30910, 56623, 56623, 54542, 50154, 31761, 31155, 31633, 31815, 54534,\n",
      "        32693, 54662, 55409, 31155, 35632, 31123, 31633, 34383, 57427, 47658,\n",
      "        54578, 34518, 31623, 55567, 55226, 31155, 56623, 56623, 54695, 39887,\n",
      "        32437, 55567, 55226, 31155, 54790, 41309, 52624, 31123, 56856, 32660,\n",
      "        55567, 55226, 31155,    13, 30955, 54834, 54546, 31123, 54613, 31404,\n",
      "        30955, 36213, 31155, 54613, 36660, 54563, 54834, 43881, 32024, 31155,\n",
      "        56623, 56623, 32707, 54657, 33436, 31155, 54790, 54937, 56567, 40714,\n",
      "        31123, 38502, 56653, 55483, 31155,     2], dtype=torch.int32)\n",
      "è‰è‰å’Œæœ¬æ˜¯æœ‹å‹ã€‚ä»–ä»¬å–œæ¬¢åœ¨å…¬å›­é‡Œç©ã€‚æœ‰ä¸€å¤©ï¼Œä»–ä»¬åœ¨ä¸€æ£µå¤§æ ‘ä¸‹çœ‹åˆ°äº†ä¸€ä¸ªç§‹åƒã€‚è‰è‰æƒ³è¯•è¯•é‚£ä¸ªç§‹åƒã€‚å¥¹è·‘åˆ°æ ‘ä¸‹ï¼Œçˆ¬ä¸Šäº†ç§‹åƒã€‚\n",
      "\"æ¨æˆ‘ï¼Œæœ¬ï¼\"å¥¹è¯´ã€‚æœ¬è½»è½»åœ°æ¨äº†å¥¹ä¸€ä¸‹ã€‚è‰è‰æ„Ÿåˆ°å¾ˆå¼€å¿ƒã€‚å¥¹è¶Šè¡è¶Šé«˜ï¼Œç¬‘ç€å–Šå«ã€‚\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "litgpt_encoded = litgpt_tokenizer.encode(\n",
    "    json.loads(line)[\"story_zh\"][:100], eos=True\n",
    ")  # è®°å¾—è®¾ç½®eos=True\n",
    "print(litgpt_encoded)\n",
    "# print(np.array(litgpt_encoded, dtype=np.uint16))\n",
    "print(litgpt_tokenizer.decode(litgpt_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®å¤„ç†ä»£ç \n",
    "æ•°æ®å¤„ç†ç›´æ¥å‚è€ƒäº†ä¸Šé¢ç»™å‡ºçš„[litgpt samples](https://github.com/Lightning-AI/litgpt/blob/main/litgpt/data/prepare_slimpajama.py)ï¼Œæˆ‘ä»¬éœ€è¦ä»¿ç…§`prepare_slimpajama.py`å®ç°é‡Œé¢ç›¸å…³å‡½æ•°ï¼ˆä¹‹å‰**Day 02**é‡Œå®ç°çš„å‡½æ•°éœ€è¦ç¨åŠ æ”¹é€ ä¸€ä¸‹ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Lightning AI. Licensed under the Apache License 2.0, see LICENSE file.\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from litgpt.tokenizer import Tokenizer\n",
    "from litgpt.data.prepare_starcoder import DataChunkRecipe\n",
    "from litdata import TokensLoader\n",
    "from litgpt.utils import extend_checkpoint_dir\n",
    "\n",
    "\n",
    "class TinyStoriesZhDataRecipe(DataChunkRecipe):\n",
    "    is_generator = True\n",
    "\n",
    "    def __init__(self, tokenizer: Tokenizer, chunk_size: int):\n",
    "        super().__init__(chunk_size)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def prepare_structure(self, input_dir):\n",
    "        files = Path(input_dir).rglob(\"*.jsonl\")\n",
    "        return [str(file) for file in files]\n",
    "\n",
    "    def prepare_item(self, filepath):\n",
    "\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            for line in f.readlines():\n",
    "                js = json.loads(line)\n",
    "                story = js[\"story_zh\"]\n",
    "                # æ³¨æ„è¿™é‡Œè¦æ·»åŠ eos\n",
    "                # è¿˜è®°å¾—å—ï¼šæˆ‘ä»¬çš„vocab sizeåœ¨int16èŒƒå›´å†…ï¼Œæ‰€ä»¥å¯ä»¥è½¬æ¢ä¸ºuint16æ¥èŠ‚çœå†…å­˜\n",
    "                # story_ids = np.array(\n",
    "                #     self.tokenizer.encode(story, eos=True), dtype=np.uint16\n",
    "                # )\n",
    "                # å¾ˆé—æ†¾ï¼Œå®é™…ä½¿ç”¨çš„æ—¶å€™å‘ç°å¦‚æœæŒ‰ç…§ä¸Šé¢è¿™æ ·å†™ï¼Œ\n",
    "                # litdataååºåˆ—åŒ–æ•°æ®çš„æ—¶å€™ä¼šé”™è¯¯åœ°å¾—åˆ°torch.int64ä¸”è¶…ç•Œçš„Tensorï¼Œ\n",
    "                # ä½†ç›´æ¥å­˜torch.Tensoræ²¡é—®é¢˜ï¼ˆåŠ ä¸Šlitdataä¸æ”¯æŒtorch.uint16ï¼‰ï¼Œ\n",
    "                # æ‰€ä»¥æœ€åå®é™…ä½¿ç”¨çš„æ—¶å€™è¿˜æ˜¯ç”¨ä¸‹é¢è¿™ç§å†™æ³•\n",
    "                story_ids = self.tokenizer.encode(story, eos=True)\n",
    "                yield story_ids\n",
    "\n",
    "\n",
    "def prepare(\n",
    "    input_dir: Path = Path(\"../../Data/TinyStoriesChinese/raw_data/train\"),\n",
    "    output_dir: Path = Path(\"../../Data/TinyStoriesChinese/processed_data/train\"),\n",
    "    tokenizer_path: Path = Path(\"../../References/chatglm3-6b\"),\n",
    "    chunk_size: int = (2049 * 8012),\n",
    "    fast_dev_run: bool = False,\n",
    ") -> None:\n",
    "    from litdata.processing.data_processor import DataProcessor\n",
    "\n",
    "    tokenizer_path = extend_checkpoint_dir(tokenizer_path)\n",
    "    tokenizer = Tokenizer(tokenizer_path)\n",
    "    data_recipe = TinyStoriesZhDataRecipe(tokenizer=tokenizer, chunk_size=chunk_size)\n",
    "    data_processor = DataProcessor(\n",
    "        input_dir=str(input_dir),\n",
    "        output_dir=str(output_dir),\n",
    "        fast_dev_run=fast_dev_run,\n",
    "        num_workers=os.cpu_count(),\n",
    "        num_downloaders=1,\n",
    "        # è¿™é‡Œæœ‰ä¸ªã€Œå·¨å‘ã€ï¼Œå¦‚æœä¸åŠ è¿™ä¸€è¡Œï¼Œå¤„ç†å¥½çš„æ•°æ®é…å¯¹çš„index.jsoné‡Œ\n",
    "        # æœ‰ä¸€ä¸ªåä¸º\"dim\"çš„keyå€¼ä¼šä¸ºnullï¼Œå¯¼è‡´åç»­æœ‰ä¸€ä¸ªæ— æ³•è§„é¿çš„æŠ¥é”™\n",
    "        # ä½†æ˜¯å®˜æ–¹çš„ä¾‹å­é‡Œæ˜¯æ²¡æœ‰è¿™ä¸€è¡Œçš„ï¼Œå¾ˆå¥‡æ€ªä¸ºä½•ä¼šæœ‰è¿™ä¸ªé—®é¢˜\n",
    "        item_loader=TokensLoader(),\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    data_processor.run(data_recipe)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é¦–å…ˆï¼Œæˆ‘è¿™é‡Œä¸»è¦å°±æ˜¯æŠŠä¹‹å‰å®ç°çš„`line`å¤„ç†é€»è¾‘å°è£…åˆ°`litgpt`çš„`DataChunkRecipe`ä¸­ï¼š\n",
    "- `prepare_structure`å‡½æ•°ç»™å®šè·¯å¾„è¿”å›ç¬¦åˆæˆ‘ä»¬æœŸæœ›çš„æ•°æ®æ–‡ä»¶çš„è·¯å¾„åˆ—è¡¨\n",
    "- `prepare_item`å‡½æ•°ç»™å®šä¸€ä¸ªä¸Šé¢çš„æ•°æ®æ–‡ä»¶çš„è·¯å¾„ï¼Œæ ¹æ®æˆ‘ä»¬**è‡ªå®šä¹‰**çš„`tokenization`å¤„ç†é€»è¾‘è¿”å›ä¸€ä¸ª`np.array`å¯¹è±¡\n",
    "  \n",
    "ç„¶åï¼Œå®šä¹‰äº†ä¸€ä¸ª`prepare`å‡½æ•°ï¼ŒæŒ‡å®šæˆ‘ä»¬æ•°æ®çš„è¾“å…¥è·¯å¾„å’Œè¾“å‡ºè·¯å¾„ä»¥åŠä¸€äº›å…¶å®ƒå‚æ•°é…ç½®ï¼ˆå…¶å®ç”¨é»˜è®¤çš„å³å¯ï¼‰ï¼Œå…¶ä½™çš„éƒ½äº¤ç»™äº†`litdata`çš„`DataProcessor`ï¼Œå®ƒåŸºäºæˆ‘å‰é¢å®šä¹‰çš„`DataChunkRecipe`æ¥å¤„ç†æ•°æ®ã€‚\n",
    "\n",
    "æ„Ÿå…´è¶£çš„å¯ä»¥çœ‹çœ‹`DataProcessor`çš„æºç ï¼Œé‡Œé¢åšäº†å¾ˆå¤šå¹¶è¡Œä¹‹ç±»çš„æ•°æ®å¤„ç†ä¼˜åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å…ˆç”¨evalæ•°æ®é›†æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare(\n",
    "    input_dir=Path(\"../../Data/TinyStoriesChinese/raw_data/val\"),\n",
    "    output_dir=Path(\"../../Data/TinyStoriesChinese/processed_data/val\"),\n",
    "    tokenizer_path=Path(\"../../References/chatglm3-6b\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ï¼ˆä¹Ÿå¯ä»¥è®¾ç½®`fast_dev_run=True`æ¥å¤„ç†æ›´å°‘çš„æ•°æ®ï¼Œå°¤å…¶æ˜¯debugæ—¶ååˆ†æœ‰ç”¨ï¼‰\n",
    "\n",
    "æ‰§è¡Œå®Œå¯ä»¥åœ¨`processed_data/eval`ç›®å½•ä¸‹çœ‹åˆ°ç”Ÿæˆçš„`.bin`æ–‡ä»¶ä»¥åŠè®°å½•äº†æ¯ä¸ª`chunk`æ–‡ä»¶ä¿¡æ¯çš„`index.json`ã€‚\n",
    "\n",
    "æ¯”è¾ƒä¸€ä¸‹å¯ä»¥å‘ç°ä»åŸå…ˆçš„`83m`çš„`.jsonl`æ–‡ä»¶å‹ç¼©åˆ°äº†`13m`çš„`.bin`ï¼Œå‹ç¼©æ¯”ï¼ˆ83/13â‰ˆ6.385ï¼‰è¿˜æ˜¯å¾ˆå¯è§‚çš„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¤„ç†trainæ•°æ®é›†\n",
    "åœ¨32æ ¸çš„CPUä¸Šå¤„ç†`train`æ•°æ®é›†è€—æ—¶ä¸åˆ°`1min`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare(\n",
    "    input_dir=Path(\"../../Data/TinyStoriesChinese/raw_data/train\"),\n",
    "    output_dir=Path(\"../../Data/TinyStoriesChinese/processed_data/train\"),\n",
    "    tokenizer_path=Path(\"../../References/chatglm3-6b\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å°ç»“\n",
    "\n",
    "1. æ•°æ®é¢„å¤„ç†çš„é€»è¾‘ä¸»è¦æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—åºåˆ—ï¼Œä»¥ä¾¿äºæ¨¡å‹å¤„ç†ã€‚\n",
    "2. é€šè¿‡`litgpt`çš„`Tokenizer`å¯ä»¥æ–¹ä¾¿çš„å®ç°æ–‡æœ¬åˆ°æ•°å­—åºåˆ—çš„è½¬æ¢ã€‚\n",
    "3. `litdata`æä¾›äº†æ•°æ®å¤„ç†çš„`API`ï¼Œå¯ä»¥æ–¹ä¾¿çš„å°è£…æˆ‘ä»¬çš„æ•°æ®å¤„ç†é€»è¾‘ã€‚\n",
    "4. åŸºäºä¸Šé¢çš„å¼€å‘ï¼Œå°†`TinyStoriesChinese`æ•°æ®é›†åšäº†æ•°æ®åˆ’åˆ†å¹¶å®Œæˆäº†é¢„å¤„ç†ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
