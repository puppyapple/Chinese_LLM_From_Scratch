{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä»é›¶æ‰‹æ“ä¸­æ–‡å¤§æ¨¡å‹ï½œğŸš€Day03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•°æ®é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è™½ç„¶çœç•¥äº†æ•°æ®æ¸…æ´—çš„é€»è¾‘ï¼Œä½†æ˜¯æˆ‘ä»¬è¿˜æ˜¯éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œä»¥ä¾¿äºåç»­çš„æ¨¡å‹è®­ç»ƒã€‚\n",
    "\n",
    "åŒ…æ‹¬ä»¥ä¸‹ä¸¤ä¸ªç»†èŠ‚ï¼š\n",
    "\n",
    "1. åœ¨æ¯ä¸ªæ–‡æœ¬åæ·»åŠ `eos`æ ‡è®°ï¼Œä»¥ä¾¿äºæ¨¡å‹è¯†åˆ«å¥å­çš„ç»“æŸã€‚\n",
    "2. å°†æ–‡æœ¬è½¬æ¢ä¸º`æ•°å­—åºåˆ—`ï¼Œä»¥ä¾¿äºæ¨¡å‹å¤„ç†ã€‚\n",
    "   \n",
    "   è¿™ä¸€æ­¥å…¶å®ä¹Ÿå¯ä»¥æ”¾åˆ°æ¨¡å‹è®­ç»ƒçš„æ—¶å€™è¿›è¡Œï¼Œä½†æå‰å¤„ç†å¯ä»¥å‡å°‘è®­ç»ƒæ—¶çš„è®¡ç®—é‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®é›†åˆ’åˆ†\n",
    "\n",
    "è§£å‹æ•°æ®é›†ï¼Œå¾—åˆ°`48`ä¸ªjsonlæ–‡ä»¶ï¼Œå…±è®¡`3952863`è¡Œjsonæ•°æ®ã€‚\n",
    "\n",
    "æˆ‘ä¹‹å‰å·²ç»è§£å‹è¿‡äº†ï¼Œå¹¶ä¸”å°†åŸå§‹æ•°æ®å’Œå¤„ç†è¿‡åçš„æ•°æ®åˆ†åˆ«å­˜åœ¨äº†ä¸åŒè·¯å¾„ä¸‹ã€‚\n",
    "\n",
    "è¿™é‡ŒæŠŠå‘½ä»¤è´´å‡ºæ¥ä»¥ä¾›å‚è€ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !mkdir -p ../../Data/TinyStoriesChinese/raw_data/train\n",
    "# !mkdir -p ../../Data/TinyStoriesChinese/raw_data/val\n",
    "# !mkdir -p ../../Data/TinyStoriesChinese/processed_data/train\n",
    "# !mkdir -p ../../Data/TinyStoriesChinese/processed_data/val\n",
    "\n",
    "# !tar zxvf ../../Data/TinyStoriesChinese/TinyStories_all_data_zh.tar.gz -C ../../Data/TinyStoriesChinese/raw_data/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘æŠŠæœ€åä¸€ä¸ªæ–‡ä»¶`data47_zh.jsonl`ï¼ˆå…±è®¡78538è¡Œï¼‰é‡Œåˆ‡åˆ†å‡ºæ¥4wè¡Œä½œä¸º`eval`æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !mv ../../Data/TinyStoriesChinese/raw_data/train/data47_zh.jsonl ../../Data/TinyStoriesChinese/raw_data/val/\n",
    "# !head -n 40000 ../../Data/TinyStoriesChinese/raw_data/val/data47_zh.jsonl > ../../Data/TinyStoriesChinese/raw_data/val/val.jsonl\n",
    "# !tail -n +40000 ../../Data/TinyStoriesChinese/raw_data/val/data47_zh.jsonl > ../../Data/TinyStoriesChinese/raw_data/train/data47_zh.jsonl\n",
    "# !rm ../../Data/TinyStoriesChinese/raw_data/val/data47_zh.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å…ˆçœ‹ä¸€æ¡æ•°æ®\n",
    "ï¼ˆéƒ½æ‰“å°å‡ºæ¥å¤ªé•¿äº†ï¼Œæ‰€ä»¥åªè¾“å‡ºå‰100ä¸ªå­—ç¬¦ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è‰è‰å’Œæœ¬æ˜¯æœ‹å‹ã€‚ä»–ä»¬å–œæ¬¢åœ¨å…¬å›­é‡Œç©ã€‚æœ‰ä¸€å¤©ï¼Œä»–ä»¬åœ¨ä¸€æ£µå¤§æ ‘ä¸‹çœ‹åˆ°äº†ä¸€ä¸ªç§‹åƒã€‚è‰è‰æƒ³è¯•è¯•é‚£ä¸ªç§‹åƒã€‚å¥¹è·‘åˆ°æ ‘ä¸‹ï¼Œçˆ¬ä¸Šäº†ç§‹åƒã€‚\n",
      "\"æ¨æˆ‘ï¼Œæœ¬ï¼\"å¥¹è¯´ã€‚æœ¬è½»è½»åœ°æ¨äº†å¥¹ä¸€ä¸‹ã€‚è‰è‰æ„Ÿåˆ°å¾ˆå¼€å¿ƒã€‚å¥¹è¶Šè¡è¶Šé«˜ï¼Œç¬‘ç€å–Šå«ã€‚\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../../Data/TinyStoriesChinese/raw_data/train/data00_zh.jsonl\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        js = json.loads(line)\n",
    "        print(js[\"story_zh\"][:100])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é€‚é…æ¡†æ¶API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”±äºé€‰æ‹©äº†ä½¿ç”¨[âš¡ï¸litgpt](https://github.com/Lightning-AI/litgpt/tree/main)æ¡†æ¶è¿›è¡Œè®­ç»ƒï¼Œæ‰€ä»¥éœ€è¦å¼•å…¥æ¡†æ¶ç›¸å…³çš„`Class`å’Œ`API`æ¥å°è£…æˆ‘ä»¬çš„æ•°æ®å‡†å¤‡é€»è¾‘ã€‚\n",
    "\n",
    "è¿™é‡Œæˆ‘ä»¬å¯ä»¥å‚è€ƒ[æºç é‡Œé›†æˆçš„Tinyllamaçš„æ•°æ®é¢„å¤„ç†ä»£ç ](https://github.com/Lightning-AI/litgpt/blob/main/litgpt/data/prepare_slimpajama.py)é‡Œçš„ä»£ç ï¼Œç¨ä½œä¿®æ”¹ã€‚\n",
    "\n",
    "ä¸»è¦æ˜¯éœ€è¦å°†[Day02](../Day02/Day02.ipynb)é‡Œçš„`line`å¤„ç†é€»è¾‘å°è£…åˆ°`ligtgpt`çš„`API`ä¸­ã€‚\n",
    "\n",
    "ä½†åœ¨æ­¤ä¹‹å‰æˆ‘ä»¬å…ˆç†Ÿæ‚‰ä¸€ä¸‹`litgpt`çš„Tokenizerçš„ä½¿ç”¨æ–¹æ³•ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…ˆå®‰è£…ä¸€ä¸‹`litgpt`ä»¥åŠå®ƒæ‰€ä»¥èµ–çš„`litdata`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install litgpt\n",
    "# !pip install litdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from litgpt import Tokenizer\n",
    "\n",
    "litgpt_tokenizer = Tokenizer(\"../../References/chatglm3-6b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œä¹Ÿå®éªŒäº†ä¸€ä¸‹ç»“æœï¼Œå¯¹æ¯”å‘ç°å’Œå’±ä»¬ä¹‹å‰[Day02](../Day02/Day02.ipynb)é‡Œç”¨åŸç”Ÿ`Tokenizer`å¤„ç†çš„**ç»“æœä¸€è‡´**ã€‚\n",
    "\n",
    "ç»“æœè¿™é‡Œå°±ä¸è´´å‡ºæ¥äº†ï¼Œæœ‰å…´è¶£çš„å¯ä»¥è‡ªå·±è¯•ä¸€ä¸‹ã€‚\n",
    "\n",
    "> âš ï¸ä¸è¿‡éœ€è¦æ³¨æ„`litgpt`çš„`Tokenizer.encode`è¿”å›çš„æ˜¯ä¸€ä¸ª`torch`çš„`Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([30910, 56623, 56623, 54542, 50154, 31761, 31155, 31633, 31815, 54534,\n",
      "        32693, 54662, 55409, 31155, 35632, 31123, 31633, 34383, 57427, 47658,\n",
      "        54578, 34518, 31623, 55567, 55226, 31155, 56623, 56623, 54695, 39887,\n",
      "        32437, 55567, 55226, 31155, 54790, 41309, 52624, 31123, 56856, 32660,\n",
      "        55567, 55226, 31155,    13, 30955, 54834, 54546, 31123, 54613, 31404,\n",
      "        30955, 36213, 31155, 54613, 36660, 54563, 54834, 43881, 32024, 31155,\n",
      "        56623, 56623, 32707, 54657, 33436, 31155, 54790, 54937, 56567, 40714,\n",
      "        31123, 38502, 56653, 55483, 31155,    13, 54613, 32984, 56623, 56623,\n",
      "        31155, 54572, 31897, 54790, 54657, 35245, 31155, 36551, 54695, 56567,\n",
      "        55567, 55226, 31155, 33152, 56623, 56623, 51556, 31797, 39055, 31155,\n",
      "        31694, 56623, 56623, 31631, 51556, 31155, 54790, 54937, 56567, 54937,\n",
      "        54929, 31155, 54790, 55409, 40915, 34492, 54537, 31155,    13, 30955,\n",
      "        54546, 32591, 56567, 55567, 55226, 55398, 31123, 56623, 56623, 31514,\n",
      "        30955, 54613, 54761, 31155, 56623, 56623, 54721, 33906, 31804, 54887,\n",
      "        31155, 54790, 46977, 56567, 55567, 55226, 31155, 54613, 31897, 32960,\n",
      "        54597, 31155, 54572, 54942, 34675, 31155,    13, 56623, 56623, 56567,\n",
      "        40915, 54589, 31123, 36467, 33501, 31155, 54790, 54708, 55567, 55226,\n",
      "        54547, 57456, 32246, 31123, 36712, 34245, 31155, 54790, 56901, 55328,\n",
      "        54537, 55673, 31155, 54790, 56399, 37247, 31155,    13, 30955, 58394,\n",
      "        56657, 31123, 58394, 56657, 31123, 58394, 56657, 31404, 30955, 36213,\n",
      "        31155, 35957, 55227, 54613, 31155, 54790, 31772, 47554, 31934, 54790,\n",
      "        31155, 54688, 54613, 33551, 33892, 31155, 54572, 34247, 31155,    13,\n",
      "        56623, 56623, 32707, 54657, 52992, 31155, 54790, 31772, 54790, 54558,\n",
      "        54542, 54613, 32097, 55567, 55226, 31155, 54790, 31772, 33152, 33892,\n",
      "        37322, 54790, 31155, 54790, 54531, 60337, 54531, 57635, 54563, 35220,\n",
      "        52624, 31155, 54790, 31857, 33277, 32086, 44829, 49102, 54547, 31155,\n",
      "        35328, 43352, 41147, 31155, 54572, 42393, 32233, 31155,    13, 56623,\n",
      "        56623, 40466, 31155, 54790, 31897, 54613, 33058, 31155, 54790, 55947,\n",
      "        32660, 31804, 41147, 31155, 54790, 31772, 38711, 33857, 31155, 54790,\n",
      "        54695, 37300, 31155, 54790, 54695, 32462, 31705, 31761, 31155,     2],\n",
      "       dtype=torch.int32)\n",
      "è‰è‰å’Œæœ¬æ˜¯æœ‹å‹ã€‚ä»–ä»¬å–œæ¬¢åœ¨å…¬å›­é‡Œç©ã€‚æœ‰ä¸€å¤©ï¼Œä»–ä»¬åœ¨ä¸€æ£µå¤§æ ‘ä¸‹çœ‹åˆ°äº†ä¸€ä¸ªç§‹åƒã€‚è‰è‰æƒ³è¯•è¯•é‚£ä¸ªç§‹åƒã€‚å¥¹è·‘åˆ°æ ‘ä¸‹ï¼Œçˆ¬ä¸Šäº†ç§‹åƒã€‚\n",
      "\"æ¨æˆ‘ï¼Œæœ¬ï¼\"å¥¹è¯´ã€‚æœ¬è½»è½»åœ°æ¨äº†å¥¹ä¸€ä¸‹ã€‚è‰è‰æ„Ÿåˆ°å¾ˆå¼€å¿ƒã€‚å¥¹è¶Šè¡è¶Šé«˜ï¼Œç¬‘ç€å–Šå«ã€‚\n",
      "æœ¬çœ‹ç€è‰è‰ã€‚ä»–è§‰å¾—å¥¹å¾ˆå¯çˆ±ã€‚ä»–ä¹Ÿæƒ³è¡ç§‹åƒã€‚ä»–åœ¨è‰è‰åœä¸‹æ¥ä¹‹åç­‰ç€ã€‚ä½†æ˜¯è‰è‰æ²¡æœ‰åœä¸‹æ¥ã€‚å¥¹è¶Šè¡è¶Šå¿«ã€‚å¥¹ç©å¾—å¤ªé«˜å…´äº†ã€‚\n",
      "\"æˆ‘ä¹Ÿå¯ä»¥è¡ç§‹åƒå—ï¼Œè‰è‰ï¼Ÿ\"æœ¬é—®ã€‚è‰è‰æ²¡å¬åˆ°ä»–çš„è¯ã€‚å¥¹å¿™ç€è¡ç§‹åƒã€‚æœ¬è§‰å¾—å¾ˆéš¾è¿‡ã€‚ä»–èµ°å¼€äº†ã€‚\n",
      "è‰è‰è¡å¾—å¤ªé«˜ï¼Œå¤±å»äº†å¹³è¡¡ã€‚å¥¹ä»ç§‹åƒä¸Šæ‘”ä¸‹æ¥ï¼Œè½åœ¨åœ°ä¸Šã€‚å¥¹æ‰­ä¼¤äº†è„šã€‚å¥¹å“­äº†èµ·æ¥ã€‚\n",
      "\"å“å‘€ï¼Œå“å‘€ï¼Œå“å‘€ï¼\"å¥¹è¯´ã€‚å¥¹åœ¨æ‰¾æœ¬ã€‚å¥¹å¸Œæœ›ä»–èƒ½å¸®åŠ©å¥¹ã€‚ä½†æœ¬ä¸åœ¨é‚£é‡Œã€‚ä»–èµ°äº†ã€‚\n",
      "è‰è‰æ„Ÿåˆ°å¾ˆæŠ±æ­‰ã€‚å¥¹å¸Œæœ›å¥¹èƒ½å’Œæœ¬åˆ†äº«ç§‹åƒã€‚å¥¹å¸Œæœ›ä»–åœ¨é‚£é‡Œæ‹¥æŠ±å¥¹ã€‚å¥¹ä¸€ç˜¸ä¸€æ‹åœ°èµ°åˆ°æ ‘ä¸‹ã€‚å¥¹çœ‹åˆ°æœ‰ä»€ä¹ˆä¸œè¥¿æŒ‚åœ¨æ ‘æä¸Šã€‚é‚£æ˜¯æœ¬çš„å¸½å­ã€‚ä»–ç•™ç»™å¥¹çš„ã€‚\n",
      "è‰è‰ç¬‘äº†ã€‚å¥¹è§‰å¾—æœ¬å¾ˆå¥½ã€‚å¥¹æˆ´ä¸Šäº†ä»–çš„å¸½å­ã€‚å¥¹å¸Œæœ›ä»–ä¼šå›æ¥ã€‚å¥¹æƒ³é“æ­‰ã€‚å¥¹æƒ³å†æ¬¡æˆä¸ºæœ‹å‹ã€‚\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "litgpt_encoded = litgpt_tokenizer.encode(\n",
    "    json.loads(line)[\"story_zh\"], eos=True\n",
    ")  # è®°å¾—è®¾ç½®eos=True\n",
    "print(litgpt_encoded)\n",
    "# print(np.array(litgpt_encoded, dtype=np.uint16))\n",
    "print(litgpt_tokenizer.decode(litgpt_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®å¤„ç†ä»£ç \n",
    "æ•°æ®å¤„ç†ç›´æ¥å‚è€ƒäº†ä¸Šé¢ç»™å‡ºçš„[litgpt samples](https://github.com/Lightning-AI/litgpt/blob/main/litgpt/data/prepare_slimpajama.py)ï¼Œæˆ‘ä»¬éœ€è¦ä»¿ç…§`prepare_slimpajama.py`å®ç°é‡Œé¢ç›¸å…³å‡½æ•°ï¼ˆä¹‹å‰**Day 02**é‡Œå®ç°çš„å‡½æ•°éœ€è¦ç¨åŠ æ”¹é€ ä¸€ä¸‹ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Lightning AI. Licensed under the Apache License 2.0, see LICENSE file.\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from litgpt.tokenizer import Tokenizer\n",
    "from litgpt.data.prepare_starcoder import DataChunkRecipe\n",
    "from litdata import TokensLoader\n",
    "from litgpt.utils import extend_checkpoint_dir\n",
    "\n",
    "\n",
    "class TinyStoriesZhDataRecipe(DataChunkRecipe):\n",
    "    is_generator = True\n",
    "\n",
    "    def __init__(self, tokenizer: Tokenizer, chunk_size: int):\n",
    "        super().__init__(chunk_size)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def prepare_structure(self, input_dir):\n",
    "        files = Path(input_dir).rglob(\"*.jsonl\")\n",
    "        return [str(file) for file in files]\n",
    "\n",
    "    def prepare_item(self, filepath):\n",
    "\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            for line in f.readlines():\n",
    "                js = json.loads(line)\n",
    "                story = js[\"story_zh\"]\n",
    "                # æ³¨æ„è¿™é‡Œè¦æ·»åŠ eos\n",
    "                # è¿˜è®°å¾—å—ï¼šæˆ‘ä»¬çš„vocab sizeåœ¨int16èŒƒå›´å†…ï¼Œæ‰€ä»¥å¯ä»¥è½¬æ¢ä¸ºuint16æ¥èŠ‚çœå†…å­˜\n",
    "                # story_ids = np.array(\n",
    "                #     self.tokenizer.encode(story, eos=True), dtype=np.uint16\n",
    "                # )\n",
    "                # å¾ˆé—æ†¾ï¼Œå®é™…ä½¿ç”¨çš„æ—¶å€™å‘ç°å¦‚æœæŒ‰ç…§ä¸Šé¢è¿™æ ·å†™ï¼Œ\n",
    "                # litdataååºåˆ—åŒ–æ•°æ®çš„æ—¶å€™ä¼šé”™è¯¯åœ°å¾—åˆ°torch.int64ä¸”è¶…ç•Œçš„Tensorï¼Œ\n",
    "                # ä½†ç›´æ¥å­˜torch.Tensoræ²¡é—®é¢˜ï¼ˆåŠ ä¸Šlitdataä¸æ”¯æŒtorch.uint16ï¼‰ï¼Œ\n",
    "                # æ‰€ä»¥æœ€åå®é™…ä½¿ç”¨çš„æ—¶å€™è¿˜æ˜¯ç”¨ä¸‹é¢è¿™ç§å†™æ³•\n",
    "                story_ids = self.tokenizer.encode(story, eos=True)\n",
    "                yield story_ids\n",
    "\n",
    "\n",
    "def prepare(\n",
    "    input_dir: Path = Path(\"../../Data/TinyStoriesChinese/raw_data/train\"),\n",
    "    output_dir: Path = Path(\"../../Data/TinyStoriesChinese/processed_data/train\"),\n",
    "    tokenizer_path: Path = Path(\"../../References/chatglm3-6b\"),\n",
    "    chunk_size: int = (2049 * 8012),\n",
    "    fast_dev_run: bool = False,\n",
    ") -> None:\n",
    "    from litdata.processing.data_processor import DataProcessor\n",
    "\n",
    "    tokenizer_path = extend_checkpoint_dir(tokenizer_path)\n",
    "    tokenizer = Tokenizer(tokenizer_path)\n",
    "    data_recipe = TinyStoriesZhDataRecipe(tokenizer=tokenizer, chunk_size=chunk_size)\n",
    "    data_processor = DataProcessor(\n",
    "        input_dir=str(input_dir),\n",
    "        output_dir=str(output_dir),\n",
    "        fast_dev_run=fast_dev_run,\n",
    "        num_workers=os.cpu_count(),\n",
    "        num_downloaders=1,\n",
    "        # è¿™é‡Œæœ‰ä¸ªã€Œå·¨å‘ã€ï¼Œå¦‚æœä¸åŠ è¿™ä¸€è¡Œï¼Œå¤„ç†å¥½çš„æ•°æ®é…å¯¹çš„index.jsoné‡Œ\n",
    "        # æœ‰ä¸€ä¸ªåä¸º\"dim\"çš„keyå€¼ä¼šä¸ºnullï¼Œå¯¼è‡´åç»­æœ‰ä¸€ä¸ªæ— æ³•è§„é¿çš„æŠ¥é”™\n",
    "        # ä½†æ˜¯å®˜æ–¹çš„ä¾‹å­é‡Œæ˜¯æ²¡æœ‰è¿™ä¸€è¡Œçš„ï¼Œå¾ˆå¥‡æ€ªä¸ºä½•ä¼šæœ‰è¿™ä¸ªé—®é¢˜\n",
    "        item_loader=TokensLoader(),\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    data_processor.run(data_recipe)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é¦–å…ˆï¼Œæˆ‘è¿™é‡Œä¸»è¦å°±æ˜¯æŠŠä¹‹å‰å®ç°çš„`line`å¤„ç†é€»è¾‘å°è£…åˆ°`litgpt`çš„`DataChunkRecipe`ä¸­ï¼š\n",
    "- `prepare_structure`å‡½æ•°ç»™å®šè·¯å¾„è¿”å›ç¬¦åˆæˆ‘ä»¬æœŸæœ›çš„æ•°æ®æ–‡ä»¶çš„è·¯å¾„åˆ—è¡¨\n",
    "- `prepare_item`å‡½æ•°ç»™å®šä¸€ä¸ªä¸Šé¢çš„æ•°æ®æ–‡ä»¶çš„è·¯å¾„ï¼Œæ ¹æ®æˆ‘ä»¬**è‡ªå®šä¹‰**çš„`tokenization`å¤„ç†é€»è¾‘è¿”å›ä¸€ä¸ª`np.array`å¯¹è±¡\n",
    "  \n",
    "ç„¶åï¼Œå®šä¹‰äº†ä¸€ä¸ª`prepare`å‡½æ•°ï¼ŒæŒ‡å®šæˆ‘ä»¬æ•°æ®çš„è¾“å…¥è·¯å¾„å’Œè¾“å‡ºè·¯å¾„ä»¥åŠä¸€äº›å…¶å®ƒå‚æ•°é…ç½®ï¼ˆå…¶å®ç”¨é»˜è®¤çš„å³å¯ï¼‰ï¼Œå…¶ä½™çš„éƒ½äº¤ç»™äº†`litdata`çš„`DataProcessor`ï¼Œå®ƒåŸºäºæˆ‘å‰é¢å®šä¹‰çš„`DataChunkRecipe`æ¥å¤„ç†æ•°æ®ã€‚\n",
    "\n",
    "æ„Ÿå…´è¶£çš„å¯ä»¥çœ‹çœ‹`DataProcessor`çš„æºç ï¼Œé‡Œé¢åšäº†å¾ˆå¤šå¹¶è¡Œä¹‹ç±»çš„æ•°æ®å¤„ç†ä¼˜åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å…ˆç”¨evalæ•°æ®é›†æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting multiprocessing start_method to fork. Tip: Libraries relying on lock can hang with `fork`. To use `spawn` in notebooks, move your code to files and import it within the notebook.\n",
      "Storing the files under /home/puppyapple/Server/BigAI/Chinese_LLM_From_Scratch/Data/TinyStoriesChinese/processed_data/val\n",
      "Setup started with fast_dev_run=False.\n",
      "Worker 0 gets 87.2 MB (1 files)\n",
      "Worker 1 gets 0.0 MB (0 files)\n",
      "Worker 2 gets 0.0 MB (0 files)\n",
      "Worker 3 gets 0.0 MB (0 files)\n",
      "Worker 4 gets 0.0 MB (0 files)\n",
      "Worker 5 gets 0.0 MB (0 files)\n",
      "Worker 6 gets 0.0 MB (0 files)\n",
      "Worker 7 gets 0.0 MB (0 files)\n",
      "Worker 8 gets 0.0 MB (0 files)\n",
      "Worker 9 gets 0.0 MB (0 files)\n",
      "Worker 10 gets 0.0 MB (0 files)\n",
      "Worker 11 gets 0.0 MB (0 files)\n",
      "Worker 12 gets 0.0 MB (0 files)\n",
      "Worker 13 gets 0.0 MB (0 files)\n",
      "Worker 14 gets 0.0 MB (0 files)\n",
      "Worker 15 gets 0.0 MB (0 files)\n",
      "Worker 16 gets 0.0 MB (0 files)\n",
      "Worker 17 gets 0.0 MB (0 files)\n",
      "Worker 18 gets 0.0 MB (0 files)\n",
      "Worker 19 gets 0.0 MB (0 files)\n",
      "Worker 20 gets 0.0 MB (0 files)\n",
      "Worker 21 gets 0.0 MB (0 files)\n",
      "Worker 22 gets 0.0 MB (0 files)\n",
      "Worker 23 gets 0.0 MB (0 files)\n",
      "Worker 24 gets 0.0 MB (0 files)\n",
      "Worker 25 gets 0.0 MB (0 files)\n",
      "Worker 26 gets 0.0 MB (0 files)\n",
      "Worker 27 gets 0.0 MB (0 files)\n",
      "Worker 28 gets 0.0 MB (0 files)\n",
      "Worker 29 gets 0.0 MB (0 files)\n",
      "Worker 30 gets 0.0 MB (0 files)\n",
      "Worker 31 gets 0.0 MB (0 files)\n",
      "Setup finished in 0.002 seconds. Found 1 items to process.\n",
      "Starting 32 workers with 1 items. The progress bar is only updated when a worker finishes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 1 is terminating.\n",
      "Worker 2 is terminating.\n",
      "Worker 1 is done.Worker 3 is terminating.\n",
      "\n",
      "Worker 2 is done.\n",
      "Worker 4 is terminating.\n",
      "Worker 3 is done.\n",
      "\n",
      "Worker 5 is terminating.Worker 4 is done.Worker 6 is terminating.\n",
      "\n",
      "Worker 5 is done.Worker 7 is terminating.\n",
      "\n",
      "Worker 8 is terminating.\n",
      "Worker 6 is done.\n",
      "Worker 9 is terminating.\n",
      "\n",
      "Worker 7 is done.Rank 0 inferred the following `['no_header_tensor:16']` data format.Worker 8 is done.\n",
      "\n",
      "Worker 10 is terminating.\n",
      "Worker 9 is done.\n",
      "Worker 11 is terminating.\n",
      "Worker 10 is done.\n",
      "Worker 12 is terminating.\n",
      "Worker 11 is done.Worker 13 is terminating.\n",
      "\n",
      "Worker 14 is terminating.\n",
      "Worker 12 is done.\n",
      "Worker 15 is terminating.\n",
      "Worker 14 is done.Worker 13 is done.\n",
      "\n",
      "Worker 16 is terminating.\n",
      "Worker 17 is terminating.Worker 15 is done.Worker 16 is done.Worker 18 is terminating.\n",
      "\n",
      "\n",
      "\n",
      "Worker 19 is terminating.\n",
      "Worker 18 is done.Worker 17 is done.Worker 20 is terminating.\n",
      "\n",
      "\n",
      "Worker 19 is done.\n",
      "Worker 21 is terminating.\n",
      "Worker 20 is done.Worker 22 is terminating.\n",
      "\n",
      "Worker 23 is terminating.Worker 21 is done.\n",
      "\n",
      "Worker 22 is done.\n",
      "Worker 24 is terminating.\n",
      "Worker 25 is terminating.\n",
      "Worker 23 is done.\n",
      "Worker 26 is terminating.\n",
      "Worker 24 is done.\n",
      "\n",
      "Worker 27 is terminating.Worker 25 is done.\n",
      "Worker 26 is done.Worker 28 is terminating.Workers are ready ! Starting data processing...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9e35d0aa3c43af8b4ff9fba7c70374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Worker 27 is done.\n",
      "Worker 29 is terminating.\n",
      "Worker 30 is terminating.Worker 28 is done.\n",
      "\n",
      "Worker 31 is terminating.\n",
      "Worker 29 is done.\n",
      "Worker 30 is done.\n",
      "Worker 31 is done.\n",
      "Worker 0 is terminating.\n",
      "Worker 0 is done.\n",
      "Workers are finished.\n",
      "Finished data processing!\n",
      "Time taken: 6.83 seconds\n"
     ]
    }
   ],
   "source": [
    "prepare(\n",
    "    input_dir=Path(\"../../Data/TinyStoriesChinese/raw_data/val\"),\n",
    "    output_dir=Path(\"../../Data/TinyStoriesChinese/processed_data/val\"),\n",
    "    tokenizer_path=Path(\"../../References/chatglm3-6b\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ï¼ˆä¹Ÿå¯ä»¥è®¾ç½®`fast_dev_run=True`æ¥å¤„ç†æ›´å°‘çš„æ•°æ®ï¼Œå°¤å…¶æ˜¯debugæ—¶ååˆ†æœ‰ç”¨ï¼‰\n",
    "\n",
    "æ‰§è¡Œå®Œå¯ä»¥åœ¨`processed_data/eval`ç›®å½•ä¸‹çœ‹åˆ°ç”Ÿæˆçš„chunkæ–‡ä»¶ã€‚\n",
    "\n",
    "æ¯”è¾ƒä¸€ä¸‹å¯ä»¥å‘ç°ä»åŸå…ˆçš„`83m`çš„`.jsonl`æ–‡ä»¶å‹ç¼©åˆ°äº†`13m`çš„`.bin`ï¼Œå‹ç¼©æ¯”ï¼ˆ83/13â‰ˆ6.385ï¼‰è¿˜æ˜¯å¾ˆå¯è§‚çš„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¤„ç†trainæ•°æ®é›†\n",
    "åœ¨32æ ¸çš„CPUä¸Šå¤„ç†`train`æ•°æ®é›†è€—æ—¶ä¸åˆ°`1min`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting multiprocessing start_method to fork. Tip: Libraries relying on lock can hang with `fork`. To use `spawn` in notebooks, move your code to files and import it within the notebook.\n",
      "Storing the files under /home/puppyapple/Server/BigAI/Chinese_LLM_From_Scratch/Data/TinyStoriesChinese/processed_data/train\n",
      "Setup started with fast_dev_run=False.\n",
      "Worker 0 gets 218.5 MB (1 files)\n",
      "Worker 1 gets 218.5 MB (1 files)\n",
      "Worker 2 gets 218.4 MB (1 files)\n",
      "Worker 3 gets 218.4 MB (1 files)\n",
      "Worker 4 gets 218.4 MB (1 files)\n",
      "Worker 5 gets 218.3 MB (1 files)\n",
      "Worker 6 gets 218.3 MB (1 files)\n",
      "Worker 7 gets 218.3 MB (1 files)\n",
      "Worker 8 gets 218.3 MB (1 files)\n",
      "Worker 9 gets 218.2 MB (1 files)\n",
      "Worker 10 gets 218.2 MB (1 files)\n",
      "Worker 11 gets 218.2 MB (1 files)\n",
      "Worker 12 gets 218.1 MB (1 files)\n",
      "Worker 13 gets 218.1 MB (1 files)\n",
      "Worker 14 gets 217.9 MB (1 files)\n",
      "Worker 15 gets 217.8 MB (1 files)\n",
      "Worker 16 gets 257.6 MB (2 files)\n",
      "Worker 17 gets 320.8 MB (2 files)\n",
      "Worker 18 gets 320.7 MB (2 files)\n",
      "Worker 19 gets 321.1 MB (2 files)\n",
      "Worker 20 gets 321.1 MB (2 files)\n",
      "Worker 21 gets 321.1 MB (2 files)\n",
      "Worker 22 gets 321.1 MB (2 files)\n",
      "Worker 23 gets 321.2 MB (2 files)\n",
      "Worker 24 gets 320.9 MB (2 files)\n",
      "Worker 25 gets 320.9 MB (2 files)\n",
      "Worker 26 gets 320.9 MB (2 files)\n",
      "Worker 27 gets 321.0 MB (2 files)\n",
      "Worker 28 gets 320.8 MB (2 files)\n",
      "Worker 29 gets 320.8 MB (2 files)\n",
      "Worker 30 gets 320.1 MB (2 files)\n",
      "Worker 31 gets 297.4 MB (2 files)\n",
      "Setup finished in 0.01 seconds. Found 48 items to process.\n",
      "Starting 32 workers with 48 items. The progress bar is only updated when a worker finishes.\n",
      "Workers are ready ! Starting data processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703ff0846c1947b4bb5dce2c4ec1508f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 16 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 1 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 0 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 2 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 3 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 9 inferred the following `['no_header_tensor:16']` data format.Rank 17 inferred the following `['no_header_tensor:16']` data format.\n",
      "\n",
      "Rank 5 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 20 inferred the following `['no_header_tensor:16']` data format.Rank 4 inferred the following `['no_header_tensor:16']` data format.Rank 18 inferred the following `['no_header_tensor:16']` data format.\n",
      "\n",
      "Rank 14 inferred the following `['no_header_tensor:16']` data format.\n",
      "\n",
      "Rank 10 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 19 inferred the following `['no_header_tensor:16']` data format.Rank 23 inferred the following `['no_header_tensor:16']` data format.Rank 6 inferred the following `['no_header_tensor:16']` data format.\n",
      "\n",
      "\n",
      "Rank 22 inferred the following `['no_header_tensor:16']` data format.Rank 8 inferred the following `['no_header_tensor:16']` data format.\n",
      "\n",
      "Rank 21 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 7 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 25 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 15 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 12 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 26 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 31 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 24 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 11 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 29 inferred the following `['no_header_tensor:16']` data format.Rank 13 inferred the following `['no_header_tensor:16']` data format.\n",
      "\n",
      "Rank 28 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 27 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 30 inferred the following `['no_header_tensor:16']` data format.\n",
      "Worker 14 is terminating.\n",
      "Worker 14 is done.\n",
      "Worker 12 is terminating.\n",
      "Worker 12 is done.\n",
      "Worker 4 is terminating.\n",
      "Worker 4 is done.\n",
      "Worker 10 is terminating.\n",
      "Worker 7 is terminating.\n",
      "Worker 13 is terminating.\n",
      "Worker 10 is done.Worker 15 is terminating.\n",
      "\n",
      "Worker 7 is done.\n",
      "Worker 13 is done.\n",
      "Worker 15 is done.\n",
      "Worker 2 is terminating.\n",
      "Worker 5 is terminating.\n",
      "Worker 2 is done.\n",
      "Worker 5 is done.\n",
      "Worker 16 is terminating.\n",
      "Worker 3 is terminating.\n",
      "Worker 16 is done.\n",
      "Worker 3 is done.\n",
      "Worker 8 is terminating.\n",
      "Worker 8 is done.\n",
      "Worker 9 is terminating.\n",
      "Worker 0 is terminating.\n",
      "Worker 9 is done.\n",
      "Worker 0 is done.\n",
      "Worker 11 is terminating.\n",
      "Worker 11 is done.\n",
      "Worker 1 is terminating.\n",
      "Worker 1 is done.\n",
      "Worker 6 is terminating.\n",
      "Worker 6 is done.\n",
      "Worker 31 is terminating.\n",
      "Worker 31 is done.\n",
      "Worker 29 is terminating.\n",
      "Worker 29 is done.\n",
      "Worker 17 is terminating.\n",
      "Worker 20 is terminating.\n",
      "Worker 26 is terminating.\n",
      "Worker 28 is terminating.\n",
      "Worker 17 is done.\n",
      "Worker 20 is done.\n",
      "Worker 19 is terminating.\n",
      "Worker 22 is terminating.\n",
      "Worker 23 is terminating.\n",
      "Worker 26 is done.\n",
      "Worker 28 is done.\n",
      "Worker 19 is done.\n",
      "Worker 22 is done.\n",
      "Worker 23 is done.\n",
      "Worker 30 is terminating.\n",
      "Worker 18 is terminating.\n",
      "Worker 25 is terminating.\n",
      "Worker 30 is done.\n",
      "Worker 21 is terminating.\n",
      "Worker 18 is done.\n",
      "Worker 25 is done.\n",
      "Worker 21 is done.\n",
      "Worker 27 is terminating.\n",
      "Worker 27 is done.\n",
      "Worker 24 is terminating.\n",
      "Worker 24 is done.\n",
      "Workers are finished.\n",
      "Finished data processing!\n",
      "Time taken: 52.66 seconds\n"
     ]
    }
   ],
   "source": [
    "prepare(\n",
    "    input_dir=Path(\"../../Data/TinyStoriesChinese/raw_data/train\"),\n",
    "    output_dir=Path(\"../../Data/TinyStoriesChinese/processed_data/train\"),\n",
    "    tokenizer_path=Path(\"../../References/chatglm3-6b\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å°ç»“\n",
    "\n",
    "1. æ•°æ®é¢„å¤„ç†çš„é€»è¾‘ä¸»è¦æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—åºåˆ—ï¼Œä»¥ä¾¿äºæ¨¡å‹å¤„ç†ã€‚\n",
    "2. é€šè¿‡`litgpt`çš„`Tokenizer`å¯ä»¥æ–¹ä¾¿çš„å®ç°æ–‡æœ¬åˆ°æ•°å­—åºåˆ—çš„è½¬æ¢ã€‚\n",
    "3. `litdata`æä¾›äº†æ•°æ®å¤„ç†çš„`API`ï¼Œå¯ä»¥æ–¹ä¾¿çš„å°è£…æˆ‘ä»¬çš„æ•°æ®å¤„ç†é€»è¾‘ã€‚\n",
    "4. åŸºäºä¸Šé¢çš„å¼€å‘ï¼Œå°†`TinyStoriesChinese`æ•°æ®é›†åšäº†æ•°æ®åˆ’åˆ†å¹¶å®Œæˆäº†é¢„å¤„ç†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
