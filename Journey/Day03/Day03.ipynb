{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零手搓中文大模型｜🚀Day03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然省略了数据清洗的逻辑，但是我们还是需要对数据进行预处理，以便于后续的模型训练。\n",
    "\n",
    "包括以下两个细节：\n",
    "\n",
    "1. 在每个文本后添加`eos`标记，以便于模型识别句子的结束。\n",
    "2. 将文本转换为`数字序列`，以便于模型处理。\n",
    "   \n",
    "   这一步其实也可以放到模型训练的时候进行，但提前处理可以减少训练时的计算量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集划分\n",
    "\n",
    "解压数据集，得到`48`个jsonl文件，共计`3952863`行json数据。\n",
    "\n",
    "我之前已经解压过了，并且将原始数据和处理过后的数据分别存在了不同路径下。\n",
    "\n",
    "这里把命令贴出来以供参考。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !mkdir -p ../../Data/TinyStoriesChinese/raw_data/train\n",
    "# !mkdir -p ../../Data/TinyStoriesChinese/raw_data/val\n",
    "# !mkdir -p ../../Data/TinyStoriesChinese/processed_data/train\n",
    "# !mkdir -p ../../Data/TinyStoriesChinese/processed_data/val\n",
    "\n",
    "# !tar zxvf ../../Data/TinyStoriesChinese/TinyStories_all_data_zh.tar.gz -C ../../Data/TinyStoriesChinese/raw_data/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我把最后一个文件`data47_zh.jsonl`（共计78538行）里切分出来4w行作为`eval`数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !mv ../../Data/TinyStoriesChinese/raw_data/train/data47_zh.jsonl ../../Data/TinyStoriesChinese/raw_data/val/\n",
    "# !head -n 40000 ../../Data/TinyStoriesChinese/raw_data/val/data47_zh.jsonl > ../../Data/TinyStoriesChinese/raw_data/val/val.jsonl\n",
    "# !tail -n +40000 ../../Data/TinyStoriesChinese/raw_data/val/data47_zh.jsonl > ../../Data/TinyStoriesChinese/raw_data/train/data47_zh.jsonl\n",
    "# !rm ../../Data/TinyStoriesChinese/raw_data/val/data47_zh.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先看一条数据\n",
    "（都打印出来太长了，所以只输出前100个字符）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "莉莉和本是朋友。他们喜欢在公园里玩。有一天，他们在一棵大树下看到了一个秋千。莉莉想试试那个秋千。她跑到树下，爬上了秋千。\n",
      "\"推我，本！\"她说。本轻轻地推了她一下。莉莉感到很开心。她越荡越高，笑着喊叫。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../../Data/TinyStoriesChinese/raw_data/train/data00_zh.jsonl\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        js = json.loads(line)\n",
    "        print(js[\"story_zh\"][:100])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 适配框架API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于选择了使用[⚡️litgpt](https://github.com/Lightning-AI/litgpt/tree/main)框架进行训练，所以需要引入框架相关的`Class`和`API`来封装我们的数据准备逻辑。\n",
    "\n",
    "这里我们可以参考[源码里集成的Tinyllama的数据预处理代码](https://github.com/Lightning-AI/litgpt/blob/main/litgpt/data/prepare_slimpajama.py)里的代码，稍作修改。\n",
    "\n",
    "主要是需要将[Day02](../Day02/Day02.ipynb)里的`line`处理逻辑封装到`ligtgpt`的`API`中。\n",
    "\n",
    "但在此之前我们先熟悉一下`litgpt`的Tokenizer的使用方法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先安装一下`litgpt`以及它所以赖的`litdata`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install litgpt\n",
    "# !pip install litdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from litgpt import Tokenizer\n",
    "\n",
    "litgpt_tokenizer = Tokenizer(\"../../References/chatglm3-6b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里也实验了一下结果，对比发现和咱们之前[Day02](../Day02/Day02.ipynb)里用原生`Tokenizer`处理的**结果一致**。\n",
    "\n",
    "结果这里就不贴出来了，有兴趣的可以自己试一下。\n",
    "\n",
    "> ⚠️不过需要注意`litgpt`的`Tokenizer.encode`返回的是一个`torch`的`Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([30910, 56623, 56623, 54542, 50154, 31761, 31155, 31633, 31815, 54534,\n",
      "        32693, 54662, 55409, 31155, 35632, 31123, 31633, 34383, 57427, 47658,\n",
      "        54578, 34518, 31623, 55567, 55226, 31155, 56623, 56623, 54695, 39887,\n",
      "        32437, 55567, 55226, 31155, 54790, 41309, 52624, 31123, 56856, 32660,\n",
      "        55567, 55226, 31155,    13, 30955, 54834, 54546, 31123, 54613, 31404,\n",
      "        30955, 36213, 31155, 54613, 36660, 54563, 54834, 43881, 32024, 31155,\n",
      "        56623, 56623, 32707, 54657, 33436, 31155, 54790, 54937, 56567, 40714,\n",
      "        31123, 38502, 56653, 55483, 31155,    13, 54613, 32984, 56623, 56623,\n",
      "        31155, 54572, 31897, 54790, 54657, 35245, 31155, 36551, 54695, 56567,\n",
      "        55567, 55226, 31155, 33152, 56623, 56623, 51556, 31797, 39055, 31155,\n",
      "        31694, 56623, 56623, 31631, 51556, 31155, 54790, 54937, 56567, 54937,\n",
      "        54929, 31155, 54790, 55409, 40915, 34492, 54537, 31155,    13, 30955,\n",
      "        54546, 32591, 56567, 55567, 55226, 55398, 31123, 56623, 56623, 31514,\n",
      "        30955, 54613, 54761, 31155, 56623, 56623, 54721, 33906, 31804, 54887,\n",
      "        31155, 54790, 46977, 56567, 55567, 55226, 31155, 54613, 31897, 32960,\n",
      "        54597, 31155, 54572, 54942, 34675, 31155,    13, 56623, 56623, 56567,\n",
      "        40915, 54589, 31123, 36467, 33501, 31155, 54790, 54708, 55567, 55226,\n",
      "        54547, 57456, 32246, 31123, 36712, 34245, 31155, 54790, 56901, 55328,\n",
      "        54537, 55673, 31155, 54790, 56399, 37247, 31155,    13, 30955, 58394,\n",
      "        56657, 31123, 58394, 56657, 31123, 58394, 56657, 31404, 30955, 36213,\n",
      "        31155, 35957, 55227, 54613, 31155, 54790, 31772, 47554, 31934, 54790,\n",
      "        31155, 54688, 54613, 33551, 33892, 31155, 54572, 34247, 31155,    13,\n",
      "        56623, 56623, 32707, 54657, 52992, 31155, 54790, 31772, 54790, 54558,\n",
      "        54542, 54613, 32097, 55567, 55226, 31155, 54790, 31772, 33152, 33892,\n",
      "        37322, 54790, 31155, 54790, 54531, 60337, 54531, 57635, 54563, 35220,\n",
      "        52624, 31155, 54790, 31857, 33277, 32086, 44829, 49102, 54547, 31155,\n",
      "        35328, 43352, 41147, 31155, 54572, 42393, 32233, 31155,    13, 56623,\n",
      "        56623, 40466, 31155, 54790, 31897, 54613, 33058, 31155, 54790, 55947,\n",
      "        32660, 31804, 41147, 31155, 54790, 31772, 38711, 33857, 31155, 54790,\n",
      "        54695, 37300, 31155, 54790, 54695, 32462, 31705, 31761, 31155,     2],\n",
      "       dtype=torch.int32)\n",
      "莉莉和本是朋友。他们喜欢在公园里玩。有一天，他们在一棵大树下看到了一个秋千。莉莉想试试那个秋千。她跑到树下，爬上了秋千。\n",
      "\"推我，本！\"她说。本轻轻地推了她一下。莉莉感到很开心。她越荡越高，笑着喊叫。\n",
      "本看着莉莉。他觉得她很可爱。他也想荡秋千。他在莉莉停下来之后等着。但是莉莉没有停下来。她越荡越快。她玩得太高兴了。\n",
      "\"我也可以荡秋千吗，莉莉？\"本问。莉莉没听到他的话。她忙着荡秋千。本觉得很难过。他走开了。\n",
      "莉莉荡得太高，失去了平衡。她从秋千上摔下来，落在地上。她扭伤了脚。她哭了起来。\n",
      "\"哎呀，哎呀，哎呀！\"她说。她在找本。她希望他能帮助她。但本不在那里。他走了。\n",
      "莉莉感到很抱歉。她希望她能和本分享秋千。她希望他在那里拥抱她。她一瘸一拐地走到树下。她看到有什么东西挂在树枝上。那是本的帽子。他留给她的。\n",
      "莉莉笑了。她觉得本很好。她戴上了他的帽子。她希望他会回来。她想道歉。她想再次成为朋友。\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "litgpt_encoded = litgpt_tokenizer.encode(\n",
    "    json.loads(line)[\"story_zh\"], eos=True\n",
    ")  # 记得设置eos=True\n",
    "print(litgpt_encoded)\n",
    "# print(np.array(litgpt_encoded, dtype=np.uint16))\n",
    "print(litgpt_tokenizer.decode(litgpt_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理代码\n",
    "数据处理直接参考了上面给出的[litgpt samples](https://github.com/Lightning-AI/litgpt/blob/main/litgpt/data/prepare_slimpajama.py)，我们需要仿照`prepare_slimpajama.py`实现里面相关函数（之前**Day 02**里实现的函数需要稍加改造一下）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Lightning AI. Licensed under the Apache License 2.0, see LICENSE file.\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from litgpt.tokenizer import Tokenizer\n",
    "from litgpt.data.prepare_starcoder import DataChunkRecipe\n",
    "from litdata import TokensLoader\n",
    "from litgpt.utils import extend_checkpoint_dir\n",
    "\n",
    "\n",
    "class TinyStoriesZhDataRecipe(DataChunkRecipe):\n",
    "    is_generator = True\n",
    "\n",
    "    def __init__(self, tokenizer: Tokenizer, chunk_size: int):\n",
    "        super().__init__(chunk_size)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def prepare_structure(self, input_dir):\n",
    "        files = Path(input_dir).rglob(\"*.jsonl\")\n",
    "        return [str(file) for file in files]\n",
    "\n",
    "    def prepare_item(self, filepath):\n",
    "\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            for line in f.readlines():\n",
    "                js = json.loads(line)\n",
    "                story = js[\"story_zh\"]\n",
    "                # 注意这里要添加eos\n",
    "                # 还记得吗：我们的vocab size在int16范围内，所以可以转换为uint16来节省内存\n",
    "                # story_ids = np.array(\n",
    "                #     self.tokenizer.encode(story, eos=True), dtype=np.uint16\n",
    "                # )\n",
    "                # 很遗憾，实际使用的时候发现如果按照上面这样写，\n",
    "                # litdata反序列化数据的时候会错误地得到torch.int64且超界的Tensor，\n",
    "                # 但直接存torch.Tensor没问题（加上litdata不支持torch.uint16），\n",
    "                # 所以最后实际使用的时候还是用下面这种写法\n",
    "                story_ids = self.tokenizer.encode(story, eos=True)\n",
    "                yield story_ids\n",
    "\n",
    "\n",
    "def prepare(\n",
    "    input_dir: Path = Path(\"../../Data/TinyStoriesChinese/raw_data/train\"),\n",
    "    output_dir: Path = Path(\"../../Data/TinyStoriesChinese/processed_data/train\"),\n",
    "    tokenizer_path: Path = Path(\"../../References/chatglm3-6b\"),\n",
    "    chunk_size: int = (2049 * 8012),\n",
    "    fast_dev_run: bool = False,\n",
    ") -> None:\n",
    "    from litdata.processing.data_processor import DataProcessor\n",
    "\n",
    "    tokenizer_path = extend_checkpoint_dir(tokenizer_path)\n",
    "    tokenizer = Tokenizer(tokenizer_path)\n",
    "    data_recipe = TinyStoriesZhDataRecipe(tokenizer=tokenizer, chunk_size=chunk_size)\n",
    "    data_processor = DataProcessor(\n",
    "        input_dir=str(input_dir),\n",
    "        output_dir=str(output_dir),\n",
    "        fast_dev_run=fast_dev_run,\n",
    "        num_workers=os.cpu_count(),\n",
    "        num_downloaders=1,\n",
    "        # 这里有个「巨坑」，如果不加这一行，处理好的数据配对的index.json里\n",
    "        # 有一个名为\"dim\"的key值会为null，导致后续有一个无法规避的报错\n",
    "        # 但是官方的例子里是没有这一行的，很奇怪为何会有这个问题\n",
    "        item_loader=TokensLoader(),\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    data_processor.run(data_recipe)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time taken: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我这里主要就是把之前实现的`line`处理逻辑封装到`litgpt`的`DataChunkRecipe`中：\n",
    "- `prepare_structure`函数给定路径返回符合我们期望的数据文件的路径列表\n",
    "- `prepare_item`函数给定一个上面的数据文件的路径，根据我们**自定义**的`tokenization`处理逻辑返回一个`np.array`对象\n",
    "  \n",
    "然后，定义了一个`prepare`函数，指定我们数据的输入路径和输出路径以及一些其它参数配置（其实用默认的即可），其余的都交给了`litdata`的`DataProcessor`，它基于我前面定义的`DataChunkRecipe`来处理数据。\n",
    "\n",
    "感兴趣的可以看看`DataProcessor`的源码，里面做了很多并行之类的数据处理优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 先用eval数据集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting multiprocessing start_method to fork. Tip: Libraries relying on lock can hang with `fork`. To use `spawn` in notebooks, move your code to files and import it within the notebook.\n",
      "Storing the files under /home/puppyapple/Server/BigAI/Chinese_LLM_From_Scratch/Data/TinyStoriesChinese/processed_data/val\n",
      "Setup started with fast_dev_run=False.\n",
      "Worker 0 gets 87.2 MB (1 files)\n",
      "Worker 1 gets 0.0 MB (0 files)\n",
      "Worker 2 gets 0.0 MB (0 files)\n",
      "Worker 3 gets 0.0 MB (0 files)\n",
      "Worker 4 gets 0.0 MB (0 files)\n",
      "Worker 5 gets 0.0 MB (0 files)\n",
      "Worker 6 gets 0.0 MB (0 files)\n",
      "Worker 7 gets 0.0 MB (0 files)\n",
      "Worker 8 gets 0.0 MB (0 files)\n",
      "Worker 9 gets 0.0 MB (0 files)\n",
      "Worker 10 gets 0.0 MB (0 files)\n",
      "Worker 11 gets 0.0 MB (0 files)\n",
      "Worker 12 gets 0.0 MB (0 files)\n",
      "Worker 13 gets 0.0 MB (0 files)\n",
      "Worker 14 gets 0.0 MB (0 files)\n",
      "Worker 15 gets 0.0 MB (0 files)\n",
      "Worker 16 gets 0.0 MB (0 files)\n",
      "Worker 17 gets 0.0 MB (0 files)\n",
      "Worker 18 gets 0.0 MB (0 files)\n",
      "Worker 19 gets 0.0 MB (0 files)\n",
      "Worker 20 gets 0.0 MB (0 files)\n",
      "Worker 21 gets 0.0 MB (0 files)\n",
      "Worker 22 gets 0.0 MB (0 files)\n",
      "Worker 23 gets 0.0 MB (0 files)\n",
      "Worker 24 gets 0.0 MB (0 files)\n",
      "Worker 25 gets 0.0 MB (0 files)\n",
      "Worker 26 gets 0.0 MB (0 files)\n",
      "Worker 27 gets 0.0 MB (0 files)\n",
      "Worker 28 gets 0.0 MB (0 files)\n",
      "Worker 29 gets 0.0 MB (0 files)\n",
      "Worker 30 gets 0.0 MB (0 files)\n",
      "Worker 31 gets 0.0 MB (0 files)\n",
      "Setup finished in 0.002 seconds. Found 1 items to process.\n",
      "Starting 32 workers with 1 items. The progress bar is only updated when a worker finishes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 1 is terminating.\n",
      "Worker 2 is terminating.\n",
      "Worker 1 is done.Worker 3 is terminating.\n",
      "\n",
      "Worker 2 is done.\n",
      "Worker 4 is terminating.\n",
      "Worker 3 is done.\n",
      "\n",
      "Worker 5 is terminating.Worker 4 is done.Worker 6 is terminating.\n",
      "\n",
      "Worker 5 is done.Worker 7 is terminating.\n",
      "\n",
      "Worker 8 is terminating.\n",
      "Worker 6 is done.\n",
      "Worker 9 is terminating.\n",
      "\n",
      "Worker 7 is done.Rank 0 inferred the following `['no_header_tensor:16']` data format.Worker 8 is done.\n",
      "\n",
      "Worker 10 is terminating.\n",
      "Worker 9 is done.\n",
      "Worker 11 is terminating.\n",
      "Worker 10 is done.\n",
      "Worker 12 is terminating.\n",
      "Worker 11 is done.Worker 13 is terminating.\n",
      "\n",
      "Worker 14 is terminating.\n",
      "Worker 12 is done.\n",
      "Worker 15 is terminating.\n",
      "Worker 14 is done.Worker 13 is done.\n",
      "\n",
      "Worker 16 is terminating.\n",
      "Worker 17 is terminating.Worker 15 is done.Worker 16 is done.Worker 18 is terminating.\n",
      "\n",
      "\n",
      "\n",
      "Worker 19 is terminating.\n",
      "Worker 18 is done.Worker 17 is done.Worker 20 is terminating.\n",
      "\n",
      "\n",
      "Worker 19 is done.\n",
      "Worker 21 is terminating.\n",
      "Worker 20 is done.Worker 22 is terminating.\n",
      "\n",
      "Worker 23 is terminating.Worker 21 is done.\n",
      "\n",
      "Worker 22 is done.\n",
      "Worker 24 is terminating.\n",
      "Worker 25 is terminating.\n",
      "Worker 23 is done.\n",
      "Worker 26 is terminating.\n",
      "Worker 24 is done.\n",
      "\n",
      "Worker 27 is terminating.Worker 25 is done.\n",
      "Worker 26 is done.Worker 28 is terminating.Workers are ready ! Starting data processing...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9e35d0aa3c43af8b4ff9fba7c70374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Worker 27 is done.\n",
      "Worker 29 is terminating.\n",
      "Worker 30 is terminating.Worker 28 is done.\n",
      "\n",
      "Worker 31 is terminating.\n",
      "Worker 29 is done.\n",
      "Worker 30 is done.\n",
      "Worker 31 is done.\n",
      "Worker 0 is terminating.\n",
      "Worker 0 is done.\n",
      "Workers are finished.\n",
      "Finished data processing!\n",
      "Time taken: 6.83 seconds\n"
     ]
    }
   ],
   "source": [
    "prepare(\n",
    "    input_dir=Path(\"../../Data/TinyStoriesChinese/raw_data/val\"),\n",
    "    output_dir=Path(\"../../Data/TinyStoriesChinese/processed_data/val\"),\n",
    "    tokenizer_path=Path(\"../../References/chatglm3-6b\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（也可以设置`fast_dev_run=True`来处理更少的数据，尤其是debug时十分有用）\n",
    "\n",
    "执行完可以在`processed_data/eval`目录下看到生成的chunk文件。\n",
    "\n",
    "比较一下可以发现从原先的`83m`的`.jsonl`文件压缩到了`13m`的`.bin`，压缩比（83/13≈6.385）还是很可观的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理train数据集\n",
    "在32核的CPU上处理`train`数据集耗时不到`1min`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting multiprocessing start_method to fork. Tip: Libraries relying on lock can hang with `fork`. To use `spawn` in notebooks, move your code to files and import it within the notebook.\n",
      "Storing the files under /home/puppyapple/Server/BigAI/Chinese_LLM_From_Scratch/Data/TinyStoriesChinese/processed_data/train\n",
      "Setup started with fast_dev_run=False.\n",
      "Worker 0 gets 218.5 MB (1 files)\n",
      "Worker 1 gets 218.5 MB (1 files)\n",
      "Worker 2 gets 218.4 MB (1 files)\n",
      "Worker 3 gets 218.4 MB (1 files)\n",
      "Worker 4 gets 218.4 MB (1 files)\n",
      "Worker 5 gets 218.3 MB (1 files)\n",
      "Worker 6 gets 218.3 MB (1 files)\n",
      "Worker 7 gets 218.3 MB (1 files)\n",
      "Worker 8 gets 218.3 MB (1 files)\n",
      "Worker 9 gets 218.2 MB (1 files)\n",
      "Worker 10 gets 218.2 MB (1 files)\n",
      "Worker 11 gets 218.2 MB (1 files)\n",
      "Worker 12 gets 218.1 MB (1 files)\n",
      "Worker 13 gets 218.1 MB (1 files)\n",
      "Worker 14 gets 217.9 MB (1 files)\n",
      "Worker 15 gets 217.8 MB (1 files)\n",
      "Worker 16 gets 257.6 MB (2 files)\n",
      "Worker 17 gets 320.8 MB (2 files)\n",
      "Worker 18 gets 320.7 MB (2 files)\n",
      "Worker 19 gets 321.1 MB (2 files)\n",
      "Worker 20 gets 321.1 MB (2 files)\n",
      "Worker 21 gets 321.1 MB (2 files)\n",
      "Worker 22 gets 321.1 MB (2 files)\n",
      "Worker 23 gets 321.2 MB (2 files)\n",
      "Worker 24 gets 320.9 MB (2 files)\n",
      "Worker 25 gets 320.9 MB (2 files)\n",
      "Worker 26 gets 320.9 MB (2 files)\n",
      "Worker 27 gets 321.0 MB (2 files)\n",
      "Worker 28 gets 320.8 MB (2 files)\n",
      "Worker 29 gets 320.8 MB (2 files)\n",
      "Worker 30 gets 320.1 MB (2 files)\n",
      "Worker 31 gets 297.4 MB (2 files)\n",
      "Setup finished in 0.01 seconds. Found 48 items to process.\n",
      "Starting 32 workers with 48 items. The progress bar is only updated when a worker finishes.\n",
      "Workers are ready ! Starting data processing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703ff0846c1947b4bb5dce2c4ec1508f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 16 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 1 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 0 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 2 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 3 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 9 inferred the following `['no_header_tensor:16']` data format.Rank 17 inferred the following `['no_header_tensor:16']` data format.\n",
      "\n",
      "Rank 5 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 20 inferred the following `['no_header_tensor:16']` data format.Rank 4 inferred the following `['no_header_tensor:16']` data format.Rank 18 inferred the following `['no_header_tensor:16']` data format.\n",
      "\n",
      "Rank 14 inferred the following `['no_header_tensor:16']` data format.\n",
      "\n",
      "Rank 10 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 19 inferred the following `['no_header_tensor:16']` data format.Rank 23 inferred the following `['no_header_tensor:16']` data format.Rank 6 inferred the following `['no_header_tensor:16']` data format.\n",
      "\n",
      "\n",
      "Rank 22 inferred the following `['no_header_tensor:16']` data format.Rank 8 inferred the following `['no_header_tensor:16']` data format.\n",
      "\n",
      "Rank 21 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 7 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 25 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 15 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 12 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 26 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 31 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 24 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 11 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 29 inferred the following `['no_header_tensor:16']` data format.Rank 13 inferred the following `['no_header_tensor:16']` data format.\n",
      "\n",
      "Rank 28 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 27 inferred the following `['no_header_tensor:16']` data format.\n",
      "Rank 30 inferred the following `['no_header_tensor:16']` data format.\n",
      "Worker 14 is terminating.\n",
      "Worker 14 is done.\n",
      "Worker 12 is terminating.\n",
      "Worker 12 is done.\n",
      "Worker 4 is terminating.\n",
      "Worker 4 is done.\n",
      "Worker 10 is terminating.\n",
      "Worker 7 is terminating.\n",
      "Worker 13 is terminating.\n",
      "Worker 10 is done.Worker 15 is terminating.\n",
      "\n",
      "Worker 7 is done.\n",
      "Worker 13 is done.\n",
      "Worker 15 is done.\n",
      "Worker 2 is terminating.\n",
      "Worker 5 is terminating.\n",
      "Worker 2 is done.\n",
      "Worker 5 is done.\n",
      "Worker 16 is terminating.\n",
      "Worker 3 is terminating.\n",
      "Worker 16 is done.\n",
      "Worker 3 is done.\n",
      "Worker 8 is terminating.\n",
      "Worker 8 is done.\n",
      "Worker 9 is terminating.\n",
      "Worker 0 is terminating.\n",
      "Worker 9 is done.\n",
      "Worker 0 is done.\n",
      "Worker 11 is terminating.\n",
      "Worker 11 is done.\n",
      "Worker 1 is terminating.\n",
      "Worker 1 is done.\n",
      "Worker 6 is terminating.\n",
      "Worker 6 is done.\n",
      "Worker 31 is terminating.\n",
      "Worker 31 is done.\n",
      "Worker 29 is terminating.\n",
      "Worker 29 is done.\n",
      "Worker 17 is terminating.\n",
      "Worker 20 is terminating.\n",
      "Worker 26 is terminating.\n",
      "Worker 28 is terminating.\n",
      "Worker 17 is done.\n",
      "Worker 20 is done.\n",
      "Worker 19 is terminating.\n",
      "Worker 22 is terminating.\n",
      "Worker 23 is terminating.\n",
      "Worker 26 is done.\n",
      "Worker 28 is done.\n",
      "Worker 19 is done.\n",
      "Worker 22 is done.\n",
      "Worker 23 is done.\n",
      "Worker 30 is terminating.\n",
      "Worker 18 is terminating.\n",
      "Worker 25 is terminating.\n",
      "Worker 30 is done.\n",
      "Worker 21 is terminating.\n",
      "Worker 18 is done.\n",
      "Worker 25 is done.\n",
      "Worker 21 is done.\n",
      "Worker 27 is terminating.\n",
      "Worker 27 is done.\n",
      "Worker 24 is terminating.\n",
      "Worker 24 is done.\n",
      "Workers are finished.\n",
      "Finished data processing!\n",
      "Time taken: 52.66 seconds\n"
     ]
    }
   ],
   "source": [
    "prepare(\n",
    "    input_dir=Path(\"../../Data/TinyStoriesChinese/raw_data/train\"),\n",
    "    output_dir=Path(\"../../Data/TinyStoriesChinese/processed_data/train\"),\n",
    "    tokenizer_path=Path(\"../../References/chatglm3-6b\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "1. 数据预处理的逻辑主要是将文本转换为数字序列，以便于模型处理。\n",
    "2. 通过`litgpt`的`Tokenizer`可以方便的实现文本到数字序列的转换。\n",
    "3. `litdata`提供了数据处理的`API`，可以方便的封装我们的数据处理逻辑。\n",
    "4. 基于上面的开发，将`TinyStoriesChinese`数据集做了数据划分并完成了预处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
