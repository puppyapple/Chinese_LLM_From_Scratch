# ä»0ï¸âƒ£è·‘é€šä¸­æ–‡å¤§æ¨¡å‹æ„å»ºï½œğŸš€Day002

## Tokenizeré€‰æ‹©

**tokenization**æ˜¯å¤§æ¨¡å‹è®­ç»ƒçš„ç¬¬ä¸€æ­¥ï¼Œæ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—è¡¨ç¤ºï¼ˆåé¢ä¹Ÿèƒ½åå‘decodeå›æ¥ï¼‰ã€‚

å…¶ä¸­ç›®å‰æ¯”è¾ƒä¸»æµçš„æ˜¯[BPE(Byte Pair Encoding)](https://zhuanlan.zhihu.com/p/424631681)ï¼ˆè¯¦ç»†çš„ä»‹ç»å¯ä»¥å‚è€ƒé“¾æ¥æ–‡ç« ï¼Œä¸‹é¢åªè¿›è¡Œä¸€äº›ç®€å•çš„ä»‹ç»ï¼‰ã€‚

**BPE**æ˜¯ä¸€ç§ç®€å•çš„æ•°æ®å‹ç¼©å½¢å¼ï¼Œè¿™ç§æ–¹æ³•ç”¨æ•°æ®ä¸­ä¸å­˜åœ¨çš„ä¸€ä¸ªå­—èŠ‚è¡¨ç¤ºæœ€å¸¸å‡ºç°çš„è¿ç»­å­—èŠ‚æ•°æ®ã€‚è¿™æ ·çš„æ›¿æ¢éœ€è¦é‡å»ºå…¨éƒ¨åŸå§‹æ•°æ®ã€‚

### BPEç®€ä»‹

å‡è®¾æˆ‘ä»¬è¦ç¼–ç å¦‚ä¸‹æ•°æ®

> aaabdaaabac

å­—èŠ‚å¯¹â€œaaâ€å‡ºç°æ¬¡æ•°æœ€å¤šï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨æ•°æ®ä¸­æ²¡æœ‰å‡ºç°çš„å­—èŠ‚â€œZâ€æ›¿æ¢â€œaaâ€å¾—åˆ°æ›¿æ¢è¡¨

> Z <- aa

æ•°æ®è½¬å˜ä¸º

> ZabdZabac

åœ¨è¿™ä¸ªæ•°æ®ä¸­ï¼Œå­—èŠ‚å¯¹â€œZaâ€å‡ºç°çš„æ¬¡æ•°æœ€å¤šï¼Œæˆ‘ä»¬ç”¨å¦å¤–ä¸€ä¸ªå­—èŠ‚â€œYâ€æ¥æ›¿æ¢å®ƒï¼ˆè¿™ç§æƒ…å†µä¸‹ç”±äºæ‰€æœ‰çš„â€œZâ€éƒ½å°†è¢«æ›¿æ¢ï¼Œæ‰€ä»¥ä¹Ÿå¯ä»¥ç”¨â€œZâ€æ¥æ›¿æ¢â€œZaâ€ï¼‰ï¼Œå¾—åˆ°æ›¿æ¢è¡¨ä»¥åŠæ•°æ®

> Z <- aa
> Y <- Za

> YbdYbac

æˆ‘ä»¬å†æ¬¡æ›¿æ¢æœ€å¸¸å‡ºç°çš„å­—èŠ‚å¯¹å¾—åˆ°ï¼š

> Z <- aa
> Y <- Za
> X <- Yb

> XdXac

ç”±äºä¸å†æœ‰é‡å¤å‡ºç°çš„å­—èŠ‚å¯¹ï¼Œæ‰€ä»¥è¿™ä¸ªæ•°æ®ä¸èƒ½å†è¢«è¿›ä¸€æ­¥å‹ç¼©ã€‚

è§£å‹çš„æ—¶å€™ï¼Œå°±æ˜¯æŒ‰ç…§ç›¸åçš„é¡ºåºæ‰§è¡Œæ›¿æ¢è¿‡ç¨‹ã€‚




### ä»¥ChatGLM-6Bçš„tokenizerä¸ºä¾‹


```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)
```

    Setting eos_token is not supported, use the default one.
    Setting pad_token is not supported, use the default one.
    Setting unk_token is not supported, use the default one.



```python
tokenizer("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•")
# fsdfdsf
```




    {'input_ids': [64790, 64792, 30910, 36037, 32882], 'attention_mask': [1, 1, 1, 1, 1], 'position_ids': [0, 1, 2, 3, 4]}




```python
tokenizer.decode(tokenizer("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•")["input_ids"])
```




    '[gMASK] sop è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•'




```python
tokenizer.vocab_size
```




    64798




```python

```
