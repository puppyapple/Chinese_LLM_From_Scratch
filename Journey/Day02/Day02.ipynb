{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零手搓中文大模型｜🚀Day02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tokenization**是大模型训练的第一步，是将文本转换为模型可以理解的数字表示（后面也能反向decode回来）。\n",
    "\n",
    "其中目前比较主流的是[BPE(Byte Pair Encoding)](https://zhuanlan.zhihu.com/p/424631681)（详细的介绍可以参考链接文章，下面只进行一些简单的介绍）。\n",
    "\n",
    "**BPE**是一种简单的数据压缩形式，这种方法用数据中不存在的一个字节表示最常出现的连续字节数据。这样的替换需要重建全部原始数据。\n",
    "\n",
    "### BPE简介\n",
    "\n",
    "假设我们要编码如下数据\n",
    "\n",
    "> aaabdaaabac\n",
    "\n",
    "字节对“aa”出现次数最多，所以我们用数据中没有出现的字节“Z”替换“aa”得到替换表\n",
    "\n",
    "> Z <- aa\n",
    "\n",
    "数据转变为\n",
    "\n",
    "> ZabdZabac\n",
    "\n",
    "在这个数据中，字节对“Za”出现的次数最多，我们用另外一个字节“Y”来替换它（这种情况下由于所有的“Z”都将被替换，所以也可以用“Z”来替换“Za”），得到替换表以及数据\n",
    "\n",
    "> Z <- aa\n",
    "> Y <- Za\n",
    "\n",
    "> YbdYbac\n",
    "\n",
    "我们再次替换最常出现的字节对得到：\n",
    "\n",
    "> Z <- aa\n",
    "> Y <- Za\n",
    "> X <- Yb\n",
    "\n",
    "> XdXac\n",
    "\n",
    "由于不再有重复出现的字节对，所以这个数据不能再被进一步压缩。\n",
    "\n",
    "解压的时候，就是按照相反的顺序执行替换过程。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试Tokenizer(以ChatGLM3-6B的tokenizer为例)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [64790, 64792, 30910, 36037, 32882], 'attention_mask': [1, 1, 1, 1, 1], 'position_ids': [0, 1, 2, 3, 4]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"这是一个测试\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[gMASK] sop 这是一个测试'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(\"这是一个测试\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64798"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def process_line(line, tokenizer, add_eos=True, dtype=np.uint16):\n",
    "    js = json.loads(line)\n",
    "    story = js[\"story_zh\"]\n",
    "    story = tokenizer.encode(story, add_special_tokens=False)\n",
    "    if add_eos:\n",
    "        story.append(tokenizer.eos_token_id)\n",
    "    # 还记得么，这里可以用np.unint16，因为我们的vocab_size是小于65536的\n",
    "    arr = np.array(story, dtype=dtype)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗️这里有几个需要注意的点：\n",
    "- `add_special_tokens`参数的作用是添加特殊token。\n",
    "  是chatglm自定义的例如[gMASK]/sop，属于glm架构里特有的（可以参考[这里](https://github.com/THUDM/ChatGLM3/issues/183)）。\n",
    "  \n",
    "  由于我们后续并不使用glm的架构，因此这里不需要添加，**直接设置为False**。\n",
    "- 需要在末尾加上`eos`标记对应的token_id。\n",
    "- chatglm3-6b使用的词表大小为`64798`，刚好在uint16的表示范围内，所以上面我们给numpy.array设置了`dtype=np.uint16`。\n",
    "\n",
    "拿一行测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30910 56623 56623 54542 50154 31761 31155 31633 31815 54534 32693 54662\n",
      " 55409 31155 35632 31123 31633 34383 57427 47658 54578 34518 31623 55567\n",
      " 55226 31155 56623 56623 54695 39887 32437 55567 55226 31155 54790 41309\n",
      " 52624 31123 56856 32660 55567 55226 31155    13 30955 54834 54546 31123\n",
      " 54613 31404 30955 36213 31155 54613 36660 54563 54834 43881 32024 31155\n",
      " 56623 56623 32707 54657 33436 31155 54790 54937 56567 40714 31123 38502\n",
      " 56653 55483 31155    13 54613 32984 56623 56623 31155 54572 31897 54790\n",
      " 54657 35245 31155 36551 54695 56567 55567 55226 31155 33152 56623 56623\n",
      " 51556 31797 39055 31155 31694 56623 56623 31631 51556 31155 54790 54937\n",
      " 56567 54937 54929 31155 54790 55409 40915 34492 54537 31155    13 30955\n",
      " 54546 32591 56567 55567 55226 55398 31123 56623 56623 31514 30955 54613\n",
      " 54761 31155 56623 56623 54721 33906 31804 54887 31155 54790 46977 56567\n",
      " 55567 55226 31155 54613 31897 32960 54597 31155 54572 54942 34675 31155\n",
      "    13 56623 56623 56567 40915 54589 31123 36467 33501 31155 54790 54708\n",
      " 55567 55226 54547 57456 32246 31123 36712 34245 31155 54790 56901 55328\n",
      " 54537 55673 31155 54790 56399 37247 31155    13 30955 58394 56657 31123\n",
      " 58394 56657 31123 58394 56657 31404 30955 36213 31155 35957 55227 54613\n",
      " 31155 54790 31772 47554 31934 54790 31155 54688 54613 33551 33892 31155\n",
      " 54572 34247 31155    13 56623 56623 32707 54657 52992 31155 54790 31772\n",
      " 54790 54558 54542 54613 32097 55567 55226 31155 54790 31772 33152 33892\n",
      " 37322 54790 31155 54790 54531 60337 54531 57635 54563 35220 52624 31155\n",
      " 54790 31857 33277 32086 44829 49102 54547 31155 35328 43352 41147 31155\n",
      " 54572 42393 32233 31155    13 56623 56623 40466 31155 54790 31897 54613\n",
      " 33058 31155 54790 55947 32660 31804 41147 31155 54790 31772 38711 33857\n",
      " 31155 54790 54695 37300 31155 54790 54695 32462 31705 31761 31155     2]\n",
      "莉莉和本是朋友。他们喜欢在公园里玩。有一天，他们在一棵大树下看到了一个秋千。莉莉想试试那个秋千。她跑到树下，爬上了秋千。\n",
      "\"推我，本！\"她说。本轻轻地推了她一下。莉莉感到很开心。她越荡越高，笑着喊叫。\n",
      "本看着莉莉。他觉得她很可爱。他也想荡秋千。他在莉莉停下来之后等着。但是莉莉没有停下来。她越荡越快。她玩得太高兴了。\n",
      "\"我也可以荡秋千吗，莉莉？\"本问。莉莉没听到他的话。她忙着荡秋千。本觉得很难过。他走开了。\n",
      "莉莉荡得太高，失去了平衡。她从秋千上摔下来，落在地上。她扭伤了脚。她哭了起来。\n",
      "\"哎呀，哎呀，哎呀！\"她说。她在找本。她希望他能帮助她。但本不在那里。他走了。\n",
      "莉莉感到很抱歉。她希望她能和本分享秋千。她希望他在那里拥抱她。她一瘸一拐地走到树下。她看到有什么东西挂在树枝上。那是本的帽子。他留给她的。\n",
      "莉莉笑了。她觉得本很好。她戴上了他的帽子。她希望他会回来。她想道歉。她想再次成为朋友。\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../../Data/TinyStoriesChinese/train/data00_zh.jsonl\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        data = process_line(line, tokenizer)\n",
    "        print(data)\n",
    "        print(tokenizer.decode(data))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选择ChatGLM3-6B的tokenizer的原因\n",
    "\n",
    "该词表大小为64798，值得注意的是：这是一个很妙的数字，因为它**刚好在uint16的表示范围（0～65535的无符号整数）**，每一个token只需要两个字节即可表示。\n",
    "\n",
    "当我们的语料较大时候，相比常用的int32可以**节省一半的存储空间**。\n",
    "\n",
    "另外这里选择一个小尺寸的词表还有一个更重要的原因：我们后面的模型会选择一个小参数量的，如果词表过大，会导致**大部分参数被embedding层占用**，而无法训练出更好的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
