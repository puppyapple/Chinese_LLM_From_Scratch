{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零手搓中文大模型｜🚀Day04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面已经完成了**数据预处理**，今天我们来研究一下**模型的配置**。\n",
    "\n",
    "`litgpt`使用的配置文件和`transformers`有点不太一样，它的仓库里提供了一些预训练所用的`yaml`[配置文件样例](https://github.com/Lightning-AI/litgpt/tree/main/config_hub)。这个主要用于需要自定义模型的场景。\n",
    "\n",
    "另外`litgpt`也内置了一些`huggingface`上的[现成模型](https://github.com/Lightning-AI/litgpt/blob/main/litgpt/config.py)，可以直接拿来使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置文件示例\n",
    "以下是我这次定义的一个配置文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "# The name of the model to pretrain. Choose from names in ``litgpt.config``. Mutually exclusive with\n",
    "# ``model_config``. (type: Optional[str], default: null)\n",
    "model_name: microstories\n",
    "\n",
    "# A ``litgpt.Config`` object to define the model architecture. Mutually exclusive with\n",
    "# ``model_config``. (type: Optional[Config], default: null)\n",
    "model_config:\n",
    "  name: microstories\n",
    "  hf_config: {}\n",
    "  scale_embeddings: false\n",
    "  block_size: 512\n",
    "  padded_vocab_size: 65024\n",
    "  vocab_size: 64798\n",
    "  n_layer: 6\n",
    "  n_head: 6\n",
    "  n_query_groups: 6\n",
    "  n_embd: 512\n",
    "  head_size: 48\n",
    "  rotary_percentage: 1.0\n",
    "  parallel_residual: false\n",
    "  bias: false\n",
    "  norm_class_name: RMSNorm\n",
    "  mlp_class_name: LLaMAMLP\n",
    "  intermediate_size: 768\n",
    "\n",
    "# Directory in which to save checkpoints and logs. If running in a Lightning Studio Job, look for it in\n",
    "# /teamspace/jobs/<job-name>/share. (type: <class 'Path'>, default: out/pretrain)\n",
    "out_dir: /home/puppyapple/Server/BigAI/Chinese_LLM_From_Scratch/Experiments/Output/pretrain/microstories\n",
    "\n",
    "# The precision to use for pretraining. Possible choices: \"bf16-true\", \"bf16-mixed\", \"32-true\". (type: Optional[str], default: null)\n",
    "precision: bf16-mixed\n",
    "\n",
    "# Optional path to a checkpoint directory to initialize the model from.\n",
    "# Useful for continued pretraining. Mutually exclusive with ``resume``. (type: Optional[Path], default: null)\n",
    "initial_checkpoint_dir:\n",
    "\n",
    "# Path to a checkpoint directory to resume from in case training was interrupted, or ``True`` to resume\n",
    "# from the latest checkpoint in ``out_dir``. An error will be raised if no checkpoint is found. Passing\n",
    "# ``'auto'`` will resume from the latest checkpoint but not error if no checkpoint exists.\n",
    "# (type: Union[bool, Literal[\"auto\"], Path], default: False)\n",
    "resume: false\n",
    "\n",
    "# Data-related arguments. If not provided, the default is ``litgpt.data.TinyLlama``.\n",
    "data:\n",
    "  # TinyStories\n",
    "  class_path: litgpt.data.LitData\n",
    "  init_args:\n",
    "    data_path: Chinese_LLM_From_Scratch/Data/TinyStoriesChinese/processed_data\n",
    "    split_names:\n",
    "      - train\n",
    "      - val\n",
    "\n",
    "# Training-related arguments. See ``litgpt.args.TrainArgs`` for details\n",
    "train:\n",
    "  # Number of optimizer steps between saving checkpoints (type: Optional[int], default: 1000)\n",
    "  save_interval: 1000\n",
    "\n",
    "  # Number of iterations between logging calls (type: int, default: 1)\n",
    "  log_interval: 1\n",
    "\n",
    "  # Number of samples between optimizer steps across data-parallel ranks (type: int, default: 512)\n",
    "  global_batch_size: 512\n",
    "\n",
    "  # Number of samples per data-parallel rank (type: int, default: 4)\n",
    "  micro_batch_size: 48\n",
    "\n",
    "  # Number of iterations with learning rate warmup active (type: int, default: 2000)\n",
    "  lr_warmup_steps: 1000\n",
    "\n",
    "  # Number of epochs to train on (type: Optional[int], default: null)\n",
    "  epochs:\n",
    "\n",
    "  # Total number of tokens to train on (type: Optional[int], default: 3000000000000)\n",
    "  max_tokens: 3000000000000\n",
    "\n",
    "  # Limits the number of optimizer steps to run. (type: Optional[int], default: null)\n",
    "  max_steps:\n",
    "\n",
    "  # Limits the length of samples. Off by default (type: Optional[int], default: null)\n",
    "  max_seq_length: 512\n",
    "\n",
    "  # Whether to tie the embedding weights with the language modeling head weights. (type: Optional[bool], default: False)\n",
    "  tie_embeddings: true\n",
    "\n",
    "  #   (type: Optional[float], default: 1.0)\n",
    "  max_norm: 1.0\n",
    "\n",
    "  #   (type: float, default: 4e-05)\n",
    "  min_lr: 0.0\n",
    "\n",
    "# Evaluation-related arguments. See ``litgpt.args.EvalArgs`` for details\n",
    "eval:\n",
    "  # Number of optimizer steps between evaluation calls (type: int, default: 1000)\n",
    "  interval: 2000\n",
    "\n",
    "  # Number of tokens to generate (type: Optional[int], default: null)\n",
    "  max_new_tokens:\n",
    "\n",
    "  # Number of iterations (type: int, default: 100)\n",
    "  max_iters: 100\n",
    "\n",
    "  # Whether to evaluate on the validation set at the beginning of the training\n",
    "  initial_validation: false\n",
    "\n",
    "  # Whether to evaluate on the validation set at the end the training\n",
    "  final_validation: false\n",
    "\n",
    "# Optimizer-related arguments\n",
    "optimizer:\n",
    "  class_path: torch.optim.AdamW\n",
    "\n",
    "  init_args:\n",
    "    #   (type: float, default: 0.001)\n",
    "    lr: 0.0005\n",
    "\n",
    "    #   (type: float, default: 0.01)\n",
    "    weight_decay: 0.1\n",
    "\n",
    "    #   (type: tuple, default: (0.9,0.999))\n",
    "    betas:\n",
    "      - 0.9\n",
    "      - 0.95\n",
    "\n",
    "# How many devices/GPUs to use. Uses all GPUs by default. (type: Union[int, str], default: auto)\n",
    "devices: auto\n",
    "\n",
    "# How many nodes to use. (type: int, default: 1)\n",
    "num_nodes: 1\n",
    "\n",
    "# Optional path to the tokenizer dir that was used for preprocessing the dataset. Only some data\n",
    "# module require this. (type: Optional[Path], default: null)\n",
    "tokenizer_dir: Chinese_LLM_From_Scratch/References/chatglm3-6b\n",
    "\n",
    "# The name of the logger to send metrics to. (type: Literal['wandb', 'tensorboard', 'csv'], default: tensorboard)\n",
    "logger_name: wandb\n",
    "\n",
    "# The random seed to use for reproducibility. (type: int, default: 42)\n",
    "seed: 42\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
