{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä»é›¶æ‰‹æ“ä¸­æ–‡å¤§æ¨¡å‹ï½œğŸš€Day04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‰é¢å·²ç»å®Œæˆäº†**æ•°æ®é¢„å¤„ç†**ï¼Œä»Šå¤©æˆ‘ä»¬æ¥ç ”ç©¶ä¸€ä¸‹**æ¨¡å‹çš„é…ç½®**ã€‚\n",
    "\n",
    "`litgpt`ä½¿ç”¨çš„é…ç½®æ–‡ä»¶å’Œ`transformers`æœ‰ç‚¹ä¸å¤ªä¸€æ ·ï¼Œå®ƒçš„ä»“åº“é‡Œæä¾›äº†ä¸€äº›é¢„è®­ç»ƒæ‰€ç”¨çš„`yaml`[é…ç½®æ–‡ä»¶æ ·ä¾‹](https://github.com/Lightning-AI/litgpt/tree/main/config_hub)ã€‚è¿™ä¸ªä¸»è¦ç”¨äºéœ€è¦è‡ªå®šä¹‰æ¨¡å‹çš„åœºæ™¯ã€‚\n",
    "\n",
    "å¦å¤–`litgpt`ä¹Ÿå†…ç½®äº†ä¸€äº›`huggingface`ä¸Šçš„[ç°æˆæ¨¡å‹](https://github.com/Lightning-AI/litgpt/blob/main/litgpt/config.py)ï¼Œå¯ä»¥ç›´æ¥æ‹¿æ¥ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®­ç»ƒé…ç½®æ–‡ä»¶\n",
    "ä»¥ä¸‹æ˜¯æˆ‘è¿™æ¬¡å®šä¹‰çš„ä¸€ä¸ªé…ç½®æ–‡ä»¶ã€‚\n",
    "\n",
    "å†…å®¹æœ‰ç‚¹å¤šï¼Œä½†æ˜¯è¿˜æ˜¯éƒ½åˆ—ä¸¾å‡ºæ¥äº†ï¼Œå¯ä»¥ç›´æ¥è·³åˆ°åé¢å¯¹ä¸€äº›å…³é”®å‚æ•°çš„è§£é‡Šã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "# The name of the model to pretrain. Choose from names in ``litgpt.config``. Mutually exclusive with\n",
    "# ``model_config``. (type: Optional[str], default: null)\n",
    "model_name: microstories\n",
    "\n",
    "# A ``litgpt.Config`` object to define the model architecture. Mutually exclusive with\n",
    "# ``model_config``. (type: Optional[Config], default: null)\n",
    "model_config:\n",
    "  name: microstories\n",
    "  hf_config: {}\n",
    "  scale_embeddings: false\n",
    "  block_size: 512\n",
    "  padded_vocab_size: 65024\n",
    "  vocab_size: 64798\n",
    "  n_layer: 6\n",
    "  n_head: 6\n",
    "  n_query_groups: 6\n",
    "  n_embd: 512\n",
    "  head_size: 48\n",
    "  rotary_percentage: 1.0\n",
    "  parallel_residual: false\n",
    "  bias: false\n",
    "  norm_class_name: RMSNorm\n",
    "  mlp_class_name: LLaMAMLP\n",
    "  intermediate_size: 768\n",
    "\n",
    "# Directory in which to save checkpoints and logs. If running in a Lightning Studio Job, look for it in\n",
    "# /teamspace/jobs/<job-name>/share. (type: <class 'Path'>, default: out/pretrain)\n",
    "out_dir: Chinese_LLM_From_Scratch/Experiments/Output/pretrain/microstories\n",
    "\n",
    "# The precision to use for pretraining. Possible choices: \"bf16-true\", \"bf16-mixed\", \"32-true\". (type: Optional[str], default: null)\n",
    "precision: bf16-mixed\n",
    "\n",
    "# Optional path to a checkpoint directory to initialize the model from.\n",
    "# Useful for continued pretraining. Mutually exclusive with ``resume``. (type: Optional[Path], default: null)\n",
    "initial_checkpoint_dir:\n",
    "\n",
    "# Path to a checkpoint directory to resume from in case training was interrupted, or ``True`` to resume\n",
    "# from the latest checkpoint in ``out_dir``. An error will be raised if no checkpoint is found. Passing\n",
    "# ``'auto'`` will resume from the latest checkpoint but not error if no checkpoint exists.\n",
    "# (type: Union[bool, Literal[\"auto\"], Path], default: False)\n",
    "resume: true\n",
    "\n",
    "# Data-related arguments. If not provided, the default is ``litgpt.data.TinyLlama``.\n",
    "data:\n",
    "  # TinyStories\n",
    "  class_path: litgpt.data.LitData\n",
    "  init_args:\n",
    "    data_path: Chinese_LLM_From_Scratch/Data/TinyStoriesChinese/processed_data\n",
    "    split_names:\n",
    "      - train\n",
    "      - val\n",
    "\n",
    "# Training-related arguments. See ``litgpt.args.TrainArgs`` for details\n",
    "train:\n",
    "  # Number of optimizer steps between saving checkpoints (type: Optional[int], default: 1000)\n",
    "  save_interval: 1000\n",
    "\n",
    "  # Number of iterations between logging calls (type: int, default: 1)\n",
    "  log_interval: 1\n",
    "\n",
    "  # Number of samples between optimizer steps across data-parallel ranks (type: int, default: 512)\n",
    "  global_batch_size: 512\n",
    "\n",
    "  # Number of samples per data-parallel rank (type: int, default: 4)\n",
    "  micro_batch_size: 32\n",
    "\n",
    "  # Number of iterations with learning rate warmup active (type: int, default: 2000)\n",
    "  lr_warmup_steps: 1000\n",
    "\n",
    "  # Number of epochs to train on (type: Optional[int], default: null)\n",
    "  epochs:\n",
    "\n",
    "  # Total number of tokens to train on (type: Optional[int], default: 3000000000000)\n",
    "  max_tokens: 3000000000000\n",
    "\n",
    "  # Limits the number of optimizer steps to run. (type: Optional[int], default: null)\n",
    "  max_steps:\n",
    "\n",
    "  # Limits the length of samples. Off by default (type: Optional[int], default: null)\n",
    "  max_seq_length: 512\n",
    "\n",
    "  # Whether to tie the embedding weights with the language modeling head weights. (type: Optional[bool], default: False)\n",
    "  tie_embeddings: true\n",
    "\n",
    "  #   (type: Optional[float], default: 1.0)\n",
    "  max_norm: 1.0\n",
    "\n",
    "  #   (type: float, default: 4e-05)\n",
    "  min_lr: 0.0\n",
    "\n",
    "# Evaluation-related arguments. See ``litgpt.args.EvalArgs`` for details\n",
    "eval:\n",
    "  # Number of optimizer steps between evaluation calls (type: int, default: 1000)\n",
    "  interval: 2000\n",
    "\n",
    "  # Number of tokens to generate (type: Optional[int], default: null)\n",
    "  max_new_tokens:\n",
    "\n",
    "  # Number of iterations (type: int, default: 100)\n",
    "  max_iters: 100\n",
    "\n",
    "  # Whether to evaluate on the validation set at the beginning of the training\n",
    "  initial_validation: false\n",
    "\n",
    "  # Whether to evaluate on the validation set at the end the training\n",
    "  final_validation: false\n",
    "\n",
    "# Optimizer-related arguments\n",
    "optimizer:\n",
    "  class_path: torch.optim.AdamW\n",
    "\n",
    "  init_args:\n",
    "    #   (type: float, default: 0.001)\n",
    "    lr: 0.0005\n",
    "\n",
    "    #   (type: float, default: 0.01)\n",
    "    weight_decay: 0.1\n",
    "\n",
    "    #   (type: tuple, default: (0.9,0.999))\n",
    "    betas:\n",
    "      - 0.9\n",
    "      - 0.95\n",
    "\n",
    "# How many devices/GPUs to use. Uses all GPUs by default. (type: Union[int, str], default: auto)\n",
    "devices: auto\n",
    "\n",
    "# How many nodes to use. (type: int, default: 1)\n",
    "num_nodes: 1\n",
    "\n",
    "# Optional path to the tokenizer dir that was used for preprocessing the dataset. Only some data\n",
    "# module require this. (type: Optional[Path], default: null)\n",
    "tokenizer_dir: Chinese_LLM_From_Scratch/References/chatglm3-6b\n",
    "\n",
    "# The name of the logger to send metrics to. (type: Literal['wandb', 'tensorboard', 'csv'], default: tensorboard)\n",
    "logger_name: wandb\n",
    "\n",
    "# The random seed to use for reproducibility. (type: int, default: 42)\n",
    "seed: 42\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "model_config:\n",
    "  name: microstories\n",
    "  hf_config: {}\n",
    "  scale_embeddings: false\n",
    "  block_size: 512\n",
    "  padded_vocab_size: 65024\n",
    "  vocab_size: 64798\n",
    "  n_layer: 6\n",
    "  n_head: 6\n",
    "  n_query_groups: 6\n",
    "  n_embd: 512\n",
    "  head_size: 48\n",
    "  rotary_percentage: 1.0\n",
    "  parallel_residual: false\n",
    "  bias: false\n",
    "  norm_class_name: RMSNorm\n",
    "  mlp_class_name: LLaMAMLP\n",
    "  intermediate_size: 768\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `scale_embeddings`æ§åˆ¶æ˜¯å¦å¯¹embeddingè¿›è¡Œç¼©æ”¾ã€‚\n",
    "  \n",
    "  ![scale_embedding](https://erxuanyi-1257355350.cos.ap-beijing.myqcloud.com/scale_embedding.png)\n",
    "  \n",
    "  å¦‚æœä¸º`True`ï¼Œé‚£ä¹ˆåœ¨`forward`å‡½æ•°ä¸­ä¼šå¯¹`embedding`è¿›è¡Œç¼©æ”¾ã€‚æ³¨æ„ä¸ªç¼©æ”¾å’Œ`sefl-attention`ä¸­çš„ç¼©æ”¾ä¸æ˜¯ä¸€å›äº‹ï¼Œä¸è¦å¼„æ··äº†ã€‚\n",
    "  å…¶å®ä¹Ÿæœ‰å¾ˆå¤šè®¨è®ºå…³äºè¿™ä¸ªåœ°æ–¹è¿™ä¸€æ­¥**æ˜¯å¦æœ‰å¿…è¦**çš„ï¼Œç›®å‰çœ‹æ¥ä¼¼ä¹æ˜¯åŒºåˆ«ä¸å¤§ï¼Œå¯ä»¥è®¾ç½®ä¸º`False`ã€‚\n",
    "- `transformer`ä¸­çš„`block_size`ï¼Œä¹Ÿå°±æ˜¯`max_seq_length`ã€‚\n",
    "- `padded_vovab_size`å’Œ`vocab_size`ç›´æ¥å–è‡ª`tokenizer`ã€‚\n",
    "- `n_layer`å’Œ`n_head`éƒ½æ˜¯`6`ï¼Œæ„å»ºäº†ä¸€ä¸ª`6`å±‚`6`å¤´çš„`transformer`ã€‚\n",
    "- `n_query_groups`æ˜¯`6`ï¼Œè¿™æ˜¯`GQA(Grouped-Query Attention)`çš„ä¸€ä¸ªå‚æ•°ï¼Œæ§åˆ¶`query`çš„åˆ†ç»„ã€‚å½“`n_query_groups`ç­‰äº`n_head`æ—¶ï¼Œå…¶å®å°±æ˜¯`MHA(Multi-Head Attention)`ã€‚ä¸‹é¢è¿™ä¸ªå›¾æ¯”è¾ƒç›´è§‚ï¼š\n",
    "  \n",
    "  ![GQA_2](https://erxuanyi-1257355350.cos.ap-beijing.myqcloud.com/GQA_2.png)\n",
    "\n",
    "- å¤´çš„å¤§å°`head_size`æ˜¯`48`ï¼Œ`n_embd`æ˜¯`512`ã€‚\n",
    "- `rotary_percentage`æ˜¯`1.0`ï¼Œè¿™ä¸ªæ˜¯`æ—‹è½¬ç¼–ç ï¼ˆRotary Position Embedding, RoPEï¼‰`çš„æœ‰å…³å‚æ•°ï¼Œè¿™é‡Œå…ˆä¸å±•å¼€ä»‹ç»äº†ã€‚\n",
    "- `parallel_residual`æ˜¯`false`ï¼Œå…³äº`parallel residual`å’Œ`non-parallel residual`çš„è§£é‡Šå¯ä»¥å‚è€ƒè¿™ä¸ªå›¾ï¼š\n",
    "  \n",
    "  ![parallel_residual](https://erxuanyi-1257355350.cos.ap-beijing.myqcloud.com/parallel_residual.png)\n",
    "- `bias`æ§åˆ¶`Linear`å±‚çš„`bias`æ˜¯å¦å­˜åœ¨ï¼Œç°åœ¨å¤§å¤šæ¨¡å‹ä¸€èˆ¬éƒ½æ˜¯`false`ã€‚\n",
    "- `norm_class_name`æ˜¯`RMSNorm`ï¼Œ`mlp_class_name`æ˜¯`LLaMAMLP`ï¼Œå…·ä½“å¯ä»¥å‚è§`litgpt`é‡Œ[`model.py`](https://github.com/Lightning-AI/litgpt/blob/main/litgpt/model.py#L30)ä¸­çš„å®ç°ã€‚\n",
    "- `intermediate_size`æ˜¯`768`ï¼Œè¿™ä¸ªæ˜¯ä¸Šé¢çš„`MLP`ä¸­é—´å±‚çš„å¤§å°ã€‚\n",
    "\n",
    "æŒ‰ç…§ä¸Šé¢çš„é…ç½®å¾—åˆ°çš„æ¨¡å‹å‚æ•°é‡åœ¨`44M`å·¦å³ï¼Œä¹Ÿå°±æ˜¯åªæœ‰`0.044B`çš„å¤§å°ã€‚\n",
    "\n",
    "ä½†æ ¹æ®å¾®è½¯çš„[TinyStories](https://arxiv.org/pdf/2305.07759)è®ºæ–‡ç»“è®ºï¼Œ`10-80M`çº§åˆ«çš„æ¨¡å‹èƒ½åœ¨å°æ•…äº‹ç”Ÿæˆè¿™ç§ç®€å•çš„è¯­è¨€ä»»åŠ¡ä¸Šè¾¾åˆ°ä¸é”™çš„æ•ˆæœï¼ˆä¾æ—§èƒ½è¯´äººè¯ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å…¶ä»–å‚æ•°\n",
    "\n",
    "å…¶ä½™çš„éƒ½æ˜¯ä¸€äº›è®­ç»ƒçš„å‚æ•°ï¼Œæ¯”å¦‚`batch_size`ï¼Œ`lr`ï¼Œ`weight_decay`ç­‰ç­‰ï¼Œè¿™äº›éƒ½æ˜¯æ¯”è¾ƒå¸¸è§çš„å‚æ•°ï¼Œä¸å†èµ˜è¿°ã€‚\n",
    "\n",
    "`logger`æˆ‘è¿™é‡Œé€‰æ‹©çš„æ˜¯`wandb`ï¼Œå¯ä»¥ç›´æ¥åœ¨`wandb`ä¸ŠæŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸€äº›æŒ‡æ ‡ã€‚\n",
    "\n",
    "`data`è®¾ç½®æˆä¹‹å‰é¢„å¤„ç†å¥½çš„æ•°æ®é›†çš„è·¯å¾„ï¼ˆå…¶ä¸­æŒ‡å®šäº†åŠ è½½æ•°æ®æ‰€ç”¨çš„`litdata`çš„ç±»åï¼‰\n",
    "\n",
    "`tokenizer_dir`æ˜¯é€‰ç”¨çš„æˆ–è€…è‡ªå·±è®­ç»ƒå¥½çš„`tokenizer`çš„è·¯å¾„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¯åŠ¨è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "litgpt pretrain --config Experiments/configs/microstories.yaml\n",
    "```\n",
    "é¢„è®­ç»ƒå¯åŠ¨çš„å‘½ä»¤éå¸¸ç®€å•ï¼Œåªéœ€è¦æŒ‡å®šä¸Šé¢çš„é…ç½®æ–‡ä»¶çš„è·¯å¾„å³å¯ã€‚\n",
    "\n",
    "ä¸å‡ºæ„å¤–åœ°è¯æ¨¡å‹å°±èƒ½å¼€å§‹è®­ç»ƒäº†ï¼Œå¯ä»¥åœ¨`wandb`ä¸ŠæŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡æ ‡ã€‚\n",
    "\n",
    "æˆ‘çš„æ¨¡å‹å…¶å®å·²ç»è®­ç»ƒäº†ä¸€æ®µæ—¶é—´ï¼Œshowä¸€ä¸‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„å›¾è¡¨ï¼š\n",
    "\n",
    "![image](https://erxuanyi-1257355350.cos.ap-beijing.myqcloud.com/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å°ç»“\n",
    "1. è¯¦ç»†ä»‹ç»äº†`litgpt`çš„é¢„è®­ç»ƒæ¨¡å‹é…ç½®æ–‡ä»¶ã€‚\n",
    "2. é¡ºå¸¦è§£é‡Šäº†ä¸€äº›é‡è¦å‚æ•°çš„åŸç†ã€‚\n",
    "3. è®­ç»ƒå¯åŠ¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
