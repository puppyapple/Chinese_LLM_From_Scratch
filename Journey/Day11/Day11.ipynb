{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä»é›¶æ‰‹æ“ä¸­æ–‡å¤§æ¨¡å‹ï½œğŸš€Day11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ä¹‹å‰å·²ç»æŠŠ`SFT`é˜¶æ®µç»™è·‘é€šäº†ï¼Œå°½ç®¡æ•´ä½“æ•ˆæœå·®å¼ºäººæ„ï¼Œä½†è‡³å°‘è¯æ˜åœ¨è¿™ä¹ˆå°çš„å‚æ•°é‡çº§ä¸Šä¹Ÿæ˜¯å¯è¡Œçš„ã€‚\n",
    "\n",
    "æ¥ä¸‹æ¥æˆ‘ç»§ç»­å°è¯•ä¸€ä¸‹`DPO`é˜¶æ®µï¼Œä½†æ˜¯é¦–å…ˆä¾ç„¶å¾—**ææ•°æ®**ã€‚\n",
    "\n",
    "## DPOæ•°æ®æ„é€ \n",
    "\n",
    "`DPO`æ•°æ®ä¸»è¦æ˜¯éœ€è¦è·å¾—`rejected`å’Œ`chosen`çš„æ•°æ®å¯¹ã€‚\n",
    "\n",
    "`chosen`çš„æ•°æ®å¾ˆå¥½è¯´ï¼Œç›´æ¥ä½¿ç”¨`SFT`æ•°æ®é‡Œçš„`response`å³å¯ã€‚\n",
    "\n",
    "è€Œ`rejected`çš„æ•°æ®å…¶å®å°±æ˜¯å°±æ˜¯å›ç­”è´¨é‡ç›¸å¯¹è¾ƒå·®çš„æ•°æ®ï¼Œå¾ˆå®¹æ˜“å°±èƒ½æƒ³`SFT`ä¹‹åçš„æ¨¡å‹æ ¹æ®`prompt`ç»™å‡ºçš„`response`è‚¯å®šæ˜¯è´¨é‡ä½äº`ground truth`çš„ï¼Œå¤©ç„¶å°±å¯ä»¥ä½œä¸º`rejected`çš„æ•°æ®ã€‚\n",
    "\n",
    "æ„é€ è·¯å¾„å€’æ˜¯å¾ˆå®¹æ˜“ï¼Œä½†æ˜¯æ ¹æ®ä¹‹å‰è·‘ç”Ÿæˆçš„ç»éªŒï¼Œå•æ¡`prompt`æ•°æ®ç”Ÿæˆ`response`çš„æ—¶é—´å¤§æ¦‚åœ¨0.5ç§’å·¦å³ï¼Œå¦‚æœä½¿ç”¨`SFT`æ•°æ®å…¨é‡ï¼ˆåœ¨æˆ‘æœºåˆ¶çš„æ•°æ®å¢å¼ºä¹‹ä¸‹ä»1.5wå˜æˆäº†7wå¤šï¼‰ç”Ÿæˆ`DPO`æ•°æ®ï¼Œé‚£ä¹ˆå¯èƒ½éœ€è¦10å°æ—¶å·¦å³çš„æ—¶é—´ã€‚\n",
    "\n",
    "é‚£ä¹ˆå¹¶å‘ç”Ÿæˆå°±æ˜¾å¾—å°¤ä¸ºé‡è¦ï¼Œå¯è¡Œçš„æ–¹æ³•æœ‰ä¸¤ç§ï¼š\n",
    "1. åŠ è½½å¤šä¸ªæ¨¡å‹çš„å®ä¾‹ï¼Œå°†æ•°æ®å‡ç­‰åˆ‡åˆ†æˆå¤šä¸ª`chunks`æ¯ä¸ªæ¨¡å‹ç”Ÿæˆä¸€éƒ¨åˆ†æ•°æ®ï¼Œæœ€åå†åˆå¹¶ã€‚\n",
    "2. å°†æ¨¡å‹éƒ¨ç½²æˆ`API`æ¥å£ï¼Œä½¿ç”¨`aiohttp`å¼‚æ­¥è¯·æ±‚ã€‚\n",
    "\n",
    "> å…¶å®åº”è¯¥åŒæ—¶ç”¨ä¸Š`batch inference`ï¼Œä½†`litgpt`åº“è¿™å—çš„`feature`è¿˜åœ¨å¼€å‘ä¸­ï¼Œæˆ‘è‡ªå·±é­”æ”¹æ‹…å¿ƒæä¸å®šï¼Œå°±å…ˆä¸å°è¯•äº†ã€‚\n",
    "\n",
    "æ˜¾ç„¶åè€…çš„ç¨³å®šæ€§ä¼šæ›´å¥½ï¼Œé‚£ä¹ˆè¯ä¸å¤šè¯´ï¼Œç›´æ¥ä¸Šä»£ç ğŸ‘‡ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import argparse\n",
    "import hashlib\n",
    "import time\n",
    "import atexit\n",
    "from tqdm import tqdm\n",
    "from litgpt.prompts import MicroStories\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "def hash_prompt(prompt):\n",
    "    return hashlib.md5(prompt.encode()).hexdigest()\n",
    "\n",
    "\n",
    "cache = {}\n",
    "error_cache = {}\n",
    "\n",
    "\n",
    "def save_caches():\n",
    "    with open(\"dpo_cache.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cache, f, ensure_ascii=False, indent=2)\n",
    "    with open(\"error_cache.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(error_cache, f, ensure_ascii=False, indent=2)\n",
    "    logger.info(\"ç¼“å­˜å·²ä¿å­˜\")\n",
    "\n",
    "\n",
    "atexit.register(save_caches)\n",
    "\n",
    "\n",
    "async def generate_response(session, prompt, semaphore):\n",
    "    prompt_hash = hash_prompt(prompt)\n",
    "    if prompt_hash in cache:\n",
    "        return cache[prompt_hash]\n",
    "\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            async with session.post(\n",
    "                \"http://127.0.0.1:8000/predict\", json={\"prompt\": prompt}\n",
    "            ) as response:\n",
    "                result = await response.json()\n",
    "                cache[prompt_hash] = result\n",
    "                return result\n",
    "        except Exception as e:\n",
    "            error_msg = f\"ç”Ÿæˆå“åº”æ—¶å‡ºé”™: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            error_cache[prompt_hash] = error_msg\n",
    "            return None\n",
    "\n",
    "\n",
    "async def main(concurrency, test_mode):\n",
    "    global cache, error_cache\n",
    "    ms = MicroStories()\n",
    "\n",
    "    with open(\n",
    "        \"../../Data/TinyStoriesInstruct/sft_data_v2.json\", \"r\", encoding=\"utf-8\"\n",
    "    ) as f:\n",
    "        sft_data = json.load(f)\n",
    "\n",
    "    if test_mode:\n",
    "        sft_data = sft_data[:100]\n",
    "\n",
    "    # è¯»å–ç¼“å­˜\n",
    "    try:\n",
    "        with open(\"dpo_cache.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            cache = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        cache = {}\n",
    "\n",
    "    try:\n",
    "        with open(\"error_cache.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            error_cache = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        error_cache = {}\n",
    "\n",
    "    semaphore = asyncio.Semaphore(concurrency)\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for i, case in enumerate(tqdm(sft_data, desc=\"ç”ŸæˆDPOæ•°æ®\")):\n",
    "            prompt = ms.apply(prompt=case[\"instruction\"], input=case[\"input\"])\n",
    "            task = asyncio.create_task(generate_response(session, prompt, semaphore))\n",
    "            tasks.append(task)\n",
    "\n",
    "            # æ¯å¤„ç†100ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ç¼“å­˜\n",
    "            if (i + 1) % 100 == 0:\n",
    "                save_caches()\n",
    "\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "\n",
    "    dpo_data = []\n",
    "    for case, response in zip(sft_data, responses):\n",
    "        prompt = ms.apply(prompt=case[\"instruction\"], input=case[\"input\"])\n",
    "        dpo_sample = {\n",
    "            \"prompt\": prompt,\n",
    "            \"rejected\": response.get(\"output\") or response.get(\"rejected\"),\n",
    "            \"chosen\": case[\"output\"],\n",
    "        }\n",
    "        dpo_data.append(dpo_sample)\n",
    "\n",
    "    # ä¿å­˜é”™è¯¯ç¼“å­˜\n",
    "    save_caches()  # æœ€åå†ä¿å­˜ä¸€æ¬¡ç¼“å­˜\n",
    "\n",
    "    output_file = \"dpo_data_test.json\" if test_mode else \"dpo_data.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dpo_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    logger.info(f\"DPOæ•°æ®å·²ç”Ÿæˆå¹¶ä¿å­˜åˆ° {output_file}\")\n",
    "    logger.info(f\"ç¼“å­˜å·²æ›´æ–°å¹¶ä¿å­˜åˆ° dpo_cache.json\")\n",
    "    logger.info(f\"é”™è¯¯ç¼“å­˜å·²ä¿å­˜åˆ° error_cache.json\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    logger.info(f\"æ€»æ‰§è¡Œæ—¶é—´: {execution_time:.2f} ç§’\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"ç”ŸæˆDPOæ•°æ®\")\n",
    "    parser.add_argument(\"--concurrency\", type=int, default=10, help=\"å¹¶å‘æ•°é‡\")\n",
    "    parser.add_argument(\"--test\", action=\"store_true\", help=\"æµ‹è¯•æ¨¡å¼\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    logger.add(\"generate_dpo_data.log\", rotation=\"500 MB\")\n",
    "    start_time = time.time()\n",
    "    asyncio.run(main(args.concurrency, args.test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸Šé¢çš„è„šæœ¬åšäº†è¿™æ ·å‡ ä»¶äº‹ï¼š\n",
    "1. æ„å»ºäº†ä¸€ä¸ª`generate_response`çš„å‡½æ•°ï¼Œç”¨äºæ ¹æ®`prompt`ç”Ÿæˆ`response`\n",
    "2. å¯¹ä¸Šé¢çš„å‡½æ•°åšäº†å¼‚æ­¥è°ƒåº¦ï¼Œå¯ä»¥æ§åˆ¶å¹¶å‘æ•°é‡æ¥ç”Ÿæˆ`response`\n",
    "3. è®¾ç½®äº†å¯¹å·²ç»ç”Ÿæˆçš„æ ·æœ¬çš„ç»“æœçš„ç¼“å­˜ä»¥åŠå¼‚å¸¸æ ·æœ¬çš„ç¼“å­˜ï¼ˆæ¯100ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ï¼Œä¸”å¦‚æœè„šæœ¬å¼‚å¸¸é€€å‡º`atexit`ä¼šè‡ªåŠ¨ä¿å­˜ï¼‰\n",
    "4. æœ€åå°†`SFT`æ•°æ®å’Œç”Ÿæˆçš„`response`è¿›è¡Œæ‹¼æ¥ï¼Œå¾—åˆ°æœ€ç»ˆçš„`DPO`æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# ! python generate_dpo_data.py  --concurrency 25 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "èªæ˜çš„ä½ è‚¯å®šä¼šé—®äº†ï¼šå¼‚æ­¥è°ƒåº¦å•¥çš„å€’æ˜¯éƒ½æœ‰äº†ï¼Œ**å¯æˆ‘å“ªå„¿æ¥çš„æ¥å£å‘¢ï¼Ÿ**\n",
    "\n",
    "å…¶å®`litgpt`åº“åŒæ—¶ä¹Ÿæä¾›äº†æ¨¡å‹çš„`serving`åŠŸèƒ½ï¼Œåªè¦å®‰è£…äº†é¢å¤–çš„`litserve`ä¾èµ–ï¼Œå°±å¯ä»¥ä¸€é”®éƒ¨ç½²ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# ! litgpt serve out/custom-model/final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸è¿‡è¿™æ ·å¾—åˆ°çš„æœåŠ¡æ˜¯å•å®ä¾‹çš„ï¼Œæ— æ³•æ»¡è¶³æˆ‘ä»¬æ‰¹é‡åˆ·æ•°æ®çš„éœ€æ±‚ã€‚\n",
    "\n",
    "å¤§å®¶åˆ«å¿˜äº†ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å°ºå¯¸åªæœ‰`0.044B`ï¼Œæ˜¾å­˜å ç”¨æ‰`600M`ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬åœ¨ä¸€å¼ å¡ä¸Šå¯ä»¥è½»æ¾éƒ¨ç½²å¤šä¸ªå®ä¾‹ã€‚\n",
    "\n",
    "å…¶å®`litserve`æ˜¯æ”¯æŒå¤š`workers`çš„ï¼Œä¸è¿‡åœ¨`litgpt`åº“é›†æˆçš„æ—¶å€™æ²¡æœ‰æš´éœ²å‡ºå‚æ•°ï¼Œé—®é¢˜ä¸å¤§ï¼Œæˆ‘ä»¬è‡ªå·±åŸºäº[litgpté‡Œçš„serve.py](https://github.com/Lightning-AI/litgpt/blob/main/litgpt/deploy/serve.py)é­”æ”¹ä¸€ä¸‹å°±å¥½äº†ã€‚\n",
    "\n",
    "ä»£ç å¤ªé•¿è¿™é‡Œå°±ä¸å®Œæ•´åœ°è´´å‡ºäº†ï¼Œæ„Ÿå…´è¶£çš„å¯ä»¥çœ‹[è¿™é‡Œ](https://github.com/puppyapple/Chinese_LLM_From_Scratch/blob/main/Journey/Day11/service.py)ã€‚\n",
    "\n",
    "ä¿®æ”¹å…¶å®å¾ˆç®€å•ï¼Œå°±æ˜¯æŠŠ`workers_per_device`å‚æ•°æš´éœ²äº†å‡ºæ¥ï¼Œè¿™æ ·å°±å¯ä»¥åœ¨å¯åŠ¨æœåŠ¡çš„æ—¶å€™æŒ‡å®š`workers_per_device`çš„å€¼äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@click.command()\n",
    "@click.option(\"--checkpoint_dir\", type=str)\n",
    "@click.option(\"--quantize\", type=str, default=None)\n",
    "@click.option(\"--precision\", type=str, default=\"bf16-true\")\n",
    "@click.option(\"--temperature\", type=float, default=0.8)\n",
    "@click.option(\"--top_k\", type=int, default=50)\n",
    "@click.option(\"--top_p\", type=float, default=1.0)\n",
    "@click.option(\"--max_new_tokens\", type=int, default=50)\n",
    "@click.option(\"--devices\", type=int, default=1)\n",
    "@click.option(\"--workers_per_device\", type=int, default=20)\n",
    "@click.option(\"--port\", type=int, default=8000)\n",
    "@click.option(\"--stream\", type=bool, default=False)\n",
    "@click.option(\"--accelerator\", type=str, default=\"auto\")\n",
    "def run_server(\n",
    "    checkpoint_dir: Path,\n",
    "    quantize: Optional[\n",
    "        Literal[\"bnb.nf4\", \"bnb.nf4-dq\", \"bnb.fp4\", \"bnb.fp4-dq\", \"bnb.int8\"]\n",
    "    ] = None,\n",
    "    precision: Optional[str] = None,\n",
    "    temperature: float = 0.8,\n",
    "    top_k: int = 50,\n",
    "    top_p: float = 1.0,\n",
    "    max_new_tokens: int = 50,\n",
    "    devices: int = 1,\n",
    "    port: int = 8000,\n",
    "    accelerator: str = \"auto\",\n",
    "    workers_per_device: int = 20,\n",
    "    stream: bool = False,\n",
    "    access_token: Optional[str] = None,\n",
    ") -> None:\n",
    "    # ...\n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘è®¾ç½®äº†`25`ä¸ª`workers`ï¼Œç„¶åç”Ÿæˆçš„è„šæœ¬é…ç½®äº†`--concurrency 25`ã€‚\n",
    "\n",
    "è¿è¡Œæ—¶çš„æ•´ä½“`GPU`å ç”¨æ˜¯`20G`å·¦å³ã€‚\n",
    "\n",
    "![image](https://erxuanyi-1257355350.cos.ap-beijing.myqcloud.com/image.png)\n",
    "\n",
    "æˆªæ­¢å†™è¿™ç¯‡æ–‡ç« ï¼Œæ•°æ®è¿˜åœ¨è¿è¡Œä¸­ï¼Œå…·ä½“è€—æ—¶å¤šä¹…ç­‰æˆ‘è·‘å®Œäº†åœ¨åŒæ­¥ç»™å¤§å®¶ã€‚\n",
    "\n",
    "> å¦å¤–æˆ‘è°ƒç ”çš„æ—¶å€™å‘ç°`litserve`çš„`batch inference`å…¶å®å·²ç»æ”¯æŒäº†ï¼Œåé¢æœ‰æ—¶é—´å°è¯•ä¸€ä¸‹ï¼Œå¦‚æœæœ‰æ•ˆä¼šæ›´æ–°åˆ°é¡¹ç›®é‡Œã€‚\n",
    "> ä»“åº“é‡Œæˆ‘ä¹ŸåŒæ—¶æä¾›äº†å•æ¨¡å‹å’Œå¤šæ¨¡å‹å®ä¾‹æ¥è·‘æ•°æ®çš„è„šæœ¬ã€‚\n",
    ">\n",
    "## å°ç»“\n",
    "\n",
    "ä»Šå¤©çš„å†…å®¹å…¶å®å¾ˆç®€å•ï¼Œå°±æ˜¯æ„é€ äº†`DPO`æ•°æ®ï¼Œå¹¶ä¸”é€šè¿‡å¼‚æ­¥è¯·æ±‚çš„æ–¹å¼æé«˜äº†æ•°æ®æ„é€ çš„æ•ˆç‡ã€‚\n",
    "\n",
    "ç­‰æ•°æ®è·‘å®Œäº†æˆ‘ä¼šç€æ‰‹è¿›è¡Œ`DPO`çš„è®­ç»ƒã€‚\n",
    "\n",
    "ç”±äº`litgpt`åº“è‡ªèº«è¿˜ä¸æ”¯æŒ`DPO`ï¼Œæ‰€ä»¥è¿™éƒ¨åˆ†éœ€è¦å®Œå…¨è‡ªå·±`DIY`äº†ï¼Œå¯èƒ½ä¼šç¨å¾®è´¹ç‚¹åŠ²ï¼Œè¯·å¤§å®¶æ‹­ç›®ä»¥å¾…ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
