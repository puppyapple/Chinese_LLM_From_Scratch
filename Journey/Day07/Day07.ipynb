{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä»é›¶æ‰‹æ“ä¸­æ–‡å¤§æ¨¡å‹ï½œğŸš€ Day06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT æ•°æ®å‡†å¤‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TinyStories`æ•°æ®é›†å…¶å®ä¹Ÿæä¾›äº†[Instructæ•°æ®](https://huggingface.co/datasets/roneneldan/TinyStoriesInstruct)ï¼Œæˆ‘å¯ä»¥åŸºäºè¿™ä¸ªæ•°æ®é›†åœ¨ä¹‹å‰çš„é¢„è®­ç»ƒæ¨¡å‹ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒã€‚\n",
    "\n",
    "å…ˆçœ‹çœ‹æ•°æ®é›†çš„æ ¼å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -10 ../../Data/TinyStoriesInstruct/TinyStories-Instruct-valid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™äº›æŒ‡ä»¤æœ‰å››ç§ç±»å‹ï¼š\n",
    "1. ä¸€ä¸ªå•è¯åˆ—è¡¨ï¼ŒåŒ…å«åœ¨æ•…äº‹ä¸­ã€‚\n",
    "2. ä¸€ä¸ªå¥å­ï¼Œåº”è¯¥å‡ºç°åœ¨æ•…äº‹çš„æŸä¸ªåœ°æ–¹ã€‚\n",
    "3. ä¸€ä¸ªç‰¹å¾åˆ—è¡¨ï¼ˆå¯èƒ½çš„ç‰¹å¾ï¼šå¯¹è¯ã€åç»“å±€ã€é“å¾·ä»·å€¼ã€æƒ…èŠ‚è½¬æŠ˜ã€ä¼ç¬”ã€å†²çªï¼‰ã€‚\n",
    "4. ä¸€ä¸ªç®€çŸ­çš„æ€»ç»“ï¼ˆ1-2è¡Œï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨é¢ä¸´ä¸¤ä¸ªé—®é¢˜ï¼š\n",
    "- æ•°æ®é›†æ˜¯è‹±æ–‡çš„ï¼Œæˆ‘éœ€è¦æƒ³åŠæ³•ç»™æ•´æˆä¸­æ–‡çš„ã€‚\n",
    "- æ•°æ®é›†çš„å½¢å¼å’Œä¸»æµçš„SFTæ•°æ®é›†ä¸å¤ªä¸€æ ·ï¼Œéœ€è¦åšä¸€äº›é€‚é…ã€‚\n",
    "\n",
    "  > ä¸ªäººç†è§£è¿™é‡Œæ˜¯å› ä¸ºè¿™é‡Œçš„æŒ‡ä»¤ç›¸å¯¹å•ä¸€ï¼ˆç”Ÿæˆæ•…äº‹ï¼‰ï¼Œåªæ˜¯çº¦æŸæœ‰ä¸€äº›åŒºåˆ«ï¼Œæ‰€ä»¥ä½œè€…é‡‡å–äº†ç®€å•çš„æ‹¼æ¥æ–¹å¼ã€‚\n",
    "  >\n",
    "  > è¿™é‡Œå‡ºäºå­¦ä¹ çš„ç›®çš„è¿˜æ˜¯å¾€ä¸»æµçš„SFTæ•°æ®é›†ä¸Šé æ‹¢ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å´æ©è¾¾è€å¸ˆçš„ç¿»è¯‘Agentæµ‹è¯•\n",
    "\n",
    "è¿™é‡Œç›´æ¥è¯•äº†ä¸‹[å´æ©è¾¾è€å¸ˆçš„translation-agent](https://github.com/andrewyng/translation-agent)é¡¹ç›®ï¼ˆ`translation-agent.py`æ–‡ä»¶ï¼‰ï¼Œä½¿ç”¨çš„æ˜¯`gpt-4o-mini`çš„`api`ï¼ˆä¹Ÿå°è¯•è¿‡`Ollama`æœ¬åœ°éƒ¨ç½²çš„`qwen14b`ã€`qwen7b`ï¼Œç›¸å¯¹æ¥è¯´ä¸å¤ªç¨³å®šï¼‰ã€‚\n",
    "\n",
    "å¯ä»¥çœ‹åˆ°è¿™é‡Œå•æ¬¡ç¿»è¯‘çš„è€—æ—¶åœ¨10ç§’å·¦å³ï¼ˆå› ä¸ºå•è¯ç¿»è¯‘çš„æ—¶å€™`agent`é€»è¾‘é‡Œæœ‰å¤šæ¬¡`api`è°ƒç”¨ï¼‰ï¼Œå› æ­¤è¿™é‡Œä¸ºäº†åé¢èƒ½å¤Ÿå¹¶å‘è°ƒç”¨åˆ·æ•°æ®ï¼Œæˆ‘å°†ä»£ç å…¨éƒ¨æ”¹é€ æˆäº†`async`çš„å¼‚æ­¥è°ƒç”¨ã€‚\n",
    "\n",
    "å¤§å®¶å¦‚æœæœ‰å…¶ä»–çš„ç¿»è¯‘`api`æˆ–è€…æ¨¡å‹ä¹Ÿå¯ä»¥æ›¿æ¢ï¼Œè¿™é‡Œçº¯å±å¿ƒè¡€æ¥æ½®ç©ä¸€ç©å„¿ã€‚\n",
    "\n",
    "`translation-agent`é¡¹ç›®å…¶å®åªæœ‰ä¸€ä¸ª`utils.py`æ–‡ä»¶ï¼Œä½†å› ä¸ºå¤ªé•¿äº†ï¼Œè¿™é‡Œå°±ä¸æŠŠæ”¹é€ åçš„ä»£ç è´´å‡ºæ¥äº†ï¼Œæœ‰å…´è¶£çš„åŒå­¦å¯ä»¥å»ä»“åº“é‡ŒæŸ¥çœ‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| num_tokens_in_text: 416\n",
      "ic| 'Translating text as a single chunk'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "éšæœºå¥å­ï¼šä»–ä»¬éå¸¸å…´å¥‹ï¼Œä¹Ÿæƒ³é£èµ·æ¥ã€‚  \n",
      "ç‰¹ç‚¹ï¼šå¯¹è¯  \n",
      "æ‘˜è¦ï¼šæ±¤å§†å’Œå®‰å¨œå¾ˆå…´å¥‹è¦å’Œçˆ¶æ¯ä¸€èµ·åº¦å‡ï¼Œä»–ä»¬ä¹˜åä¸€æ¶å¤§é£æœºé£å¾€é˜³å…‰æ˜åªšã€æ²™æ»©ä¼—å¤šçš„åœ°æ–¹ã€‚  \n",
      "æ•…äº‹ï¼š  \n",
      "æ±¤å§†å’Œå®‰å¨œæ˜¯å…„å¦¹ã€‚ä»–ä»¬å–œæ¬¢ç©ç©å…·å’Œè¯»ä¹¦ã€‚ä»–ä»¬éå¸¸é«˜å…´ï¼Œå› ä¸ºä»–ä»¬è¦å’Œå¦ˆå¦ˆçˆ¸çˆ¸ä¸€èµ·å»åº¦å‡ã€‚ä»–ä»¬å°†ä¹˜åä¸€æ¶å¤§é£æœºå»ä¸€ä¸ªé˜³å…‰æ˜åªšã€æ²™æ»©ä¼—å¤šçš„åœ°æ–¹ã€‚  \n",
      "åº¦å‡æ—¥ç»ˆäºåˆ°äº†ï¼Œä»–ä»¬æ”¶æ‹¾å¥½è¡Œæã€‚ä»–ä»¬å»æœºåœºï¼Œç­‰å¾…ä»–ä»¬çš„é£æœºã€‚ä»–ä»¬çœ‹åˆ°è®¸å¤šé£æœºåœ¨å¤©ç©ºä¸­é£ã€‚ä»–ä»¬éå¸¸å…´å¥‹ï¼Œä¹Ÿæƒ³é£èµ·æ¥ã€‚  \n",
      "â€œçœ‹ï¼Œå®‰å¨œï¼Œé‚£æ¶é£æœºçœŸå¤§åˆå¿«ï¼â€æ±¤å§†è¯´ã€‚  \n",
      "â€œæ˜¯çš„ï¼Œæ±¤å§†ï¼Œå®ƒæœ‰ç¿…è†€å’Œå°¾å·´ã€‚æˆ‘æƒ³çŸ¥é“å®ƒè¦å»å“ªé‡Œï¼Œâ€å®‰å¨œè¯´ã€‚  \n",
      "ä»–ä»¬å¬åˆ°å¦ˆå¦ˆå«ä»–ä»¬ã€‚â€œå¿«æ¥ï¼Œå­©å­ä»¬ï¼Œæ˜¯æ—¶å€™ç™»æœºäº†ã€‚æˆ‘ä»¬å¾—å‡ºç¤ºæœºç¥¨ï¼Œé€šè¿‡ç™»æœºå£ã€‚â€  \n",
      "ä»–ä»¬è·Ÿç€å¦ˆå¦ˆå’Œçˆ¸çˆ¸ä¸Šäº†é£æœºã€‚ä»–ä»¬æ‰¾åˆ°åº§ä½ï¼Œç³»å¥½å®‰å…¨å¸¦ã€‚ä»–ä»¬æœ›å‘çª—å¤–ï¼Œçœ‹åˆ°åœ°é¢ã€æ±½è½¦å’Œè¡Œäººã€‚ä»–ä»¬å¬åˆ°é£è¡Œå‘˜åœ¨æ‰¬å£°å™¨ä¸Šè¯´è¯ã€‚  \n",
      "â€œå¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ä½ ä»¬çš„æœºé•¿ã€‚æ¬¢è¿ä¹˜å123èˆªç­å‰å¾€é˜³å…‰æµ·æ»©ã€‚æˆ‘ä»¬å‡†å¤‡èµ·é£ã€‚è¯·åå¥½ï¼Œç¥å¤§å®¶æ—…é€”æ„‰å¿«ã€‚â€  \n",
      "é£æœºå¼€å§‹ç§»åŠ¨ï¼Œå‘å‡ºè½°é¸£å£°ã€‚æ±¤å§†å’Œå®‰å¨œæ„Ÿåˆ°é£æœºè¶Šæ¥è¶Šå¿«ã€‚ä»–ä»¬çœ‹åˆ°åœ°é¢å˜å¾—è¶Šæ¥è¶Šå°ã€‚ä»–ä»¬çœ‹åˆ°äº‘æœµè¶Šæ¥è¶Šè¿‘ã€‚ä»–ä»¬é£èµ·æ¥äº†ï¼  \n",
      "â€œå“‡ï¼Œå®‰å¨œï¼Œæˆ‘ä»¬é£èµ·æ¥äº†ï¼æˆ‘ä»¬åœ¨å¤©ç©ºä¸­ï¼â€æ±¤å§†è¯´ã€‚  \n",
      "â€œæˆ‘çŸ¥é“ï¼Œæ±¤å§†ï¼ŒçœŸæ˜¯å¤ªç¥å¥‡äº†ï¼æˆ‘ä»¬è¿™ä¹ˆé«˜ï¼çœ‹ï¼Œé‚£æ˜¯å¤ªé˜³ï¼â€å®‰å¨œè¯´ã€‚  \n",
      "ä»–ä»¬å¾®ç¬‘ã€æ¬¢ç¬‘ï¼Œæ‹ç€æ‰‹ã€‚ä»–ä»¬ä¸€ç‚¹éƒ½ä¸éš¾è¿‡ã€‚ä»–ä»¬éå¸¸å¿«ä¹ã€‚ä»–ä»¬æ­£åœ¨é£å¾€åº¦å‡åœ°ã€‚\n"
     ]
    }
   ],
   "source": [
    "from translation_agent import translate\n",
    "\n",
    "text = \"\"\"\n",
    "Random sentence: They are very excited and want to fly too.\n",
    "Features: Dialogue\n",
    "Summary: Tom and Anna are excited to go on a holiday with their parents, and they fly on a big plane to a place with sun and sand.\n",
    "Story: \n",
    "Tom and Anna are brother and sister. They like to play with their toys and read books. They are very happy because they are going on a holiday with their mum and dad. They will fly on a big plane to a place with a lot of sun and sand.\n",
    "The day of the holiday comes and they pack their bags. They go to the airport and wait for their plane. They see many other planes flying in the sky. They are very excited and want to fly too.\n",
    "\"Look, Anna, that plane is so big and fast!\" Tom says.\n",
    "\"Yes, Tom, and it has wings and a tail. I wonder where it is going,\" Anna says.\n",
    "They hear their mum call them. \"Come on, kids, it's time to board our plane. We have to show our tickets and go through the gate.\"\n",
    "They follow their mum and dad and get on their plane. They find their seats and buckle their belts. They look out the window and see the ground and the cars and the people. They hear the pilot say something on the speaker.\n",
    "\"Hello, everyone, this is your pilot speaking. Welcome aboard flight 123 to Sunny Beach. We are ready to take off. Please sit back and enjoy the flight.\"\n",
    "The plane starts to move and makes a loud noise. Tom and Anna feel the plane go faster and faster. They see the ground get smaller and smaller. They see the clouds get closer and closer. They are flying!\n",
    "\"Wow, Anna, we are flying! We are in the sky!\" Tom says.\n",
    "\"I know, Tom, it's amazing! We are so high! Look, there is the sun!\" Anna says.\n",
    "They smile and laugh and clap their hands. They are not sad at all. They are very happy. They are flying to their holiday.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "result = await translate(\n",
    "    source_lang=\"English\",\n",
    "    target_lang=\"Chinese\",\n",
    "    source_text=text,\n",
    "    country=\"China\",\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®é‡‡æ ·\n",
    "\n",
    "æˆ‘å…ˆçœ‹çœ‹è®­ç»ƒé›†æœ‰å¤šå°‘æ¡æ•°æ®ï¼Œå¯ä»¥å‘ç°æ–‡æœ¬éƒ½æ˜¯ä»¥`<|endoftext|>`ç»“å°¾çš„ï¼Œæ‰€ä»¥é€šè¿‡ç»Ÿè®¡`endoftext`çš„ä¸ªæ•°å°±å¯ä»¥çŸ¥é“æ•°æ®é›†çš„æ¡æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2476532\n"
     ]
    }
   ],
   "source": [
    "! grep -o \"endoftext\" ../../Data/TinyStoriesInstruct/TinyStories-Instruct-train.txt  | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥è¿‘250wçš„é‡çº§æœ‰ç‚¹å¤§ï¼ˆå› ä¸ºå¾®è½¯çš„è®ºæ–‡é‡Œæ˜¯ç›´æ¥åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåšçš„`pretrain`çš„ï¼‰ã€‚\n",
    "\n",
    "å…¶å®å¾ˆå¤šç ”ç©¶è¡¨æ˜ï¼Œ`SFT`æ•°æ®çš„é‡çº§ä¸é‡è¦ï¼Œè´¨é‡å¤Ÿé«˜çš„æ—¶å€™å³ä½¿å¾ˆå°‘çš„æ•°æ®ä¹Ÿèƒ½è®­ç»ƒå‡ºå¾ˆå¥½çš„æ•ˆæœã€‚\n",
    "\n",
    "æ‰€ä»¥è¿™é‡Œæˆ‘æ‰“ç®—éšæœºæŠ½å–11000æ¡æ•°æ®æ¥è¯•è¯•ã€‚\n",
    "\n",
    "æˆ‘çš„ç­–ç•¥å¦‚ä¸‹ï¼š\n",
    "1. éå†`train`æ•°æ®é›†ï¼Œè®©å››ç±»æŒ‡ä»¤çš„ç»„åˆå°½é‡å‡è¡¡ï¼ˆéœ€è¦å…ˆç»Ÿè®¡æŒ‡ä»¤ç»„åˆçš„çš„åˆ†å¸ƒï¼‰\n",
    "2. ç”¨å¾—åˆ°çš„11000æ¡æ•°æ®è°ƒç”¨ä¸Šé¢çš„`translation-agent`è¿›è¡Œç¿»è¯‘\n",
    "3. å°†ç¿»è¯‘åçš„æ•°æ®æ•´ç†æˆ`SFT`æ•°æ®é›†çš„`json`æ ¼å¼\n",
    "\n",
    "å…ˆæ¥åšæ•°æ®çš„é‡‡æ ·ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "\n",
    "def count_field_combinations(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    blocks = content.split(\"<|endoftext|>\")\n",
    "    combinations = []\n",
    "\n",
    "    for block in blocks:\n",
    "        fields = set()\n",
    "        if \"Words:\" in block:\n",
    "            fields.add(\"Words\")\n",
    "        if \"Random sentence:\" in block:\n",
    "            fields.add(\"Random sentence\")\n",
    "        if \"Features:\" in block:\n",
    "            fields.add(\"Features\")\n",
    "        if \"Summary:\" in block:\n",
    "            fields.add(\"Summary\")\n",
    "\n",
    "        if fields:  # åªæœ‰å½“å­—æ®µä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ ç»„åˆ\n",
    "            combinations.append(frozenset(fields))\n",
    "\n",
    "    return Counter(combinations)\n",
    "\n",
    "\n",
    "def sample_data(file_path, total_samples=11000):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    blocks = content.split(\"<|endoftext|>\")\n",
    "    blocks = [block.strip() for block in blocks if block.strip()]  # ç§»é™¤ç©ºå—\n",
    "\n",
    "    combinations = count_field_combinations(file_path)\n",
    "    combination_more_than_1 = {k: v for k, v in combinations.items() if v > 1}\n",
    "    samples_per_combination = total_samples // len(combination_more_than_1)\n",
    "\n",
    "    sampled_data = []\n",
    "    for combination in combinations:\n",
    "        matching_blocks = [\n",
    "            block for block in blocks if set(get_fields(block)) == set(combination)\n",
    "        ]\n",
    "        sampled_data.extend(\n",
    "            random.sample(\n",
    "                matching_blocks, min(samples_per_combination, len(matching_blocks))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "\n",
    "def get_fields(block):\n",
    "    fields = set()\n",
    "    if \"Words:\" in block:\n",
    "        fields.add(\"Words\")\n",
    "    if \"Random sentence:\" in block:\n",
    "        fields.add(\"Random sentence\")\n",
    "    if \"Features:\" in block:\n",
    "        fields.add(\"Features\")\n",
    "    if \"Summary:\" in block:\n",
    "        fields.add(\"Summary\")\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰§è¡Œä¸€ä¸‹çœ‹çœ‹æ•ˆæœï¼ˆä¸ºäº†æœ‰å¤‡æ— æ‚£ï¼Œå¤šé‡‡æ ·äº†5000æ¡æ•°æ®ï¼‰ï¼Œè€—æ—¶1-2åˆ†é’Ÿï¼Œè‚¯å®šè¿˜æœ‰ä¼˜åŒ–ç©ºé—´ï¼Œä½†æ˜¯å¯ä»¥æ¥å—ã€‚\n",
    "\n",
    "åŒæ—¶å°†é‡‡æ ·åçš„æ•°æ®ä¿å­˜ä¸º`pkl`æ–‡ä»¶ï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é‡‡æ ·æ•°æ®æ€»æ•°: 15001\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "sft_raw = sample_data(\n",
    "    \"../../Data/TinyStoriesInstruct/TinyStories-Instruct-train.txt\", 15000\n",
    ")\n",
    "print(f\"é‡‡æ ·æ•°æ®æ€»æ•°: {len(sft_raw)}\")\n",
    "\n",
    "pickle.dump(sft_raw, open(\"sft_raw.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ‰¹é‡ç¿»è¯‘\n",
    "\n",
    "æ¥ä¸‹æ¥å°±å¯ä»¥è°ƒç”¨`translation-agent`è¿›è¡Œç¿»è¯‘äº†ã€‚\n",
    "\n",
    "è¿™é‡Œæˆ‘é™¤äº†ç”¨å¼‚æ­¥åŠ é€Ÿï¼Œè¿˜ä½¿ç”¨äº†`json`æ–‡ä»¶ç¼“å­˜æ¥é¿å…é‡å¤ç¿»è¯‘ï¼ˆ`gpt-4o-mini`çš„`api`ä¹Ÿä¸ç®—ä¾¿å®œï¼Œèƒ½çœåˆ™çœï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import aiofiles\n",
    "import asyncio\n",
    "\n",
    "cache_file = \"translation_cache.json\"\n",
    "\n",
    "\n",
    "async def translate_and_cache(block, cache, semaphore):\n",
    "    cache_key = hash(block)\n",
    "\n",
    "    if str(cache_key) in cache:\n",
    "        return cache[str(cache_key)]\n",
    "\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            result = await translate(\n",
    "                source_lang=\"English\",\n",
    "                target_lang=\"Chinese\",\n",
    "                source_text=block,\n",
    "                country=\"China\",\n",
    "            )\n",
    "            cache[str(cache_key)] = result\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"ç¿»è¯‘å¤±è´¥: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "async def batch_translate(sampled_data, cache_file, max_workers=10):\n",
    "    translated_data = []\n",
    "\n",
    "    try:\n",
    "        async with aiofiles.open(cache_file, \"r\") as f:\n",
    "            cache = json.loads(await f.read())\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        cache = {}\n",
    "\n",
    "    semaphore = asyncio.Semaphore(max_workers)\n",
    "    tasks = [translate_and_cache(block, cache, semaphore) for block in sampled_data]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    translated_data = [result for result in results if result]\n",
    "\n",
    "    async with aiofiles.open(cache_file, \"w\") as f:\n",
    "        await f.write(json.dumps(cache, ensure_ascii=False, indent=2))\n",
    "\n",
    "    return translated_data\n",
    "\n",
    "\n",
    "translated_data = await batch_translate(sft_raw, cache_file, max_workers=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨äº†100è·¯çš„å¹¶å‘ï¼Œç¿»è¯‘äº†15000æ¡æ•°æ®ï¼Œè€—æ—¶48åˆ†é’Ÿï¼Œä¹Ÿå°±æ˜¯å¤§æ¦‚æ¯åˆ†é’Ÿç¿»è¯‘300æ¡æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åç»­å¤„ç†\n",
    "\n",
    "ç¿»è¯‘å®Œæˆäº†ï¼Œæœ€åä¸€æ­¥å°±æ˜¯å°†æ•°æ®æ•´ç†æˆ`SFT`æ•°æ®é›†çš„æ ¼å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('æ‘˜è¦', 8433),\n",
      " ('éšæœºå¥å­', 5372),\n",
      " ('è¯æ±‡', 4824),\n",
      " ('ç‰¹ç‚¹', 4603),\n",
      " ('ç‰¹å¾', 1269),\n",
      " ('å•è¯', 1073),\n",
      " ('æ€»ç»“', 1001),\n",
      " ('å…³é”®è¯', 349),\n",
      " ('éšæœºçš„å¥å­', 246),\n",
      " ('éšæœºå¥', 196),\n",
      " ('æ•…äº‹ç‰¹ç‚¹', 146),\n",
      " ('ä¸»é¢˜', 122),\n",
      " ('è¯è¯­', 108),\n",
      " ('éšä¾¿ä¸€å¥è¯', 95),\n",
      " ('éšæœºä¸€å¥è¯', 87),\n",
      " ('éšæœºçš„ä¸€å¥è¯', 24),\n",
      " ('è¯', 21),\n",
      " ('æ•…äº‹ç‰¹å¾', 20),\n",
      " ('éšæœºå¥å­æ˜¯', 19),\n",
      " ('éšä¾¿è¯´ä¸€å¥', 17)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "instruction_template = \"\"\"æŒ‰ç…§ç»™å®šçš„è¦æ±‚è®²æ•…äº‹ï¼Œ\n",
    "å…¶ä¸­â€˜æ‘˜è¦â€™è¡¨ç¤ºæ•…äº‹çš„æ€»ç»“ï¼Œ\n",
    "â€˜å•è¯/è¯æ±‡/å…³é”®è¯â€™è¡¨ç¤ºæ•…äº‹ä¸­å¿…é¡»åŒ…å«çš„å•è¯ï¼Œ\n",
    "â€˜éšæœºå¥å­â€™è¡¨ç¤ºæ•…äº‹ä¸­å¿…é¡»åŒ…å«çš„å¥å­ï¼Œ\n",
    "â€˜ç‰¹å¾/ç‰¹ç‚¹â€™è¡¨ç¤ºæ•…äº‹çš„ç‰¹å¾ï¼Œå¦‚å¯¹è¯ã€åç»“å±€ã€é“å¾·ä»·å€¼ã€æƒ…èŠ‚è½¬æŠ˜ã€ä¼ç¬”ã€å†²çªç­‰ã€‚\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def process_translated_data(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_data = []\n",
    "    constraint_keys = Counter()\n",
    "\n",
    "    for key, value in data.items():\n",
    "        if \"æ•…äº‹ï¼š\" not in value:\n",
    "            continue\n",
    "        parts = value.split(\"æ•…äº‹ï¼š\")\n",
    "\n",
    "        if len(parts) == 2:\n",
    "            input_text = parts[0].strip()\n",
    "            output_text = parts[1].strip()\n",
    "\n",
    "            # æå–çº¦æŸæè¿°æ–‡æœ¬çš„å…³é”®å­—æ®µ\n",
    "            lines = input_text.split(\"\\n\")\n",
    "            for line in lines:\n",
    "                if \"ï¼š\" in line:\n",
    "                    key, _ = line.split(\"ï¼š\", 1)\n",
    "                    constraint_keys[key.strip()] += 1\n",
    "\n",
    "            processed_item = {\n",
    "                \"instruction\": instruction_template,\n",
    "                \"input\": input_text,\n",
    "                \"output\": output_text,\n",
    "            }\n",
    "\n",
    "            processed_data.append(processed_item)\n",
    "\n",
    "    # å°†å¤„ç†åçš„æ•°æ®å†™å…¥è¾“å‡ºæ–‡ä»¶\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(processed_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return processed_data, constraint_keys\n",
    "\n",
    "\n",
    "processed_data, constraint_keys = process_translated_data(\n",
    "    \"translation_cache.json\", \"../../Data/TinyStoriesInstruct/sft_data.json\"\n",
    ")\n",
    "\n",
    "pprint(constraint_keys.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "çœ‹ä¸€çœ‹å¤„ç†çš„ç»“æœï¼Œè¿™æ ·å°±å’Œç»å…¸çš„`SFT`æ•°æ®æ ¼å¼ä¸€è‡´äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'ç‰¹ç‚¹ï¼šå¯¹è¯  \\n'\n",
      "          'æ‘˜è¦ï¼šè’‚ç±³å’Œå¦ˆå¦ˆä¸€èµ·å»å•†åº—ï¼Œå¯¹å•†åº—é‡Œæ‰€æœ‰çš„ç©å…·å’Œç³–æœæ„Ÿåˆ°æƒŠè®¶ã€‚ä»–è¯·æ±‚è§¦æ‘¸ä¸€ä¸ªç©å…·ï¼Œå¦ˆå¦ˆå…è®¸ä»–ï¼Œè¿™è®©ä»–éå¸¸å¼€å¿ƒã€‚  \\n'\n",
      "          'è¯æ±‡ï¼šè§¦æ‘¸ã€å•†åº—ã€å®½æ•',\n",
      " 'instruction': 'æŒ‰ç…§ç»™å®šçš„è¦æ±‚è®²æ•…äº‹ï¼Œ\\n'\n",
      "                'å…¶ä¸­â€˜æ‘˜è¦â€™è¡¨ç¤ºæ•…äº‹çš„æ€»ç»“ï¼Œ\\n'\n",
      "                'â€˜å•è¯/è¯æ±‡/å…³é”®è¯â€™è¡¨ç¤ºæ•…äº‹ä¸­å¿…é¡»åŒ…å«çš„å•è¯ï¼Œ\\n'\n",
      "                'â€˜éšæœºå¥å­â€™è¡¨ç¤ºæ•…äº‹ä¸­å¿…é¡»åŒ…å«çš„å¥å­ï¼Œ\\n'\n",
      "                'â€˜ç‰¹å¾/ç‰¹ç‚¹â€™è¡¨ç¤ºæ•…äº‹çš„ç‰¹å¾ï¼Œå¦‚å¯¹è¯ã€åç»“å±€ã€é“å¾·ä»·å€¼ã€æƒ…èŠ‚è½¬æŠ˜ã€ä¼ç¬”ã€å†²çªç­‰ã€‚\\n',\n",
      " 'output': 'å¾ˆä¹…ä»¥å‰ï¼Œæœ‰ä¸€ä¸ªå°ç”·å­©åå«è’‚ç±³ã€‚è’‚ç±³å–œæ¬¢åœ¨å¤–é¢ç©è€å’Œæ¢ç´¢ã€‚  \\n'\n",
      "           'ä¸€å¤©ï¼Œè’‚ç±³å’Œå¦ˆå¦ˆä¸€èµ·å»å•†åº—ã€‚å•†åº—éå¸¸å¤§ï¼Œå®½æ•ã€‚è’‚ç±³å¯¹ä»–çœ‹åˆ°çš„ç©å…·å’Œç³–æœçœŸæ˜¯å¤ªå¤šäº†ï¼Œæ„Ÿåˆ°æƒŠè®¶ã€‚  \\n'\n",
      "           'çªç„¶ï¼Œè’‚ç±³çœ‹åˆ°ä¸€ä¸ªä»–éå¸¸æƒ³è§¦æ‘¸çš„ç©å…·ã€‚â€œå¦ˆå¦ˆï¼Œæˆ‘å¯ä»¥è§¦æ‘¸é‚£ä¸ªç©å…·å—ï¼Ÿâ€ä»–é—®ã€‚  \\n'\n",
      "           'â€œå½“ç„¶å¯ä»¥å•Šï¼Œè’‚ç±³ï¼Œâ€å¦ˆå¦ˆè¯´ã€‚è’‚ç±³éå¸¸å¼€å¿ƒï¼Œä»–è½»è½»æ‘¸äº†æ‘¸ç©å…·ã€‚æ‘¸èµ·æ¥è½¯è½¯çš„ï¼Œç‰¹åˆ«æœ‰å¼¹æ€§ã€‚  \\n'\n",
      "           'ç¦»å¼€å•†åº—åï¼Œè’‚ç±³å¯¹å¦ˆå¦ˆè¯´ä»–æœ‰å¤šå–œæ¬¢å’Œå¥¹ä¸€èµ·é€›å•†åº—ã€‚â€œæˆ‘ç©å¾—å¯å¼€å¿ƒäº†ï¼Œæ‘¸ç©å…·çœŸæœ‰æ„æ€ï¼Œâ€ä»–è¯´ã€‚å¦ˆå¦ˆå¾®ç¬‘ç€æŠŠä»–æŠ±ä½äº†ã€‚'}\n"
     ]
    }
   ],
   "source": [
    "pprint(processed_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å°ç»“\n",
    "1. åŸºäº`TinyStories`çš„`Instruct`æ•°æ®è¿›è¡ŒæŒ‡ä»¤ç»„åˆå±‚é¢å‡è¡¡çš„é‡‡æ ·ï¼Œè·å¾—äº†15000æ¡åŸå§‹æ•°æ®\n",
    "2. æ„é€ äº†ç¿»è¯‘å‡½æ•°ï¼Œå¼‚æ­¥ä½¿ç”¨å´æ©è¾¾è€å¸ˆçš„`translation-agent`å¯¹æ•°æ®è¿›è¡Œç¿»è¯‘\n",
    "3. åŸºäºç¿»è¯‘åçš„æ•°æ®ï¼Œæ„é€ äº†ç»å…¸æ ¼å¼çš„`SFT`æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
