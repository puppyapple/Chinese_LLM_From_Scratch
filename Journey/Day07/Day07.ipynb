{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零手搓中文大模型｜🚀 Day07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TinyStories`数据集其实也提供了[Instruct数据](https://huggingface.co/datasets/roneneldan/TinyStoriesInstruct)，我可以基于这个数据集在之前的预训练模型上进行指令微调。\n",
    "\n",
    "先看看数据集的格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -10 ../../Data/TinyStoriesInstruct/TinyStories-Instruct-valid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些指令有四种类型：\n",
    "1. 一个单词列表，包含在故事中。\n",
    "2. 一个句子，应该出现在故事的某个地方。\n",
    "3. 一个特征列表（可能的特征：对话、坏结局、道德价值、情节转折、伏笔、冲突）。\n",
    "4. 一个简短的总结（1-2行）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在面临两个问题：\n",
    "- 数据集是英文的，我需要想办法给整成中文的。\n",
    "- 数据集的形式和主流的SFT数据集不太一样，需要做一些适配。\n",
    "\n",
    "  > 个人理解这里是因为这里的指令相对单一（生成故事），只是约束有一些区别，所以作者采取了简单的拼接方式。\n",
    "  >\n",
    "  > 这里出于学习的目的还是往主流的SFT数据集上靠拢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 吴恩达老师的翻译Agent测试\n",
    "\n",
    "这里直接试了下[吴恩达老师的translation-agent](https://github.com/andrewyng/translation-agent)项目（`translation-agent.py`文件），使用的是`gpt-4o-mini`的`api`（也尝试过`Ollama`本地部署的`qwen14b`、`qwen7b`，相对来说不太稳定）。\n",
    "\n",
    "可以看到这里单次翻译的耗时在10秒左右（因为单词翻译的时候`agent`逻辑里有多次`api`调用），因此这里为了后面能够并发调用刷数据，我将代码全部改造成了`async`的异步调用。\n",
    "\n",
    "大家如果有其他的翻译`api`或者模型也可以替换，这里纯属心血来潮玩一玩儿。\n",
    "\n",
    "`translation-agent`项目其实只有一个`utils.py`文件，但因为太长了，这里就不把改造后的代码贴出来了，有兴趣的同学可以去仓库里查看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translation_agent import translate\n",
    "\n",
    "text = \"\"\"\n",
    "Random sentence: They are very excited and want to fly too.\n",
    "Features: Dialogue\n",
    "Summary: Tom and Anna are excited to go on a holiday with their parents, and they fly on a big plane to a place with sun and sand.\n",
    "Story: \n",
    "Tom and Anna are brother and sister. They like to play with their toys and read books. They are very happy because they are going on a holiday with their mum and dad. They will fly on a big plane to a place with a lot of sun and sand.\n",
    "The day of the holiday comes and they pack their bags. They go to the airport and wait for their plane. They see many other planes flying in the sky. They are very excited and want to fly too.\n",
    "\"Look, Anna, that plane is so big and fast!\" Tom says.\n",
    "\"Yes, Tom, and it has wings and a tail. I wonder where it is going,\" Anna says.\n",
    "They hear their mum call them. \"Come on, kids, it's time to board our plane. We have to show our tickets and go through the gate.\"\n",
    "They follow their mum and dad and get on their plane. They find their seats and buckle their belts. They look out the window and see the ground and the cars and the people. They hear the pilot say something on the speaker.\n",
    "\"Hello, everyone, this is your pilot speaking. Welcome aboard flight 123 to Sunny Beach. We are ready to take off. Please sit back and enjoy the flight.\"\n",
    "The plane starts to move and makes a loud noise. Tom and Anna feel the plane go faster and faster. They see the ground get smaller and smaller. They see the clouds get closer and closer. They are flying!\n",
    "\"Wow, Anna, we are flying! We are in the sky!\" Tom says.\n",
    "\"I know, Tom, it's amazing! We are so high! Look, there is the sun!\" Anna says.\n",
    "They smile and laugh and clap their hands. They are not sad at all. They are very happy. They are flying to their holiday.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "result = await translate(\n",
    "    source_lang=\"English\",\n",
    "    target_lang=\"Chinese\",\n",
    "    source_text=text,\n",
    "    country=\"China\",\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据采样\n",
    "\n",
    "我先看看训练集有多少条数据，可以发现文本都是以`<|endoftext|>`结尾的，所以通过统计`endoftext`的个数就可以知道数据集的条数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! grep -o \"endoftext\" ../../Data/TinyStoriesInstruct/TinyStories-Instruct-train.txt  | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接近250w的量级有点大（因为微软的论文里是直接在整个数据集上做的`pretrain`的）。\n",
    "\n",
    "其实很多研究表明，`SFT`数据的量级不重要，质量够高的时候即使很少的数据也能训练出很好的效果。\n",
    "\n",
    "所以这里我打算随机抽取11000条数据来试试。\n",
    "\n",
    "我的策略如下：\n",
    "1. 遍历`train`数据集，让四类指令的组合尽量均衡（需要先统计指令组合的的分布）\n",
    "2. 用得到的11000条数据调用上面的`translation-agent`进行翻译\n",
    "3. 将翻译后的数据整理成`SFT`数据集的`json`格式\n",
    "\n",
    "先来做数据的采样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "\n",
    "def count_field_combinations(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    blocks = content.split(\"<|endoftext|>\")\n",
    "    combinations = []\n",
    "\n",
    "    for block in blocks:\n",
    "        fields = set()\n",
    "        if \"Words:\" in block:\n",
    "            fields.add(\"Words\")\n",
    "        if \"Random sentence:\" in block:\n",
    "            fields.add(\"Random sentence\")\n",
    "        if \"Features:\" in block:\n",
    "            fields.add(\"Features\")\n",
    "        if \"Summary:\" in block:\n",
    "            fields.add(\"Summary\")\n",
    "\n",
    "        if fields:  # 只有当字段不为空时才添加组合\n",
    "            combinations.append(frozenset(fields))\n",
    "\n",
    "    return Counter(combinations)\n",
    "\n",
    "\n",
    "def sample_data(file_path, total_samples=11000):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    blocks = content.split(\"<|endoftext|>\")\n",
    "    blocks = [block.strip() for block in blocks if block.strip()]  # 移除空块\n",
    "\n",
    "    combinations = count_field_combinations(file_path)\n",
    "    combination_more_than_1 = {k: v for k, v in combinations.items() if v > 1}\n",
    "    samples_per_combination = total_samples // len(combination_more_than_1)\n",
    "\n",
    "    sampled_data = []\n",
    "    for combination in combinations:\n",
    "        matching_blocks = [\n",
    "            block for block in blocks if set(get_fields(block)) == set(combination)\n",
    "        ]\n",
    "        sampled_data.extend(\n",
    "            random.sample(\n",
    "                matching_blocks, min(samples_per_combination, len(matching_blocks))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "\n",
    "def get_fields(block):\n",
    "    fields = set()\n",
    "    if \"Words:\" in block:\n",
    "        fields.add(\"Words\")\n",
    "    if \"Random sentence:\" in block:\n",
    "        fields.add(\"Random sentence\")\n",
    "    if \"Features:\" in block:\n",
    "        fields.add(\"Features\")\n",
    "    if \"Summary:\" in block:\n",
    "        fields.add(\"Summary\")\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行一下看看效果（为了有备无患，多采样了5000条数据），耗时1-2分钟，肯定还有优化空间，但是可以接受。\n",
    "\n",
    "同时将采样后的数据保存为`pkl`文件，方便后续使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "采样数据总数: 15001\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# sft_raw = sample_data(\n",
    "#     \"../../Data/TinyStoriesInstruct/TinyStories-Instruct-train.txt\", 15000\n",
    "# )\n",
    "sft_raw = pickle.load(open(\"sft_raw.pkl\", \"rb\"))\n",
    "print(f\"采样数据总数: {len(sft_raw)}\")\n",
    "\n",
    "# pickle.dump(sft_raw, open(\"sft_raw.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量翻译\n",
    "\n",
    "接下来就可以调用`translation-agent`进行翻译了。\n",
    "\n",
    "这里我除了用异步加速，还使用了`json`文件缓存来避免重复翻译（`gpt-4o-mini`的`api`也不算便宜，能省则省）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import aiofiles\n",
    "import asyncio\n",
    "\n",
    "cache_file = \"translation_cache.json\"\n",
    "\n",
    "\n",
    "async def translate_and_cache(block, cache, semaphore):\n",
    "    cache_key = hash(block)\n",
    "\n",
    "    if str(cache_key) in cache:\n",
    "        return cache[str(cache_key)]\n",
    "\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            result = await translate(\n",
    "                source_lang=\"English\",\n",
    "                target_lang=\"Chinese\",\n",
    "                source_text=block,\n",
    "                country=\"China\",\n",
    "            )\n",
    "            cache[str(cache_key)] = result\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"翻译失败: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "async def batch_translate(sampled_data, cache_file, max_workers=10):\n",
    "    translated_data = []\n",
    "\n",
    "    try:\n",
    "        async with aiofiles.open(cache_file, \"r\") as f:\n",
    "            cache = json.loads(await f.read())\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        cache = {}\n",
    "\n",
    "    semaphore = asyncio.Semaphore(max_workers)\n",
    "    tasks = [translate_and_cache(block, cache, semaphore) for block in sampled_data]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    translated_data = [result for result in results if result]\n",
    "\n",
    "    async with aiofiles.open(cache_file, \"w\") as f:\n",
    "        await f.write(json.dumps(cache, ensure_ascii=False, indent=2))\n",
    "\n",
    "    return translated_data\n",
    "\n",
    "\n",
    "translated_data = await batch_translate(sft_raw, cache_file, max_workers=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用了100路的并发，翻译了15000条数据，耗时48分钟，也就是大概每分钟翻译300条数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 后续处理\n",
    "\n",
    "翻译完成了，最后一步就是将数据整理成`SFT`数据集的格式。\n",
    "\n",
    "（这里还发现了个小问题，翻译统一将**总结**字段放到了最后，导致顺序出现了问题，所以这里需要先处理一下。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "instruction_template = \"按照下面输入的约束生成故事\"\n",
    "\n",
    "\n",
    "def split_data(data, keys):\n",
    "    result = []\n",
    "    current_key = None\n",
    "    current_content = \"\"\n",
    "\n",
    "    for line in data.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if any(key in line for key in keys):\n",
    "            if current_key:\n",
    "                result.append((current_key, current_content.strip()))\n",
    "            for key in keys:\n",
    "                if key in line:\n",
    "                    current_key, current_content = line.split(key, 1)\n",
    "                    current_key = key.strip()\n",
    "                    current_content = current_content.strip().lstrip(\"：\").strip()\n",
    "                    break\n",
    "        else:\n",
    "            current_content += \" \" + line\n",
    "\n",
    "    if current_key:\n",
    "        result.append((current_key, current_content.strip()))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_translated_data(input_file, output_file, expand_data=True):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_data = []\n",
    "    constraint_keys = Counter()\n",
    "\n",
    "    for key, value in data.items():\n",
    "        if \"故事：\" not in value:\n",
    "            continue\n",
    "        parts = value.split(\"故事：\")\n",
    "\n",
    "        if len(parts) == 2:\n",
    "            input_text = parts[0].strip()\n",
    "            output_text = parts[1].strip()\n",
    "            if \"总结：\" in output_text or \"摘要：\" in output_text:\n",
    "                # 将总结或摘要提取出来放到input_text中\n",
    "                for keyword in [\"总结\", \"摘要\"]:\n",
    "                    if keyword in output_text:\n",
    "                        summary = output_text.split(f\"{keyword}：\")[1].strip()\n",
    "                        input_text += f\"\\n{keyword}：{summary}\"\n",
    "                        output_text = output_text.split(f\"{keyword}：\")[0].strip()\n",
    "                        break\n",
    "\n",
    "            # 提取约束描述文本的关键字段\n",
    "            lines = input_text.split(\"\\n\")\n",
    "            for line in lines:\n",
    "                if \"：\" in line:\n",
    "                    key, _ = line.split(\"：\", 1)\n",
    "                    constraint_keys[key.strip()] += 1\n",
    "\n",
    "            processed_item = {\n",
    "                \"instruction\": instruction_template,\n",
    "                \"input\": f\"{input_text}\",\n",
    "                \"output\": output_text,\n",
    "            }\n",
    "\n",
    "            processed_data.append(processed_item)\n",
    "    # 根据constraint_keys的频率排序，选取出现频率大于10的关键字\n",
    "    constraint_keys = {k: v for k, v in constraint_keys.items() if v > 10}\n",
    "\n",
    "    # 数据增强\n",
    "    if expand_data:\n",
    "        expanded_data = []\n",
    "        for item in processed_data:\n",
    "            input_tuple_list = split_data(item[\"input\"], constraint_keys)\n",
    "            if not input_tuple_list:\n",
    "                continue\n",
    "\n",
    "            for permutation in itertools.permutations(input_tuple_list):\n",
    "                new_item = item.copy()\n",
    "                new_item[\"input\"] = \"\\n\".join(\n",
    "                    [f\"{key}：{value}\" for key, value in permutation]\n",
    "                )\n",
    "                expanded_data.append(new_item)\n",
    "    else:\n",
    "        expanded_data = processed_data\n",
    "\n",
    "    # 对结果做一个打乱\n",
    "    random.shuffle(expanded_data)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(expanded_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return expanded_data, constraint_keys\n",
    "\n",
    "\n",
    "expanded_data, constraint_keys = process_translated_data(\n",
    "    \"translation_cache.json\",\n",
    "    \"../../Data/TinyStoriesInstruct/sft_data_no_expansion.json\",\n",
    "    expand_data=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一看处理的结果，这样就和经典的`SFT`数据格式一致了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '特点：转折\\n'\n",
      "          '摘要：莉莉在她的大红盒子里发现了一只会魔法的青蛙，并希望有一个朋友可以一起玩，青蛙实现了她的愿望，变成了一个小女孩。\\n'\n",
      "          '随机句子：盒子没有弯曲，但感觉不同。\\n'\n",
      "          '词汇：敲打，盒子，灵活',\n",
      " 'instruction': '按照下面输入的约束生成故事：',\n",
      " 'output': '从前，有一个小女孩叫莉莉。她房间里有个大红盒子。莉莉喜欢玩这个盒子。一天，她想看看盒子是否灵活，于是她试着去弯它。盒子没有弯曲，但感觉不同。  \\n'\n",
      "           '莉莉决定用手敲打盒子，看看会发生什么。当她这样做时，盒子打开了！盒子里有一只小小的绿色青蛙。青蛙看着莉莉说：“你好！我是一只会魔法的青蛙。”  \\n'\n",
      "           '莉莉非常惊讶，她简直不敢相信自己的眼睛。青蛙告诉莉莉，他可以实现她一个愿望。莉莉想了一会儿，许下了一个希望有朋友一起玩的愿望。魔法青蛙微笑着变成了一个小女孩，就像莉莉一样。她们整天玩得不亦乐乎。'}\n"
     ]
    }
   ],
   "source": [
    "pprint(expanded_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71322"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(expanded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "1. 基于`TinyStories`的`Instruct`数据进行指令组合层面均衡的采样，获得了15000条原始数据\n",
    "2. 构造了翻译函数，异步使用吴恩达老师的`translation-agent`对数据进行翻译\n",
    "3. 基于翻译后的数据，构造了经典格式的`SFT`数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
