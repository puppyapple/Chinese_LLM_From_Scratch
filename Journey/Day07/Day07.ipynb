{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä»é›¶æ‰‹æ“ä¸­æ–‡å¤§æ¨¡å‹ï½œğŸš€ Day07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT æ•°æ®å‡†å¤‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TinyStories`æ•°æ®é›†å…¶å®ä¹Ÿæä¾›äº†[Instructæ•°æ®](https://huggingface.co/datasets/roneneldan/TinyStoriesInstruct)ï¼Œæˆ‘å¯ä»¥åŸºäºè¿™ä¸ªæ•°æ®é›†åœ¨ä¹‹å‰çš„é¢„è®­ç»ƒæ¨¡å‹ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒã€‚\n",
    "\n",
    "å…ˆçœ‹çœ‹æ•°æ®é›†çš„æ ¼å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -10 ../../Data/TinyStoriesInstruct/TinyStories-Instruct-valid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™äº›æŒ‡ä»¤æœ‰å››ç§ç±»å‹ï¼š\n",
    "1. ä¸€ä¸ªå•è¯åˆ—è¡¨ï¼ŒåŒ…å«åœ¨æ•…äº‹ä¸­ã€‚\n",
    "2. ä¸€ä¸ªå¥å­ï¼Œåº”è¯¥å‡ºç°åœ¨æ•…äº‹çš„æŸä¸ªåœ°æ–¹ã€‚\n",
    "3. ä¸€ä¸ªç‰¹å¾åˆ—è¡¨ï¼ˆå¯èƒ½çš„ç‰¹å¾ï¼šå¯¹è¯ã€åç»“å±€ã€é“å¾·ä»·å€¼ã€æƒ…èŠ‚è½¬æŠ˜ã€ä¼ç¬”ã€å†²çªï¼‰ã€‚\n",
    "4. ä¸€ä¸ªç®€çŸ­çš„æ€»ç»“ï¼ˆ1-2è¡Œï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨é¢ä¸´ä¸¤ä¸ªé—®é¢˜ï¼š\n",
    "- æ•°æ®é›†æ˜¯è‹±æ–‡çš„ï¼Œæˆ‘éœ€è¦æƒ³åŠæ³•ç»™æ•´æˆä¸­æ–‡çš„ã€‚\n",
    "- æ•°æ®é›†çš„å½¢å¼å’Œä¸»æµçš„SFTæ•°æ®é›†ä¸å¤ªä¸€æ ·ï¼Œéœ€è¦åšä¸€äº›é€‚é…ã€‚\n",
    "\n",
    "  > ä¸ªäººç†è§£è¿™é‡Œæ˜¯å› ä¸ºè¿™é‡Œçš„æŒ‡ä»¤ç›¸å¯¹å•ä¸€ï¼ˆç”Ÿæˆæ•…äº‹ï¼‰ï¼Œåªæ˜¯çº¦æŸæœ‰ä¸€äº›åŒºåˆ«ï¼Œæ‰€ä»¥ä½œè€…é‡‡å–äº†ç®€å•çš„æ‹¼æ¥æ–¹å¼ã€‚\n",
    "  >\n",
    "  > è¿™é‡Œå‡ºäºå­¦ä¹ çš„ç›®çš„è¿˜æ˜¯å¾€ä¸»æµçš„SFTæ•°æ®é›†ä¸Šé æ‹¢ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å´æ©è¾¾è€å¸ˆçš„ç¿»è¯‘Agentæµ‹è¯•\n",
    "\n",
    "è¿™é‡Œç›´æ¥è¯•äº†ä¸‹[å´æ©è¾¾è€å¸ˆçš„translation-agent](https://github.com/andrewyng/translation-agent)é¡¹ç›®ï¼ˆ`translation-agent.py`æ–‡ä»¶ï¼‰ï¼Œä½¿ç”¨çš„æ˜¯`gpt-4o-mini`çš„`api`ï¼ˆä¹Ÿå°è¯•è¿‡`Ollama`æœ¬åœ°éƒ¨ç½²çš„`qwen14b`ã€`qwen7b`ï¼Œç›¸å¯¹æ¥è¯´ä¸å¤ªç¨³å®šï¼‰ã€‚\n",
    "\n",
    "å¯ä»¥çœ‹åˆ°è¿™é‡Œå•æ¬¡ç¿»è¯‘çš„è€—æ—¶åœ¨10ç§’å·¦å³ï¼ˆå› ä¸ºå•è¯ç¿»è¯‘çš„æ—¶å€™`agent`é€»è¾‘é‡Œæœ‰å¤šæ¬¡`api`è°ƒç”¨ï¼‰ï¼Œå› æ­¤è¿™é‡Œä¸ºäº†åé¢èƒ½å¤Ÿå¹¶å‘è°ƒç”¨åˆ·æ•°æ®ï¼Œæˆ‘å°†ä»£ç å…¨éƒ¨æ”¹é€ æˆäº†`async`çš„å¼‚æ­¥è°ƒç”¨ã€‚\n",
    "\n",
    "å¤§å®¶å¦‚æœæœ‰å…¶ä»–çš„ç¿»è¯‘`api`æˆ–è€…æ¨¡å‹ä¹Ÿå¯ä»¥æ›¿æ¢ï¼Œè¿™é‡Œçº¯å±å¿ƒè¡€æ¥æ½®ç©ä¸€ç©å„¿ã€‚\n",
    "\n",
    "`translation-agent`é¡¹ç›®å…¶å®åªæœ‰ä¸€ä¸ª`utils.py`æ–‡ä»¶ï¼Œä½†å› ä¸ºå¤ªé•¿äº†ï¼Œè¿™é‡Œå°±ä¸æŠŠæ”¹é€ åçš„ä»£ç è´´å‡ºæ¥äº†ï¼Œæœ‰å…´è¶£çš„åŒå­¦å¯ä»¥å»ä»“åº“é‡ŒæŸ¥çœ‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translation_agent import translate\n",
    "\n",
    "text = \"\"\"\n",
    "Random sentence: They are very excited and want to fly too.\n",
    "Features: Dialogue\n",
    "Summary: Tom and Anna are excited to go on a holiday with their parents, and they fly on a big plane to a place with sun and sand.\n",
    "Story: \n",
    "Tom and Anna are brother and sister. They like to play with their toys and read books. They are very happy because they are going on a holiday with their mum and dad. They will fly on a big plane to a place with a lot of sun and sand.\n",
    "The day of the holiday comes and they pack their bags. They go to the airport and wait for their plane. They see many other planes flying in the sky. They are very excited and want to fly too.\n",
    "\"Look, Anna, that plane is so big and fast!\" Tom says.\n",
    "\"Yes, Tom, and it has wings and a tail. I wonder where it is going,\" Anna says.\n",
    "They hear their mum call them. \"Come on, kids, it's time to board our plane. We have to show our tickets and go through the gate.\"\n",
    "They follow their mum and dad and get on their plane. They find their seats and buckle their belts. They look out the window and see the ground and the cars and the people. They hear the pilot say something on the speaker.\n",
    "\"Hello, everyone, this is your pilot speaking. Welcome aboard flight 123 to Sunny Beach. We are ready to take off. Please sit back and enjoy the flight.\"\n",
    "The plane starts to move and makes a loud noise. Tom and Anna feel the plane go faster and faster. They see the ground get smaller and smaller. They see the clouds get closer and closer. They are flying!\n",
    "\"Wow, Anna, we are flying! We are in the sky!\" Tom says.\n",
    "\"I know, Tom, it's amazing! We are so high! Look, there is the sun!\" Anna says.\n",
    "They smile and laugh and clap their hands. They are not sad at all. They are very happy. They are flying to their holiday.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "result = await translate(\n",
    "    source_lang=\"English\",\n",
    "    target_lang=\"Chinese\",\n",
    "    source_text=text,\n",
    "    country=\"China\",\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®é‡‡æ ·\n",
    "\n",
    "æˆ‘å…ˆçœ‹çœ‹è®­ç»ƒé›†æœ‰å¤šå°‘æ¡æ•°æ®ï¼Œå¯ä»¥å‘ç°æ–‡æœ¬éƒ½æ˜¯ä»¥`<|endoftext|>`ç»“å°¾çš„ï¼Œæ‰€ä»¥é€šè¿‡ç»Ÿè®¡`endoftext`çš„ä¸ªæ•°å°±å¯ä»¥çŸ¥é“æ•°æ®é›†çš„æ¡æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! grep -o \"endoftext\" ../../Data/TinyStoriesInstruct/TinyStories-Instruct-train.txt  | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥è¿‘250wçš„é‡çº§æœ‰ç‚¹å¤§ï¼ˆå› ä¸ºå¾®è½¯çš„è®ºæ–‡é‡Œæ˜¯ç›´æ¥åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåšçš„`pretrain`çš„ï¼‰ã€‚\n",
    "\n",
    "å…¶å®å¾ˆå¤šç ”ç©¶è¡¨æ˜ï¼Œ`SFT`æ•°æ®çš„é‡çº§ä¸é‡è¦ï¼Œè´¨é‡å¤Ÿé«˜çš„æ—¶å€™å³ä½¿å¾ˆå°‘çš„æ•°æ®ä¹Ÿèƒ½è®­ç»ƒå‡ºå¾ˆå¥½çš„æ•ˆæœã€‚\n",
    "\n",
    "æ‰€ä»¥è¿™é‡Œæˆ‘æ‰“ç®—éšæœºæŠ½å–11000æ¡æ•°æ®æ¥è¯•è¯•ã€‚\n",
    "\n",
    "æˆ‘çš„ç­–ç•¥å¦‚ä¸‹ï¼š\n",
    "1. éå†`train`æ•°æ®é›†ï¼Œè®©å››ç±»æŒ‡ä»¤çš„ç»„åˆå°½é‡å‡è¡¡ï¼ˆéœ€è¦å…ˆç»Ÿè®¡æŒ‡ä»¤ç»„åˆçš„çš„åˆ†å¸ƒï¼‰\n",
    "2. ç”¨å¾—åˆ°çš„11000æ¡æ•°æ®è°ƒç”¨ä¸Šé¢çš„`translation-agent`è¿›è¡Œç¿»è¯‘\n",
    "3. å°†ç¿»è¯‘åçš„æ•°æ®æ•´ç†æˆ`SFT`æ•°æ®é›†çš„`json`æ ¼å¼\n",
    "\n",
    "å…ˆæ¥åšæ•°æ®çš„é‡‡æ ·ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "\n",
    "def count_field_combinations(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    blocks = content.split(\"<|endoftext|>\")\n",
    "    combinations = []\n",
    "\n",
    "    for block in blocks:\n",
    "        fields = set()\n",
    "        if \"Words:\" in block:\n",
    "            fields.add(\"Words\")\n",
    "        if \"Random sentence:\" in block:\n",
    "            fields.add(\"Random sentence\")\n",
    "        if \"Features:\" in block:\n",
    "            fields.add(\"Features\")\n",
    "        if \"Summary:\" in block:\n",
    "            fields.add(\"Summary\")\n",
    "\n",
    "        if fields:  # åªæœ‰å½“å­—æ®µä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ ç»„åˆ\n",
    "            combinations.append(frozenset(fields))\n",
    "\n",
    "    return Counter(combinations)\n",
    "\n",
    "\n",
    "def sample_data(file_path, total_samples=11000):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    blocks = content.split(\"<|endoftext|>\")\n",
    "    blocks = [block.strip() for block in blocks if block.strip()]  # ç§»é™¤ç©ºå—\n",
    "\n",
    "    combinations = count_field_combinations(file_path)\n",
    "    combination_more_than_1 = {k: v for k, v in combinations.items() if v > 1}\n",
    "    samples_per_combination = total_samples // len(combination_more_than_1)\n",
    "\n",
    "    sampled_data = []\n",
    "    for combination in combinations:\n",
    "        matching_blocks = [\n",
    "            block for block in blocks if set(get_fields(block)) == set(combination)\n",
    "        ]\n",
    "        sampled_data.extend(\n",
    "            random.sample(\n",
    "                matching_blocks, min(samples_per_combination, len(matching_blocks))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "\n",
    "def get_fields(block):\n",
    "    fields = set()\n",
    "    if \"Words:\" in block:\n",
    "        fields.add(\"Words\")\n",
    "    if \"Random sentence:\" in block:\n",
    "        fields.add(\"Random sentence\")\n",
    "    if \"Features:\" in block:\n",
    "        fields.add(\"Features\")\n",
    "    if \"Summary:\" in block:\n",
    "        fields.add(\"Summary\")\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰§è¡Œä¸€ä¸‹çœ‹çœ‹æ•ˆæœï¼ˆä¸ºäº†æœ‰å¤‡æ— æ‚£ï¼Œå¤šé‡‡æ ·äº†5000æ¡æ•°æ®ï¼‰ï¼Œè€—æ—¶1-2åˆ†é’Ÿï¼Œè‚¯å®šè¿˜æœ‰ä¼˜åŒ–ç©ºé—´ï¼Œä½†æ˜¯å¯ä»¥æ¥å—ã€‚\n",
    "\n",
    "åŒæ—¶å°†é‡‡æ ·åçš„æ•°æ®ä¿å­˜ä¸º`pkl`æ–‡ä»¶ï¼Œæ–¹ä¾¿åç»­ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é‡‡æ ·æ•°æ®æ€»æ•°: 15001\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# sft_raw = sample_data(\n",
    "#     \"../../Data/TinyStoriesInstruct/TinyStories-Instruct-train.txt\", 15000\n",
    "# )\n",
    "sft_raw = pickle.load(open(\"sft_raw.pkl\", \"rb\"))\n",
    "print(f\"é‡‡æ ·æ•°æ®æ€»æ•°: {len(sft_raw)}\")\n",
    "\n",
    "# pickle.dump(sft_raw, open(\"sft_raw.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ‰¹é‡ç¿»è¯‘\n",
    "\n",
    "æ¥ä¸‹æ¥å°±å¯ä»¥è°ƒç”¨`translation-agent`è¿›è¡Œç¿»è¯‘äº†ã€‚\n",
    "\n",
    "è¿™é‡Œæˆ‘é™¤äº†ç”¨å¼‚æ­¥åŠ é€Ÿï¼Œè¿˜ä½¿ç”¨äº†`json`æ–‡ä»¶ç¼“å­˜æ¥é¿å…é‡å¤ç¿»è¯‘ï¼ˆ`gpt-4o-mini`çš„`api`ä¹Ÿä¸ç®—ä¾¿å®œï¼Œèƒ½çœåˆ™çœï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import aiofiles\n",
    "import asyncio\n",
    "\n",
    "cache_file = \"translation_cache.json\"\n",
    "\n",
    "\n",
    "async def translate_and_cache(block, cache, semaphore):\n",
    "    cache_key = hash(block)\n",
    "\n",
    "    if str(cache_key) in cache:\n",
    "        return cache[str(cache_key)]\n",
    "\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            result = await translate(\n",
    "                source_lang=\"English\",\n",
    "                target_lang=\"Chinese\",\n",
    "                source_text=block,\n",
    "                country=\"China\",\n",
    "            )\n",
    "            cache[str(cache_key)] = result\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"ç¿»è¯‘å¤±è´¥: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "async def batch_translate(sampled_data, cache_file, max_workers=10):\n",
    "    translated_data = []\n",
    "\n",
    "    try:\n",
    "        async with aiofiles.open(cache_file, \"r\") as f:\n",
    "            cache = json.loads(await f.read())\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        cache = {}\n",
    "\n",
    "    semaphore = asyncio.Semaphore(max_workers)\n",
    "    tasks = [translate_and_cache(block, cache, semaphore) for block in sampled_data]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    translated_data = [result for result in results if result]\n",
    "\n",
    "    async with aiofiles.open(cache_file, \"w\") as f:\n",
    "        await f.write(json.dumps(cache, ensure_ascii=False, indent=2))\n",
    "\n",
    "    return translated_data\n",
    "\n",
    "\n",
    "translated_data = await batch_translate(sft_raw, cache_file, max_workers=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨äº†100è·¯çš„å¹¶å‘ï¼Œç¿»è¯‘äº†15000æ¡æ•°æ®ï¼Œè€—æ—¶48åˆ†é’Ÿï¼Œä¹Ÿå°±æ˜¯å¤§æ¦‚æ¯åˆ†é’Ÿç¿»è¯‘300æ¡æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åç»­å¤„ç†\n",
    "\n",
    "ç¿»è¯‘å®Œæˆäº†ï¼Œæœ€åä¸€æ­¥å°±æ˜¯å°†æ•°æ®æ•´ç†æˆ`SFT`æ•°æ®é›†çš„æ ¼å¼ã€‚\n",
    "\n",
    "ï¼ˆè¿™é‡Œè¿˜å‘ç°äº†ä¸ªå°é—®é¢˜ï¼Œç¿»è¯‘ç»Ÿä¸€å°†**æ€»ç»“**å­—æ®µæ”¾åˆ°äº†æœ€åï¼Œå¯¼è‡´é¡ºåºå‡ºç°äº†é—®é¢˜ï¼Œæ‰€ä»¥è¿™é‡Œéœ€è¦å…ˆå¤„ç†ä¸€ä¸‹ã€‚ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "instruction_template = \"æŒ‰ç…§ä¸‹é¢è¾“å…¥çš„çº¦æŸç”Ÿæˆæ•…äº‹\"\n",
    "\n",
    "\n",
    "def split_data(data, keys):\n",
    "    result = []\n",
    "    current_key = None\n",
    "    current_content = \"\"\n",
    "\n",
    "    for line in data.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if any(key in line for key in keys):\n",
    "            if current_key:\n",
    "                result.append((current_key, current_content.strip()))\n",
    "            for key in keys:\n",
    "                if key in line:\n",
    "                    current_key, current_content = line.split(key, 1)\n",
    "                    current_key = key.strip()\n",
    "                    current_content = current_content.strip().lstrip(\"ï¼š\").strip()\n",
    "                    break\n",
    "        else:\n",
    "            current_content += \" \" + line\n",
    "\n",
    "    if current_key:\n",
    "        result.append((current_key, current_content.strip()))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_translated_data(input_file, output_file, expand_data=True):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_data = []\n",
    "    constraint_keys = Counter()\n",
    "\n",
    "    for key, value in data.items():\n",
    "        if \"æ•…äº‹ï¼š\" not in value:\n",
    "            continue\n",
    "        parts = value.split(\"æ•…äº‹ï¼š\")\n",
    "\n",
    "        if len(parts) == 2:\n",
    "            input_text = parts[0].strip()\n",
    "            output_text = parts[1].strip()\n",
    "            if \"æ€»ç»“ï¼š\" in output_text or \"æ‘˜è¦ï¼š\" in output_text:\n",
    "                # å°†æ€»ç»“æˆ–æ‘˜è¦æå–å‡ºæ¥æ”¾åˆ°input_textä¸­\n",
    "                for keyword in [\"æ€»ç»“\", \"æ‘˜è¦\"]:\n",
    "                    if keyword in output_text:\n",
    "                        summary = output_text.split(f\"{keyword}ï¼š\")[1].strip()\n",
    "                        input_text += f\"\\n{keyword}ï¼š{summary}\"\n",
    "                        output_text = output_text.split(f\"{keyword}ï¼š\")[0].strip()\n",
    "                        break\n",
    "\n",
    "            # æå–çº¦æŸæè¿°æ–‡æœ¬çš„å…³é”®å­—æ®µ\n",
    "            lines = input_text.split(\"\\n\")\n",
    "            for line in lines:\n",
    "                if \"ï¼š\" in line:\n",
    "                    key, _ = line.split(\"ï¼š\", 1)\n",
    "                    constraint_keys[key.strip()] += 1\n",
    "\n",
    "            processed_item = {\n",
    "                \"instruction\": instruction_template,\n",
    "                \"input\": f\"{input_text}\",\n",
    "                \"output\": output_text,\n",
    "            }\n",
    "\n",
    "            processed_data.append(processed_item)\n",
    "    # æ ¹æ®constraint_keysçš„é¢‘ç‡æ’åºï¼Œé€‰å–å‡ºç°é¢‘ç‡å¤§äº10çš„å…³é”®å­—\n",
    "    constraint_keys = {k: v for k, v in constraint_keys.items() if v > 10}\n",
    "\n",
    "    # æ•°æ®å¢å¼º\n",
    "    if expand_data:\n",
    "        expanded_data = []\n",
    "        for item in processed_data:\n",
    "            input_tuple_list = split_data(item[\"input\"], constraint_keys)\n",
    "            if not input_tuple_list:\n",
    "                continue\n",
    "\n",
    "            for permutation in itertools.permutations(input_tuple_list):\n",
    "                new_item = item.copy()\n",
    "                new_item[\"input\"] = \"\\n\".join(\n",
    "                    [f\"{key}ï¼š{value}\" for key, value in permutation]\n",
    "                )\n",
    "                expanded_data.append(new_item)\n",
    "    else:\n",
    "        expanded_data = processed_data\n",
    "\n",
    "    # å¯¹ç»“æœåšä¸€ä¸ªæ‰“ä¹±\n",
    "    random.shuffle(expanded_data)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(expanded_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return expanded_data, constraint_keys\n",
    "\n",
    "\n",
    "expanded_data, constraint_keys = process_translated_data(\n",
    "    \"translation_cache.json\",\n",
    "    \"../../Data/TinyStoriesInstruct/sft_data_no_expansion.json\",\n",
    "    expand_data=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "çœ‹ä¸€çœ‹å¤„ç†çš„ç»“æœï¼Œè¿™æ ·å°±å’Œç»å…¸çš„`SFT`æ•°æ®æ ¼å¼ä¸€è‡´äº†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'ç‰¹ç‚¹ï¼šè½¬æŠ˜\\n'\n",
      "          'æ‘˜è¦ï¼šè‰è‰åœ¨å¥¹çš„å¤§çº¢ç›’å­é‡Œå‘ç°äº†ä¸€åªä¼šé­”æ³•çš„é’è›™ï¼Œå¹¶å¸Œæœ›æœ‰ä¸€ä¸ªæœ‹å‹å¯ä»¥ä¸€èµ·ç©ï¼Œé’è›™å®ç°äº†å¥¹çš„æ„¿æœ›ï¼Œå˜æˆäº†ä¸€ä¸ªå°å¥³å­©ã€‚\\n'\n",
      "          'éšæœºå¥å­ï¼šç›’å­æ²¡æœ‰å¼¯æ›²ï¼Œä½†æ„Ÿè§‰ä¸åŒã€‚\\n'\n",
      "          'è¯æ±‡ï¼šæ•²æ‰“ï¼Œç›’å­ï¼Œçµæ´»',\n",
      " 'instruction': 'æŒ‰ç…§ä¸‹é¢è¾“å…¥çš„çº¦æŸç”Ÿæˆæ•…äº‹ï¼š',\n",
      " 'output': 'ä»å‰ï¼Œæœ‰ä¸€ä¸ªå°å¥³å­©å«è‰è‰ã€‚å¥¹æˆ¿é—´é‡Œæœ‰ä¸ªå¤§çº¢ç›’å­ã€‚è‰è‰å–œæ¬¢ç©è¿™ä¸ªç›’å­ã€‚ä¸€å¤©ï¼Œå¥¹æƒ³çœ‹çœ‹ç›’å­æ˜¯å¦çµæ´»ï¼Œäºæ˜¯å¥¹è¯•ç€å»å¼¯å®ƒã€‚ç›’å­æ²¡æœ‰å¼¯æ›²ï¼Œä½†æ„Ÿè§‰ä¸åŒã€‚  \\n'\n",
      "           'è‰è‰å†³å®šç”¨æ‰‹æ•²æ‰“ç›’å­ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆã€‚å½“å¥¹è¿™æ ·åšæ—¶ï¼Œç›’å­æ‰“å¼€äº†ï¼ç›’å­é‡Œæœ‰ä¸€åªå°å°çš„ç»¿è‰²é’è›™ã€‚é’è›™çœ‹ç€è‰è‰è¯´ï¼šâ€œä½ å¥½ï¼æˆ‘æ˜¯ä¸€åªä¼šé­”æ³•çš„é’è›™ã€‚â€  \\n'\n",
      "           'è‰è‰éå¸¸æƒŠè®¶ï¼Œå¥¹ç®€ç›´ä¸æ•¢ç›¸ä¿¡è‡ªå·±çš„çœ¼ç›ã€‚é’è›™å‘Šè¯‰è‰è‰ï¼Œä»–å¯ä»¥å®ç°å¥¹ä¸€ä¸ªæ„¿æœ›ã€‚è‰è‰æƒ³äº†ä¸€ä¼šå„¿ï¼Œè®¸ä¸‹äº†ä¸€ä¸ªå¸Œæœ›æœ‰æœ‹å‹ä¸€èµ·ç©çš„æ„¿æœ›ã€‚é­”æ³•é’è›™å¾®ç¬‘ç€å˜æˆäº†ä¸€ä¸ªå°å¥³å­©ï¼Œå°±åƒè‰è‰ä¸€æ ·ã€‚å¥¹ä»¬æ•´å¤©ç©å¾—ä¸äº¦ä¹ä¹ã€‚'}\n"
     ]
    }
   ],
   "source": [
    "pprint(expanded_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71322"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(expanded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å°ç»“\n",
    "1. åŸºäº`TinyStories`çš„`Instruct`æ•°æ®è¿›è¡ŒæŒ‡ä»¤ç»„åˆå±‚é¢å‡è¡¡çš„é‡‡æ ·ï¼Œè·å¾—äº†15000æ¡åŸå§‹æ•°æ®\n",
    "2. æ„é€ äº†ç¿»è¯‘å‡½æ•°ï¼Œå¼‚æ­¥ä½¿ç”¨å´æ©è¾¾è€å¸ˆçš„`translation-agent`å¯¹æ•°æ®è¿›è¡Œç¿»è¯‘\n",
    "3. åŸºäºç¿»è¯‘åçš„æ•°æ®ï¼Œæ„é€ äº†ç»å…¸æ ¼å¼çš„`SFT`æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
