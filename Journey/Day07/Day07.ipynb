{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零手搓中文大模型｜🚀 Day06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFT 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TinyStories`数据集其实也提供了[Instruct数据](https://huggingface.co/datasets/roneneldan/TinyStoriesInstruct)，我可以基于这个数据集在之前的预训练模型上进行指令微调。\n",
    "\n",
    "先看看数据集的格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -10 ../../Data/TinyStoriesInstruct/TinyStories-Instruct-valid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些指令有四种类型：\n",
    "1. 一个单词列表，包含在故事中。\n",
    "2. 一个句子，应该出现在故事的某个地方。\n",
    "3. 一个特征列表（可能的特征：对话、坏结局、道德价值、情节转折、伏笔、冲突）。\n",
    "4. 一个简短的总结（1-2行）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在面临两个问题：\n",
    "- 数据集是英文的，我需要想办法给整成中文的。\n",
    "- 数据集的形式和主流的SFT数据集不太一样，需要做一些适配。\n",
    "\n",
    "  > 个人理解这里是因为这里的指令相对单一（生成故事），只是约束有一些区别，所以作者采取了简单的拼接方式。\n",
    "  >\n",
    "  > 这里出于学习的目的还是往主流的SFT数据集上靠拢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 吴恩达老师的翻译Agent测试\n",
    "\n",
    "这里直接试了下[吴恩达老师的translation-agent](https://github.com/andrewyng/translation-agent)项目（`translation-agent.py`文件），使用的是`gpt-4o-mini`的`api`（也尝试过`Ollama`本地部署的`qwen14b`、`qwen7b`，相对来说不太稳定）。\n",
    "\n",
    "可以看到这里单次翻译的耗时在10秒左右（因为单词翻译的时候`agent`逻辑里有多次`api`调用），因此这里为了后面能够并发调用刷数据，我将代码全部改造成了`async`的异步调用。\n",
    "\n",
    "大家如果有其他的翻译`api`或者模型也可以替换，这里纯属心血来潮玩一玩儿。\n",
    "\n",
    "`translation-agent`项目其实只有一个`utils.py`文件，但因为太长了，这里就不把改造后的代码贴出来了，有兴趣的同学可以去仓库里查看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| num_tokens_in_text: 416\n",
      "ic| 'Translating text as a single chunk'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机句子：他们非常兴奋，也想飞起来。  \n",
      "特点：对话  \n",
      "摘要：汤姆和安娜很兴奋要和父母一起度假，他们乘坐一架大飞机飞往阳光明媚、沙滩众多的地方。  \n",
      "故事：  \n",
      "汤姆和安娜是兄妹。他们喜欢玩玩具和读书。他们非常高兴，因为他们要和妈妈爸爸一起去度假。他们将乘坐一架大飞机去一个阳光明媚、沙滩众多的地方。  \n",
      "度假日终于到了，他们收拾好行李。他们去机场，等待他们的飞机。他们看到许多飞机在天空中飞。他们非常兴奋，也想飞起来。  \n",
      "“看，安娜，那架飞机真大又快！”汤姆说。  \n",
      "“是的，汤姆，它有翅膀和尾巴。我想知道它要去哪里，”安娜说。  \n",
      "他们听到妈妈叫他们。“快来，孩子们，是时候登机了。我们得出示机票，通过登机口。”  \n",
      "他们跟着妈妈和爸爸上了飞机。他们找到座位，系好安全带。他们望向窗外，看到地面、汽车和行人。他们听到飞行员在扬声器上说话。  \n",
      "“大家好，我是你们的机长。欢迎乘坐123航班前往阳光海滩。我们准备起飞。请坐好，祝大家旅途愉快。”  \n",
      "飞机开始移动，发出轰鸣声。汤姆和安娜感到飞机越来越快。他们看到地面变得越来越小。他们看到云朵越来越近。他们飞起来了！  \n",
      "“哇，安娜，我们飞起来了！我们在天空中！”汤姆说。  \n",
      "“我知道，汤姆，真是太神奇了！我们这么高！看，那是太阳！”安娜说。  \n",
      "他们微笑、欢笑，拍着手。他们一点都不难过。他们非常快乐。他们正在飞往度假地。\n"
     ]
    }
   ],
   "source": [
    "from translation_agent import translate\n",
    "\n",
    "text = \"\"\"\n",
    "Random sentence: They are very excited and want to fly too.\n",
    "Features: Dialogue\n",
    "Summary: Tom and Anna are excited to go on a holiday with their parents, and they fly on a big plane to a place with sun and sand.\n",
    "Story: \n",
    "Tom and Anna are brother and sister. They like to play with their toys and read books. They are very happy because they are going on a holiday with their mum and dad. They will fly on a big plane to a place with a lot of sun and sand.\n",
    "The day of the holiday comes and they pack their bags. They go to the airport and wait for their plane. They see many other planes flying in the sky. They are very excited and want to fly too.\n",
    "\"Look, Anna, that plane is so big and fast!\" Tom says.\n",
    "\"Yes, Tom, and it has wings and a tail. I wonder where it is going,\" Anna says.\n",
    "They hear their mum call them. \"Come on, kids, it's time to board our plane. We have to show our tickets and go through the gate.\"\n",
    "They follow their mum and dad and get on their plane. They find their seats and buckle their belts. They look out the window and see the ground and the cars and the people. They hear the pilot say something on the speaker.\n",
    "\"Hello, everyone, this is your pilot speaking. Welcome aboard flight 123 to Sunny Beach. We are ready to take off. Please sit back and enjoy the flight.\"\n",
    "The plane starts to move and makes a loud noise. Tom and Anna feel the plane go faster and faster. They see the ground get smaller and smaller. They see the clouds get closer and closer. They are flying!\n",
    "\"Wow, Anna, we are flying! We are in the sky!\" Tom says.\n",
    "\"I know, Tom, it's amazing! We are so high! Look, there is the sun!\" Anna says.\n",
    "They smile and laugh and clap their hands. They are not sad at all. They are very happy. They are flying to their holiday.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "result = await translate(\n",
    "    source_lang=\"English\",\n",
    "    target_lang=\"Chinese\",\n",
    "    source_text=text,\n",
    "    country=\"China\",\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据采样\n",
    "\n",
    "我先看看训练集有多少条数据，可以发现文本都是以`<|endoftext|>`结尾的，所以通过统计`endoftext`的个数就可以知道数据集的条数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2476532\n"
     ]
    }
   ],
   "source": [
    "! grep -o \"endoftext\" ../../Data/TinyStoriesInstruct/TinyStories-Instruct-train.txt  | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接近250w的量级有点大（因为微软的论文里是直接在整个数据集上做的`pretrain`的）。\n",
    "\n",
    "其实很多研究表明，`SFT`数据的量级不重要，质量够高的时候即使很少的数据也能训练出很好的效果。\n",
    "\n",
    "所以这里我打算随机抽取11000条数据来试试。\n",
    "\n",
    "我的策略如下：\n",
    "1. 遍历`train`数据集，让四类指令的组合尽量均衡（需要先统计指令组合的的分布）\n",
    "2. 用得到的11000条数据调用上面的`translation-agent`进行翻译\n",
    "3. 将翻译后的数据整理成`SFT`数据集的`json`格式\n",
    "\n",
    "先来做数据的采样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "\n",
    "def count_field_combinations(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    blocks = content.split(\"<|endoftext|>\")\n",
    "    combinations = []\n",
    "\n",
    "    for block in blocks:\n",
    "        fields = set()\n",
    "        if \"Words:\" in block:\n",
    "            fields.add(\"Words\")\n",
    "        if \"Random sentence:\" in block:\n",
    "            fields.add(\"Random sentence\")\n",
    "        if \"Features:\" in block:\n",
    "            fields.add(\"Features\")\n",
    "        if \"Summary:\" in block:\n",
    "            fields.add(\"Summary\")\n",
    "\n",
    "        if fields:  # 只有当字段不为空时才添加组合\n",
    "            combinations.append(frozenset(fields))\n",
    "\n",
    "    return Counter(combinations)\n",
    "\n",
    "\n",
    "def sample_data(file_path, total_samples=11000):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    blocks = content.split(\"<|endoftext|>\")\n",
    "    blocks = [block.strip() for block in blocks if block.strip()]  # 移除空块\n",
    "\n",
    "    combinations = count_field_combinations(file_path)\n",
    "    combination_more_than_1 = {k: v for k, v in combinations.items() if v > 1}\n",
    "    samples_per_combination = total_samples // len(combination_more_than_1)\n",
    "\n",
    "    sampled_data = []\n",
    "    for combination in combinations:\n",
    "        matching_blocks = [\n",
    "            block for block in blocks if set(get_fields(block)) == set(combination)\n",
    "        ]\n",
    "        sampled_data.extend(\n",
    "            random.sample(\n",
    "                matching_blocks, min(samples_per_combination, len(matching_blocks))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "\n",
    "def get_fields(block):\n",
    "    fields = set()\n",
    "    if \"Words:\" in block:\n",
    "        fields.add(\"Words\")\n",
    "    if \"Random sentence:\" in block:\n",
    "        fields.add(\"Random sentence\")\n",
    "    if \"Features:\" in block:\n",
    "        fields.add(\"Features\")\n",
    "    if \"Summary:\" in block:\n",
    "        fields.add(\"Summary\")\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行一下看看效果（为了有备无患，多采样了5000条数据），耗时1-2分钟，肯定还有优化空间，但是可以接受。\n",
    "\n",
    "同时将采样后的数据保存为`pkl`文件，方便后续使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "采样数据总数: 15001\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "sft_raw = sample_data(\n",
    "    \"../../Data/TinyStoriesInstruct/TinyStories-Instruct-train.txt\", 15000\n",
    ")\n",
    "print(f\"采样数据总数: {len(sft_raw)}\")\n",
    "\n",
    "pickle.dump(sft_raw, open(\"sft_raw.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量翻译\n",
    "\n",
    "接下来就可以调用`translation-agent`进行翻译了。\n",
    "\n",
    "这里我除了用异步加速，还使用了`json`文件缓存来避免重复翻译（`gpt-4o-mini`的`api`也不算便宜，能省则省）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import aiofiles\n",
    "import asyncio\n",
    "\n",
    "cache_file = \"translation_cache.json\"\n",
    "\n",
    "\n",
    "async def translate_and_cache(block, cache, semaphore):\n",
    "    cache_key = hash(block)\n",
    "\n",
    "    if str(cache_key) in cache:\n",
    "        return cache[str(cache_key)]\n",
    "\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            result = await translate(\n",
    "                source_lang=\"English\",\n",
    "                target_lang=\"Chinese\",\n",
    "                source_text=block,\n",
    "                country=\"China\",\n",
    "            )\n",
    "            cache[str(cache_key)] = result\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"翻译失败: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "async def batch_translate(sampled_data, cache_file, max_workers=10):\n",
    "    translated_data = []\n",
    "\n",
    "    try:\n",
    "        async with aiofiles.open(cache_file, \"r\") as f:\n",
    "            cache = json.loads(await f.read())\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        cache = {}\n",
    "\n",
    "    semaphore = asyncio.Semaphore(max_workers)\n",
    "    tasks = [translate_and_cache(block, cache, semaphore) for block in sampled_data]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    translated_data = [result for result in results if result]\n",
    "\n",
    "    async with aiofiles.open(cache_file, \"w\") as f:\n",
    "        await f.write(json.dumps(cache, ensure_ascii=False, indent=2))\n",
    "\n",
    "    return translated_data\n",
    "\n",
    "\n",
    "translated_data = await batch_translate(sft_raw, cache_file, max_workers=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用了100路的并发，翻译了15000条数据，耗时48分钟，也就是大概每分钟翻译300条数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 后续处理\n",
    "\n",
    "翻译完成了，最后一步就是将数据整理成`SFT`数据集的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('摘要', 8433),\n",
      " ('随机句子', 5372),\n",
      " ('词汇', 4824),\n",
      " ('特点', 4603),\n",
      " ('特征', 1269),\n",
      " ('单词', 1073),\n",
      " ('总结', 1001),\n",
      " ('关键词', 349),\n",
      " ('随机的句子', 246),\n",
      " ('随机句', 196),\n",
      " ('故事特点', 146),\n",
      " ('主题', 122),\n",
      " ('词语', 108),\n",
      " ('随便一句话', 95),\n",
      " ('随机一句话', 87),\n",
      " ('随机的一句话', 24),\n",
      " ('词', 21),\n",
      " ('故事特征', 20),\n",
      " ('随机句子是', 19),\n",
      " ('随便说一句', 17)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "instruction_template = \"\"\"按照给定的要求讲故事，\n",
    "其中‘摘要’表示故事的总结，\n",
    "‘单词/词汇/关键词’表示故事中必须包含的单词，\n",
    "‘随机句子’表示故事中必须包含的句子，\n",
    "‘特征/特点’表示故事的特征，如对话、坏结局、道德价值、情节转折、伏笔、冲突等。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def process_translated_data(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_data = []\n",
    "    constraint_keys = Counter()\n",
    "\n",
    "    for key, value in data.items():\n",
    "        if \"故事：\" not in value:\n",
    "            continue\n",
    "        parts = value.split(\"故事：\")\n",
    "\n",
    "        if len(parts) == 2:\n",
    "            input_text = parts[0].strip()\n",
    "            output_text = parts[1].strip()\n",
    "\n",
    "            # 提取约束描述文本的关键字段\n",
    "            lines = input_text.split(\"\\n\")\n",
    "            for line in lines:\n",
    "                if \"：\" in line:\n",
    "                    key, _ = line.split(\"：\", 1)\n",
    "                    constraint_keys[key.strip()] += 1\n",
    "\n",
    "            processed_item = {\n",
    "                \"instruction\": instruction_template,\n",
    "                \"input\": input_text,\n",
    "                \"output\": output_text,\n",
    "            }\n",
    "\n",
    "            processed_data.append(processed_item)\n",
    "\n",
    "    # 将处理后的数据写入输出文件\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(processed_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return processed_data, constraint_keys\n",
    "\n",
    "\n",
    "processed_data, constraint_keys = process_translated_data(\n",
    "    \"translation_cache.json\", \"../../Data/TinyStoriesInstruct/sft_data.json\"\n",
    ")\n",
    "\n",
    "pprint(constraint_keys.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一看处理的结果，这样就和经典的`SFT`数据格式一致了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '特点：对话  \\n'\n",
      "          '摘要：蒂米和妈妈一起去商店，对商店里所有的玩具和糖果感到惊讶。他请求触摸一个玩具，妈妈允许他，这让他非常开心。  \\n'\n",
      "          '词汇：触摸、商店、宽敞',\n",
      " 'instruction': '按照给定的要求讲故事，\\n'\n",
      "                '其中‘摘要’表示故事的总结，\\n'\n",
      "                '‘单词/词汇/关键词’表示故事中必须包含的单词，\\n'\n",
      "                '‘随机句子’表示故事中必须包含的句子，\\n'\n",
      "                '‘特征/特点’表示故事的特征，如对话、坏结局、道德价值、情节转折、伏笔、冲突等。\\n',\n",
      " 'output': '很久以前，有一个小男孩名叫蒂米。蒂米喜欢在外面玩耍和探索。  \\n'\n",
      "           '一天，蒂米和妈妈一起去商店。商店非常大，宽敞。蒂米对他看到的玩具和糖果真是太多了，感到惊讶。  \\n'\n",
      "           '突然，蒂米看到一个他非常想触摸的玩具。“妈妈，我可以触摸那个玩具吗？”他问。  \\n'\n",
      "           '“当然可以啊，蒂米，”妈妈说。蒂米非常开心，他轻轻摸了摸玩具。摸起来软软的，特别有弹性。  \\n'\n",
      "           '离开商店后，蒂米对妈妈说他有多喜欢和她一起逛商店。“我玩得可开心了，摸玩具真有意思，”他说。妈妈微笑着把他抱住了。'}\n"
     ]
    }
   ],
   "source": [
    "pprint(processed_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "1. 基于`TinyStories`的`Instruct`数据进行指令组合层面均衡的采样，获得了15000条原始数据\n",
    "2. 构造了翻译函数，异步使用吴恩达老师的`translation-agent`对数据进行翻译\n",
    "3. 基于翻译后的数据，构造了经典格式的`SFT`数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigmodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
